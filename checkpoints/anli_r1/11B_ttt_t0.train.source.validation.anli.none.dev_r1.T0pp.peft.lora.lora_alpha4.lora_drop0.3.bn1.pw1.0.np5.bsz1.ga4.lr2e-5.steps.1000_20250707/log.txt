[2025-07-07 19:36:20,417] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-07 19:36:22,480] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-07-07 19:36:22,516] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-07-07 19:36:22,516] [INFO] [runner.py:610:main] cmd = /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py --deepspeed /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json --dataset_name anli --subset_name none --prompt_set_name anli --testset_name dev_r1 --model_name_or_path T0pp --per_device_train_batch_size 1 --per_device_eval_batch_size 100 --test_mode ttt_t0 --cache_dir /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface --metric_name accuracy --debug_size -1 --peft_option lora --bottleneck_dim 1 --do_train --logging_steps 10 --num_train_epochs 50 --max_steps 1000 --adam_beta1 0.9 --adam_beta2 0.98 --adam_epsilon 1e-6 --seed 42 --debug_size 10000 --max_dev_size 1000 --learning_rate 2e-5 --evaluation_strategy steps --eval_steps 50 --disable_eval_mode 0 --pseudo_target_mode pairwise --ensemble_subset_size 0.0 --loss_option consistency --jsd 0 --detach_kl_left 1 --detach_kl_right 0 --ensemble_option avg_prob --pseudo_train_loss_weight 1.0 --pseudo_dist smooth --lora_dropout 0.3 --lora_alpha 4 --lora_pos encdec --prob_temperature 1.0 --combine_option uniform --train_random_n_prompts 5 --train_data_source validation --save_strategy steps --save_steps 10 --save_total_limit 10 --warmup_steps 100 --gradient_accumulation_steps 4 --lr_scheduler_type polynomial --output_dir /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707 --overwrite_output_dir --report_to wandb --bf16 --disable_tqdm True
[2025-07-07 19:36:24,783] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-07 19:36:27,189] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-07-07 19:36:27,223] [INFO] [launch.py:139:main] 0 NCCL_P2P_DISABLE=1
[2025-07-07 19:36:27,223] [INFO] [launch.py:139:main] 0 NCCL_DEBUG=INFO
[2025-07-07 19:36:27,223] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5]}
[2025-07-07 19:36:27,223] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=6, node_rank=0
[2025-07-07 19:36:27,223] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5]})
[2025-07-07 19:36:27,223] [INFO] [launch.py:164:main] dist_world_size=6
[2025-07-07 19:36:27,223] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
[2025-07-07 19:36:27,243] [INFO] [launch.py:256:main] process 3458270 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=0', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'anli', '--subset_name', 'none', '--prompt_set_name', 'anli', '--testset_name', 'dev_r1', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-07-07 19:36:27,261] [INFO] [launch.py:256:main] process 3458271 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=1', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'anli', '--subset_name', 'none', '--prompt_set_name', 'anli', '--testset_name', 'dev_r1', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-07-07 19:36:27,277] [INFO] [launch.py:256:main] process 3458272 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=2', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'anli', '--subset_name', 'none', '--prompt_set_name', 'anli', '--testset_name', 'dev_r1', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-07-07 19:36:27,293] [INFO] [launch.py:256:main] process 3458273 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=3', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'anli', '--subset_name', 'none', '--prompt_set_name', 'anli', '--testset_name', 'dev_r1', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-07-07 19:36:27,307] [INFO] [launch.py:256:main] process 3458274 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=4', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'anli', '--subset_name', 'none', '--prompt_set_name', 'anli', '--testset_name', 'dev_r1', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-07-07 19:36:27,317] [INFO] [launch.py:256:main] process 3458275 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=5', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'anli', '--subset_name', 'none', '--prompt_set_name', 'anli', '--testset_name', 'dev_r1', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-07-07 19:36:30,511] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-07-07 19:36:31,072] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-07-07 19:36:31,579] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-07 19:36:32,080] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-07 19:36:32,162] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-07 19:36:32,170] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-07 19:36:33,056] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-07-07 19:36:33,059] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-07-07 19:36:34,241] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-07-07 19:36:34,245] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-07-07 19:36:34,461] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-07-07 19:36:34,464] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-07-07 19:36:34,465] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
07/07/2025 19:36:35 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
[2025-07-07 19:36:35,247] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-07-07 19:36:35,251] [INFO] [comm.py:675:init_distributed] cdb=None
07/07/2025 19:36:35 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
07/07/2025 19:36:35 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[2025-07-07 19:36:35,845] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-07-07 19:36:35,851] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-07-07 19:36:35,942] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-07-07 19:36:35,948] [INFO] [comm.py:675:init_distributed] cdb=None
07/07/2025 19:36:35 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:604] 2025-07-07 19:36:36,179 >> loading configuration file https://huggingface.co/bigscience/T0pp/resolve/main/config.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/4d1146d0ecdfd40f42d8bb242217b1c3525f9a6527d3776ed01d77fd33a7a441.0f92c21f2009b0840bbd769b797c754010294dc29ff942ac5b8e53ce3393c880
[INFO|configuration_utils.py:641] 2025-07-07 19:36:36,180 >> Model config T5Config {
  "_name_or_path": "bigscience/T0pp",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 10240,
  "d_kv": 64,
  "d_model": 4096,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 24,
  "num_heads": 64,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.14.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

07/07/2025 19:36:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.98,
adam_epsilon=1e-06,
bf16=True,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json,
disable_tqdm=True,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=50,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/runs/Jul07_19-36-30_bosco,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.POLYNOMIAL,
max_grad_norm=1.0,
max_steps=1000,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=50.0,
output_dir=/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=100,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707,
save_on_each_node=False,
save_steps=10,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=100,
weight_decay=0.0,
xpu_backend=None,
)
07/07/2025 19:36:36 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
07/07/2025 19:36:36 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:604] 2025-07-07 19:36:36,475 >> loading configuration file https://huggingface.co/bigscience/T0pp/resolve/main/config.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/4d1146d0ecdfd40f42d8bb242217b1c3525f9a6527d3776ed01d77fd33a7a441.0f92c21f2009b0840bbd769b797c754010294dc29ff942ac5b8e53ce3393c880
[INFO|configuration_utils.py:641] 2025-07-07 19:36:36,477 >> Model config T5Config {
  "_name_or_path": "bigscience/T0pp",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 10240,
  "d_kv": 64,
  "d_model": 4096,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 24,
  "num_heads": 64,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.14.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:1742] 2025-07-07 19:36:37,151 >> loading file https://huggingface.co/bigscience/T0pp/resolve/main/spiece.model from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/8114f85106b092be35d1fd155f8e0198e4ab5b91bfde792524b2248496f7009b.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551
[INFO|tokenization_utils_base.py:1742] 2025-07-07 19:36:37,151 >> loading file https://huggingface.co/bigscience/T0pp/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1742] 2025-07-07 19:36:37,151 >> loading file https://huggingface.co/bigscience/T0pp/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1742] 2025-07-07 19:36:37,151 >> loading file https://huggingface.co/bigscience/T0pp/resolve/main/special_tokens_map.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/187314ce7257d07c6c662e057bb8d7b56fa65059b054b505902c338051b5f510.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46
[INFO|tokenization_utils_base.py:1742] 2025-07-07 19:36:37,151 >> loading file https://huggingface.co/bigscience/T0pp/resolve/main/tokenizer_config.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/2e98ea1bd83bead9304da27afdce5f9ffd57c298d8fa5e4bbc351c500a1cb69f.b1a2e3c152960fdc6b3d16520fa9f1591e2818d7dd66946c219e651f224894bf
[INFO|configuration_utils.py:604] 2025-07-07 19:36:37,380 >> loading configuration file https://huggingface.co/bigscience/T0pp/resolve/main/config.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/4d1146d0ecdfd40f42d8bb242217b1c3525f9a6527d3776ed01d77fd33a7a441.0f92c21f2009b0840bbd769b797c754010294dc29ff942ac5b8e53ce3393c880
[INFO|configuration_utils.py:641] 2025-07-07 19:36:37,382 >> Model config T5Config {
  "_name_or_path": "bigscience/T0pp",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 10240,
  "d_kv": 64,
  "d_model": 4096,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 24,
  "num_heads": 64,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.14.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|configuration_utils.py:604] 2025-07-07 19:36:37,626 >> loading configuration file https://huggingface.co/bigscience/T0pp/resolve/main/config.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/4d1146d0ecdfd40f42d8bb242217b1c3525f9a6527d3776ed01d77fd33a7a441.0f92c21f2009b0840bbd769b797c754010294dc29ff942ac5b8e53ce3393c880
[INFO|configuration_utils.py:641] 2025-07-07 19:36:37,627 >> Model config T5Config {
  "_name_or_path": "bigscience/T0pp",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 10240,
  "d_kv": 64,
  "d_model": 4096,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 24,
  "num_heads": 64,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.14.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

Generating train_r1 split:   0%|          | 0/16946 [00:00<?, ? examples/s]Generating train_r1 split: 100%|██████████| 16946/16946 [00:00<00:00, 278021.20 examples/s]
Generating dev_r1 split:   0%|          | 0/1000 [00:00<?, ? examples/s]Generating dev_r1 split: 100%|██████████| 1000/1000 [00:00<00:00, 95288.27 examples/s]
Generating test_r1 split:   0%|          | 0/1000 [00:00<?, ? examples/s]Generating test_r1 split: 100%|██████████| 1000/1000 [00:00<00:00, 90031.64 examples/s]
Generating train_r2 split:   0%|          | 0/45460 [00:00<?, ? examples/s]Generating train_r2 split:  97%|█████████▋| 44000/45460 [00:00<00:00, 430243.19 examples/s]Generating train_r2 split: 100%|██████████| 45460/45460 [00:00<00:00, 424572.72 examples/s]
Generating dev_r2 split:   0%|          | 0/1000 [00:00<?, ? examples/s]Generating dev_r2 split: 100%|██████████| 1000/1000 [00:00<00:00, 99532.61 examples/s]
Generating test_r2 split:   0%|          | 0/1000 [00:00<?, ? examples/s]Generating test_r2 split: 100%|██████████| 1000/1000 [00:00<00:00, 103084.55 examples/s]
Generating train_r3 split:   0%|          | 0/100459 [00:00<?, ? examples/s]Generating train_r3 split:  34%|███▍      | 34000/100459 [00:00<00:00, 333609.39 examples/s]Generating train_r3 split:  95%|█████████▍| 95000/100459 [00:00<00:00, 488132.45 examples/s]Generating train_r3 split: 100%|██████████| 100459/100459 [00:00<00:00, 471371.92 examples/s]
Generating dev_r3 split:   0%|          | 0/1200 [00:00<?, ? examples/s]Generating dev_r3 split: 100%|██████████| 1200/1200 [00:00<00:00, 113541.13 examples/s]
Generating test_r3 split:   0%|          | 0/1200 [00:00<?, ? examples/s]Generating test_r3 split: 100%|██████████| 1200/1200 [00:00<00:00, 117792.71 examples/s]
Found cached dataset anli (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9)
07/07/2025 19:36:53 - INFO - datasets.builder - Found cached dataset anli (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9)
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9
07/07/2025 19:36:53 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9
 has 15 original task prompts, number choices = 3, total test examples = 1000
 has 15 original task prompts, number choices = 3, total test examples = 1000
 has 15 original task prompts, number choices = 3, total test examples = 1000
 has 15 original task prompts, number choices = 3, total test examples = 1000
 has 15 original task prompts, number choices = 3, total test examples = 1000
 has 15 original task prompts, number choices = 3, total test examples = 1000
07/07/2025 19:36:54 - INFO - __main__ - Model parameters T5Config {
  "_name_or_path": "bigscience/T0pp",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "bottleneck_dim": 1,
  "combine_option": "uniform",
  "d_ff": 10240,
  "d_kv": 64,
  "d_model": 4096,
  "debug_size": 10000,
  "decoder_start_token_id": 0,
  "detach_kl_left": 1,
  "detach_kl_right": 0,
  "disable_eval_mode": 0,
  "dropout_rate": 0.1,
  "ensemble_option": "avg_prob",
  "ensemble_subset_size": 0.0,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "jsd": 0,
  "layer_norm_epsilon": 1e-06,
  "lora_alpha": 4.0,
  "lora_dropout": 0.3,
  "lora_layer_k": 24,
  "lora_pos": "encdec",
  "loss_option": "consistency",
  "max_dev_size": 1000,
  "max_early_stop_patience": 2,
  "metric_name": "accuracy",
  "min_train_steps": 300,
  "model_type": "t5",
  "num_choices": 3,
  "num_decoder_layers": 24,
  "num_heads": 64,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "peft_option": "lora",
  "prob_temperature": 1.0,
  "prune_prompt": null,
  "pseudo_dist": "smooth",
  "pseudo_target_mode": "pairwise",
  "pseudo_train_loss_weight": 1.0,
  "relative_attention_num_buckets": 32,
  "self_train_option": "none",
  "split_answer_groups": 1,
  "test_mode": "ttt_t0",
  "tie_word_embeddings": false,
  "train_data_source": "validation",
  "train_duplicates": 1,
  "train_random_n_prompts": 5,
  "transformers_version": "4.14.0.dev0",
  "use_cache": true,
  "use_deepspeed": false,
  "vocab_size": 32128
}

[INFO|modeling_utils.py:1352] 2025-07-07 19:36:54,578 >> loading weights file https://huggingface.co/bigscience/T0pp/resolve/main/pytorch_model.bin from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/1f25255af64f1faddb22578b89598cf590af9decc1f32e825724c27d067bf1f2.09d001758c33b10b097a250b8ba56686ad015c7bd75e74a56cd6d8ece9ad2d70
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.0.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.0.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.0.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.0.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.1.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.1.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.1.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.1.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.2.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.2.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.2.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.2.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.3.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.3.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.3.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.3.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.4.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.4.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.4.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.4.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.5.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.5.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.5.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.5.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.6.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.6.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.6.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.6.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.7.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.7.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.7.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.7.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.8.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.8.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.8.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.8.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.9.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.9.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.9.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.9.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.10.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.10.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.10.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.10.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.11.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.11.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.11.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.11.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.12.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.12.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.12.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.12.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.13.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.13.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.13.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.13.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.14.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.14.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.14.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.14.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.15.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.15.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.15.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.15.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.16.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.16.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.16.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.16.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.17.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.17.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.17.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.17.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.18.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.18.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.18.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.18.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.19.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.19.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.19.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.19.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.20.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.20.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.20.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.20.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.21.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.21.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.21.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.21.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.22.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.22.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.22.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.22.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.23.layer.1.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.23.layer.1.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.23.layer.1.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune encoder.block.23.layer.1.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.0.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.0.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.0.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.0.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.1.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.1.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.1.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.1.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.2.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.2.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.2.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.2.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.3.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.3.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.3.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.3.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.4.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.4.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.4.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.4.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.5.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.5.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.5.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.5.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.6.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.6.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.6.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.6.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.7.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.7.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.7.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.7.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.8.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.8.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.8.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.8.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.9.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.9.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.9.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.9.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.10.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.10.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.10.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.10.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.11.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.11.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.11.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.11.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.12.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.12.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.12.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.12.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.13.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.13.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.13.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.13.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.14.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.14.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.14.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.14.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.15.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.15.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.15.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.15.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.16.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.16.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.16.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.16.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.17.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.17.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.17.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.17.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.18.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.18.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.18.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.18.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.19.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.19.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.19.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.19.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.20.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.20.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.20.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.20.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.21.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.21.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.21.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.21.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.22.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.22.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.22.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.22.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.23.layer.2.DenseReluDense.wi_0.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.23.layer.2.DenseReluDense.wi_0.ef_lora_B
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.23.layer.2.DenseReluDense.wo.ef_lora_A
07/07/2025 19:38:45 - INFO - __main__ - tune decoder.block.23.layer.2.DenseReluDense.wo.ef_lora_B
07/07/2025 19:39:14 - INFO - __main__ - there are 15 prompts in total
07/07/2025 19:39:14 - INFO - __main__ - using 5 prompts  during training
Overwrite dataset info from restored data version if exists.
07/07/2025 19:39:19 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9
07/07/2025 19:39:19 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9
Found cached dataset anli (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9)
07/07/2025 19:39:19 - INFO - datasets.builder - Found cached dataset anli (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9)
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9
07/07/2025 19:39:19 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9
 has 15 original task prompts, number choices = 3, total test examples = 1000
07/07/2025 19:39:19 - INFO - __main__ - train data number prompts: 15
Building TTT training set: validation!
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT training set: validation!
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT training set: validation!
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT training set: validation!
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT training set: validation!
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT training set: validation!
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT evaluation test set!
Overwrite dataset info from restored data version if exists.
07/07/2025 19:39:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9
07/07/2025 19:39:58 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9
Found cached dataset anli (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9)
07/07/2025 19:39:58 - INFO - datasets.builder - Found cached dataset anli (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9)
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9
07/07/2025 19:39:58 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/anli/plain_text/0.0.0/8e4813d81f46d313dac7892e1c28076917cfcdf9
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT evaluation test set!
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT evaluation test set!
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT evaluation test set!
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT evaluation test set!
 has 15 original task prompts, number choices = 3, total test examples = 1000
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
07/07/2025 19:41:05 - INFO - __main__ - prompt groups [[0], [1], [2, 6], [3, 4, 7, 8, 9, 11, 12, 13], [5, 14], [10]]
[INFO|trainer.py:430] 2025-07-07 19:41:10,074 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:450] 2025-07-07 19:41:10,074 >> Using amp half precision backend
[INFO|trainer.py:634] 2025-07-07 19:41:10,075 >> build train sampler!
[INFO|deepspeed.py:336] 2025-07-07 19:41:10,075 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[2025-07-07 19:41:10,092] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.1, git-hash=unknown, git-branch=unknown
[2025-07-07 19:41:10,092] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 6
[2025-07-07 19:41:33,328] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=6
	 self.mp_world_size=1
	 self.seq_dp_world_size=6
	 self.sequence_parallel_size=1
***********************************************
bosco:3458270:3458270 [0] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:3458270:3458270 [0] NCCL INFO cudaDriverVersion 12080
bosco:3458270:3458270 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:3458270:3458270 [0] NCCL INFO Comm config Blocking set to 1
bosco:3458272:3458272 [2] NCCL INFO cudaDriverVersion 12080
bosco:3458271:3458271 [1] NCCL INFO cudaDriverVersion 12080
bosco:3458272:3458272 [2] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:3458272:3458272 [2] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:3458271:3458271 [1] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:3458271:3458271 [1] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:3458271:3458271 [1] NCCL INFO Comm config Blocking set to 1
bosco:3458272:3458272 [2] NCCL INFO Comm config Blocking set to 1
bosco:3458270:3462285 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:3458270:3462285 [0] NCCL INFO NET/IB : No device found.
bosco:3458270:3462285 [0] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:3458270:3462285 [0] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:3458270:3462285 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:3458270:3462285 [0] NCCL INFO Using network Socket
bosco:3458270:3462285 [0] NCCL INFO ncclCommInitRankConfig comm 0x55fc022bfe60 rank 0 nranks 6 cudaDev 0 nvmlDev 0 busId 2d000 commId 0xb17fa220e710252e - Init START
bosco:3458272:3462287 [2] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:3458272:3462287 [2] NCCL INFO NET/IB : No device found.
bosco:3458272:3462287 [2] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:3458272:3462287 [2] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:3458272:3462287 [2] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:3458272:3462287 [2] NCCL INFO Using network Socket
bosco:3458271:3462286 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:3458271:3462286 [1] NCCL INFO NET/IB : No device found.
bosco:3458271:3462286 [1] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:3458271:3462286 [1] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:3458271:3462286 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:3458271:3462286 [1] NCCL INFO Using network Socket
bosco:3458272:3462287 [2] NCCL INFO ncclCommInitRankConfig comm 0x55dc8c248510 rank 2 nranks 6 cudaDev 2 nvmlDev 2 busId 3b000 commId 0xb17fa220e710252e - Init START
bosco:3458271:3462286 [1] NCCL INFO ncclCommInitRankConfig comm 0x55f4afa4a8d0 rank 1 nranks 6 cudaDev 1 nvmlDev 1 busId 3a000 commId 0xb17fa220e710252e - Init START
bosco:3458271:3462286 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:3458273:3458273 [3] NCCL INFO cudaDriverVersion 12080
bosco:3458273:3458273 [3] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:3458273:3458273 [3] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:3458273:3458273 [3] NCCL INFO Comm config Blocking set to 1
bosco:3458273:3462310 [3] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:3458273:3462310 [3] NCCL INFO NET/IB : No device found.
bosco:3458273:3462310 [3] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:3458273:3462310 [3] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:3458273:3462310 [3] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:3458273:3462310 [3] NCCL INFO Using network Socket
bosco:3458273:3462310 [3] NCCL INFO ncclCommInitRankConfig comm 0x5647e6406c50 rank 3 nranks 6 cudaDev 3 nvmlDev 3 busId 3c000 commId 0xb17fa220e710252e - Init START
bosco:3458272:3462287 [2] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:3458274:3458274 [4] NCCL INFO cudaDriverVersion 12080
bosco:3458274:3458274 [4] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:3458274:3458274 [4] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:3458274:3458274 [4] NCCL INFO Comm config Blocking set to 1
bosco:3458274:3462326 [4] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:3458274:3462326 [4] NCCL INFO NET/IB : No device found.
bosco:3458274:3462326 [4] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:3458274:3462326 [4] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:3458274:3462326 [4] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:3458274:3462326 [4] NCCL INFO Using network Socket
bosco:3458274:3462326 [4] NCCL INFO ncclCommInitRankConfig comm 0x55a90d839c40 rank 4 nranks 6 cudaDev 4 nvmlDev 4 busId ad000 commId 0xb17fa220e710252e - Init START
bosco:3458273:3462310 [3] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:3458275:3458275 [5] NCCL INFO cudaDriverVersion 12080
bosco:3458275:3458275 [5] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:3458275:3458275 [5] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:3458275:3458275 [5] NCCL INFO Comm config Blocking set to 1
bosco:3458275:3462358 [5] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:3458275:3462358 [5] NCCL INFO NET/IB : No device found.
bosco:3458275:3462358 [5] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:3458275:3462358 [5] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:3458275:3462358 [5] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:3458275:3462358 [5] NCCL INFO Using network Socket
bosco:3458275:3462358 [5] NCCL INFO ncclCommInitRankConfig comm 0x55e78b120880 rank 5 nranks 6 cudaDev 5 nvmlDev 5 busId ae000 commId 0xb17fa220e710252e - Init START
bosco:3458275:3462358 [5] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:3458274:3462326 [4] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:3458270:3462285 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:3458270:3462285 [0] NCCL INFO Bootstrap timings total 13.576942 (create 0.000028, send 0.000102, recv 0.031235, ring 0.000302, delay 0.000000)
bosco:3458275:3462358 [5] NCCL INFO Bootstrap timings total 0.001212 (create 0.000030, send 0.000129, recv 0.000386, ring 0.000373, delay 0.000000)
bosco:3458274:3462326 [4] NCCL INFO Bootstrap timings total 7.088847 (create 0.000028, send 0.000138, recv 7.087971, ring 0.000339, delay 0.000000)
bosco:3458273:3462310 [3] NCCL INFO Bootstrap timings total 9.183310 (create 0.000028, send 0.000104, recv 2.094881, ring 7.087906, delay 0.000000)
bosco:3458272:3462287 [2] NCCL INFO Bootstrap timings total 13.550203 (create 0.000022, send 0.000094, recv 4.367363, ring 9.182318, delay 0.000000)
bosco:3458271:3462286 [1] NCCL INFO Bootstrap timings total 13.546040 (create 0.000020, send 0.000076, recv 0.000295, ring 13.545375, delay 0.000000)
bosco:3458273:3462310 [3] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:3458273:3462310 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:3458273:3462310 [3] NCCL INFO NVLS multicast support is not available on dev 3
bosco:3458274:3462326 [4] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:3458274:3462326 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffffff00,00000000,0000ffff,ffffffff,ff000000,00000000
bosco:3458274:3462326 [4] NCCL INFO NVLS multicast support is not available on dev 4
bosco:3458272:3462287 [2] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:3458272:3462287 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:3458272:3462287 [2] NCCL INFO NVLS multicast support is not available on dev 2
bosco:3458271:3462286 [1] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:3458271:3462286 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:3458271:3462286 [1] NCCL INFO NVLS multicast support is not available on dev 1
bosco:3458270:3462285 [0] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:3458270:3462285 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:3458270:3462285 [0] NCCL INFO NVLS multicast support is not available on dev 0
bosco:3458275:3462358 [5] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:3458275:3462358 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffffff00,00000000,0000ffff,ffffffff,ff000000,00000000
bosco:3458275:3462358 [5] NCCL INFO NVLS multicast support is not available on dev 5
bosco:3458272:3462287 [2] NCCL INFO comm 0x55dc8c248510 rank 2 nRanks 6 nNodes 1 localRanks 6 localRank 2 MNNVL 0
bosco:3458271:3462286 [1] NCCL INFO comm 0x55f4afa4a8d0 rank 1 nRanks 6 nNodes 1 localRanks 6 localRank 1 MNNVL 0
bosco:3458270:3462285 [0] NCCL INFO comm 0x55fc022bfe60 rank 0 nRanks 6 nNodes 1 localRanks 6 localRank 0 MNNVL 0
bosco:3458272:3462287 [2] NCCL INFO Trees [0] 4/-1/-1->2->1 [1] 4/-1/-1->2->1 [2] 4/-1/-1->2->1 [3] 4/-1/-1->2->1
bosco:3458274:3462326 [4] NCCL INFO comm 0x55a90d839c40 rank 4 nRanks 6 nNodes 1 localRanks 6 localRank 4 MNNVL 0
bosco:3458272:3462287 [2] NCCL INFO P2P Chunksize set to 131072
bosco:3458273:3462310 [3] NCCL INFO comm 0x5647e6406c50 rank 3 nRanks 6 nNodes 1 localRanks 6 localRank 3 MNNVL 0
bosco:3458274:3462326 [4] NCCL INFO Trees [0] 5/-1/-1->4->2 [1] 5/-1/-1->4->2 [2] 5/-1/-1->4->2 [3] 5/-1/-1->4->2
bosco:3458270:3462285 [0] NCCL INFO Channel 00/04 : 0 1 2 4 5 3
bosco:3458274:3462326 [4] NCCL INFO P2P Chunksize set to 131072
bosco:3458270:3462285 [0] NCCL INFO Channel 01/04 : 0 1 2 4 5 3
bosco:3458270:3462285 [0] NCCL INFO Channel 02/04 : 0 1 2 4 5 3
bosco:3458271:3462286 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
bosco:3458275:3462358 [5] NCCL INFO comm 0x55e78b120880 rank 5 nRanks 6 nNodes 1 localRanks 6 localRank 5 MNNVL 0
bosco:3458270:3462285 [0] NCCL INFO Channel 03/04 : 0 1 2 4 5 3
bosco:3458271:3462286 [1] NCCL INFO P2P Chunksize set to 131072
bosco:3458270:3462285 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1
bosco:3458270:3462285 [0] NCCL INFO P2P Chunksize set to 131072
bosco:3458273:3462310 [3] NCCL INFO Trees [0] -1/-1/-1->3->5 [1] -1/-1/-1->3->5 [2] -1/-1/-1->3->5 [3] -1/-1/-1->3->5
bosco:3458275:3462358 [5] NCCL INFO Trees [0] 3/-1/-1->5->4 [1] 3/-1/-1->5->4 [2] 3/-1/-1->5->4 [3] 3/-1/-1->5->4
bosco:3458275:3462358 [5] NCCL INFO P2P Chunksize set to 131072
bosco:3458273:3462310 [3] NCCL INFO P2P Chunksize set to 131072
bosco:3458270:3462285 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
bosco:3458270:3462364 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
bosco:3458270:3462368 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 123
bosco:3458275:3462366 [5] NCCL INFO [Proxy Service] Device 5 CPU core 57
bosco:3458274:3462362 [4] NCCL INFO [Proxy Service] Device 4 CPU core 175
bosco:3458274:3462365 [4] NCCL INFO [Proxy Service UDS] Device 4 CPU core 176
bosco:3458275:3462369 [5] NCCL INFO [Proxy Service UDS] Device 5 CPU core 58
bosco:3458273:3462371 [3] NCCL INFO [Proxy Service] Device 3 CPU core 124
bosco:3458273:3462373 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 126
bosco:3458271:3462372 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 127
bosco:3458271:3462370 [1] NCCL INFO [Proxy Service] Device 1 CPU core 125
bosco:3458272:3462367 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 156
bosco:3458272:3462363 [2] NCCL INFO [Proxy Service] Device 2 CPU core 155
bosco:3458275:3462358 [5] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458275:3462358 [5] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458272:3462287 [2] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458272:3462287 [2] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458273:3462310 [3] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458273:3462310 [3] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458270:3462285 [0] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458270:3462285 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458274:3462326 [4] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458274:3462326 [4] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458271:3462286 [1] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458271:3462286 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458270:3462285 [0] NCCL INFO CC Off, workFifoBytes 1048576
bosco:3458274:3462326 [4] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:3458274:3462326 [4] NCCL INFO ncclCommInitRankConfig comm 0x55a90d839c40 rank 4 nranks 6 cudaDev 4 nvmlDev 4 busId ad000 commId 0xb17fa220e710252e - Init COMPLETE
bosco:3458274:3462326 [4] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 6 total 7.36 (kernels 0.15, alloc 0.01, bootstrap 7.09, allgathers 0.05, topo 0.03, graphs 0.02, connections 0.01, rest 0.00)
bosco:3458275:3462358 [5] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:3458275:3462358 [5] NCCL INFO ncclCommInitRankConfig comm 0x55e78b120880 rank 5 nranks 6 cudaDev 5 nvmlDev 5 busId ae000 commId 0xb17fa220e710252e - Init COMPLETE
bosco:3458275:3462358 [5] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 6 total 0.28 (kernels 0.16, alloc 0.01, bootstrap 0.00, allgathers 0.04, topo 0.03, graphs 0.02, connections 0.01, rest 0.00)
bosco:3458272:3462287 [2] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:3458272:3462287 [2] NCCL INFO ncclCommInitRankConfig comm 0x55dc8c248510 rank 2 nranks 6 cudaDev 2 nvmlDev 2 busId 3b000 commId 0xb17fa220e710252e - Init COMPLETE
bosco:3458272:3462287 [2] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 6 total 13.81 (kernels 0.14, alloc 0.01, bootstrap 13.55, allgathers 0.05, topo 0.03, graphs 0.02, connections 0.01, rest 0.00)
bosco:3458270:3462285 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:3458270:3462285 [0] NCCL INFO ncclCommInitRankConfig comm 0x55fc022bfe60 rank 0 nranks 6 cudaDev 0 nvmlDev 0 busId 2d000 commId 0xb17fa220e710252e - Init COMPLETE
bosco:3458270:3462285 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 6 total 13.83 (kernels 0.14, alloc 0.01, bootstrap 13.58, allgathers 0.02, topo 0.03, graphs 0.05, connections 0.01, rest 0.00)
bosco:3458273:3462310 [3] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:3458273:3462310 [3] NCCL INFO ncclCommInitRankConfig comm 0x5647e6406c50 rank 3 nranks 6 cudaDev 3 nvmlDev 3 busId 3c000 commId 0xb17fa220e710252e - Init COMPLETE
bosco:3458273:3462310 [3] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 6 total 9.44 (kernels 0.13, alloc 0.01, bootstrap 9.18, allgathers 0.00, topo 0.03, graphs 0.07, connections 0.01, rest 0.00)
bosco:3458271:3462286 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:3458271:3462286 [1] NCCL INFO ncclCommInitRankConfig comm 0x55f4afa4a8d0 rank 1 nranks 6 cudaDev 1 nvmlDev 1 busId 3a000 commId 0xb17fa220e710252e - Init COMPLETE
bosco:3458271:3462286 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 6 total 13.81 (kernels 0.14, alloc 0.01, bootstrap 13.55, allgathers 0.00, topo 0.03, graphs 0.07, connections 0.01, rest 0.00)
bosco:3458270:3462374 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
bosco:3458270:3462374 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
bosco:3458274:3462376 [4] NCCL INFO Channel 00 : 4[4] -> 5[5] via SHM/direct/direct
bosco:3458273:3462375 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
bosco:3458270:3462374 [0] NCCL INFO Channel 02 : 0[0] -> 1[1] via SHM/direct/direct
bosco:3458274:3462376 [4] NCCL INFO Channel 01 : 4[4] -> 5[5] via SHM/direct/direct
bosco:3458273:3462375 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
bosco:3458270:3462374 [0] NCCL INFO Channel 03 : 0[0] -> 1[1] via SHM/direct/direct
bosco:3458274:3462376 [4] NCCL INFO Channel 02 : 4[4] -> 5[5] via SHM/direct/direct
bosco:3458273:3462375 [3] NCCL INFO Channel 02 : 3[3] -> 0[0] via SHM/direct/direct
bosco:3458274:3462376 [4] NCCL INFO Channel 03 : 4[4] -> 5[5] via SHM/direct/direct
bosco:3458273:3462375 [3] NCCL INFO Channel 03 : 3[3] -> 0[0] via SHM/direct/direct
bosco:3458271:3462377 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
bosco:3458271:3462377 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
bosco:3458271:3462377 [1] NCCL INFO Channel 02 : 1[1] -> 2[2] via SHM/direct/direct
bosco:3458271:3462377 [1] NCCL INFO Channel 03 : 1[1] -> 2[2] via SHM/direct/direct
bosco:3458275:3462379 [5] NCCL INFO Channel 00 : 5[5] -> 3[3] via SHM/direct/direct
bosco:3458275:3462379 [5] NCCL INFO Channel 01 : 5[5] -> 3[3] via SHM/direct/direct
bosco:3458272:3462378 [2] NCCL INFO Channel 00 : 2[2] -> 4[4] via SHM/direct/direct
bosco:3458275:3462379 [5] NCCL INFO Channel 02 : 5[5] -> 3[3] via SHM/direct/direct
bosco:3458272:3462378 [2] NCCL INFO Channel 01 : 2[2] -> 4[4] via SHM/direct/direct
bosco:3458275:3462379 [5] NCCL INFO Channel 03 : 5[5] -> 3[3] via SHM/direct/direct
bosco:3458272:3462378 [2] NCCL INFO Channel 02 : 2[2] -> 4[4] via SHM/direct/direct
bosco:3458272:3462378 [2] NCCL INFO Channel 03 : 2[2] -> 4[4] via SHM/direct/direct
bosco:3458273:3462375 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:3458272:3462378 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:3458271:3462377 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:3458275:3462379 [5] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:3458274:3462376 [4] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:3458270:3462374 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
[2025-07-07 19:41:47,498] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-07-07 19:41:47,500] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-07-07 19:41:47,500] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-07-07 19:41:47,534] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-07-07 19:41:47,534] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2025-07-07 19:41:47,534] [WARNING] [engine.py:1359:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2025-07-07 19:41:47,534] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-07-07 19:41:47,534] [INFO] [stage_1_and_2.py:151:__init__] Reduce bucket size 200000000
[2025-07-07 19:41:47,534] [INFO] [stage_1_and_2.py:152:__init__] Allgather bucket size 200000000
[2025-07-07 19:41:47,534] [INFO] [stage_1_and_2.py:153:__init__] CPU Offload: True
[2025-07-07 19:41:47,534] [INFO] [stage_1_and_2.py:154:__init__] Round robin gradient partitioning: False
run dev evaluation first to collect initial predictions
run dev evaluation first to collect initial predictions
run dev evaluation first to collect initial predictions
run dev evaluation first to collect initial predictions
run dev evaluation first to collect initial predictions
[2025-07-07 19:41:50,380] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-07-07 19:41:50,381] [INFO] [utils.py:782:see_memory_usage] MA 20.75 GB         Max_MA 20.75 GB         CA 20.77 GB         Max_CA 21 GB 
[2025-07-07 19:41:50,381] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.46 GB, percent = 2.4%
[2025-07-07 19:41:50,811] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-07-07 19:41:50,811] [INFO] [utils.py:782:see_memory_usage] MA 20.75 GB         Max_MA 20.75 GB         CA 20.77 GB         Max_CA 21 GB 
[2025-07-07 19:41:50,811] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.55 GB, percent = 2.4%
[2025-07-07 19:41:50,811] [INFO] [stage_1_and_2.py:573:__init__] optimizer state initialized
[2025-07-07 19:41:51,239] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-07-07 19:41:51,239] [INFO] [utils.py:782:see_memory_usage] MA 20.75 GB         Max_MA 20.75 GB         CA 20.77 GB         Max_CA 21 GB 
[2025-07-07 19:41:51,240] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.54 GB, percent = 2.4%
[2025-07-07 19:41:51,241] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-07-07 19:41:51,241] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-07-07 19:41:51,241] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f2727179250>
[2025-07-07 19:41:51,241] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.98)]
[2025-07-07 19:41:51,243] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-07-07 19:41:51,243] [INFO] [config.py:921:print] DeepSpeedEngine configuration:
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   amp_enabled .................. False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   amp_params ................... False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=True immediate_grad_update=False check_grad_overflow=False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f27271791c0>
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   communication_data_type ...... None
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   dataloader_drop_last ......... False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   disable_allgather ............ False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   dump_state ................... False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-07-07 19:41:51,244] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   elasticity_enabled ........... False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   global_rank .................. 0
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   grad_accum_dtype ............. None
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 4
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   gradient_clipping ............ 1.0
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   graph_harvesting ............. False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   load_universal_checkpoint .... False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   memory_breakdown ............. False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   mics_shard_size .............. -1
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   optimizer_name ............... None
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   optimizer_params ............. None
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   pld_enabled .................. False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   pld_params ................... False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   prescale_gradients ........... False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   scheduler_name ............... None
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   scheduler_params ............. None
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   sparse_attention ............. None
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   steps_per_print .............. 2000
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   train_batch_size ............. 24
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  1
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   use_node_local_storage ....... False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   weight_quantization_config ... None
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   world_size ................... 6
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  True
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   zero_enabled ................. True
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. False
[2025-07-07 19:41:51,245] [INFO] [config.py:925:print]   zero_optimization_stage ...... 2
[2025-07-07 19:41:51,245] [INFO] [config.py:911:print_user_config]   json = {
    "bfloat16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true
    }, 
    "zero_allow_untested_optimizer": true, 
    "zero_force_ds_cpu_optimizer": false, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 2.000000e+03, 
    "train_batch_size": 24, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false
}
run dev evaluation first to collect initial predictions
[INFO|trainer.py:2925] 2025-07-07 19:41:51,246 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 19:41:51,246 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 19:41:51,246 >>   Batch size = 100
bosco:3458270:3458270 [0] NCCL INFO Comm config Blocking set to 1
bosco:3458270:3462403 [0] NCCL INFO Using network Socket
bosco:3458273:3458273 [3] NCCL INFO Comm config Blocking set to 1
bosco:3458274:3458274 [4] NCCL INFO Comm config Blocking set to 1
bosco:3458271:3458271 [1] NCCL INFO Comm config Blocking set to 1
bosco:3458272:3458272 [2] NCCL INFO Comm config Blocking set to 1
bosco:3458275:3458275 [5] NCCL INFO Comm config Blocking set to 1
bosco:3458274:3462405 [4] NCCL INFO Using network Socket
bosco:3458273:3462404 [3] NCCL INFO Using network Socket
bosco:3458271:3462406 [1] NCCL INFO Using network Socket
bosco:3458270:3462403 [0] NCCL INFO ncclCommInitRankConfig comm 0x55fc2f2890f0 rank 0 nranks 6 cudaDev 0 nvmlDev 0 busId 2d000 commId 0xcced6bedd3d68269 - Init START
bosco:3458272:3462407 [2] NCCL INFO Using network Socket
bosco:3458275:3462408 [5] NCCL INFO Using network Socket
bosco:3458274:3462405 [4] NCCL INFO ncclCommInitRankConfig comm 0x55a93a0253e0 rank 4 nranks 6 cudaDev 4 nvmlDev 4 busId ad000 commId 0xcced6bedd3d68269 - Init START
bosco:3458271:3462406 [1] NCCL INFO ncclCommInitRankConfig comm 0x55f4dc235fa0 rank 1 nranks 6 cudaDev 1 nvmlDev 1 busId 3a000 commId 0xcced6bedd3d68269 - Init START
bosco:3458273:3462404 [3] NCCL INFO ncclCommInitRankConfig comm 0x564814c0b420 rank 3 nranks 6 cudaDev 3 nvmlDev 3 busId 3c000 commId 0xcced6bedd3d68269 - Init START
bosco:3458275:3462408 [5] NCCL INFO ncclCommInitRankConfig comm 0x55e7b7919f30 rank 5 nranks 6 cudaDev 5 nvmlDev 5 busId ae000 commId 0xcced6bedd3d68269 - Init START
bosco:3458272:3462407 [2] NCCL INFO ncclCommInitRankConfig comm 0x55dcb8a3ac10 rank 2 nranks 6 cudaDev 2 nvmlDev 2 busId 3b000 commId 0xcced6bedd3d68269 - Init START
bosco:3458273:3462404 [3] NCCL INFO Bootstrap timings total 0.001041 (create 0.000035, send 0.000174, recv 0.000103, ring 0.000136, delay 0.000000)
bosco:3458274:3462405 [4] NCCL INFO Bootstrap timings total 0.001393 (create 0.000035, send 0.000174, recv 0.000566, ring 0.000445, delay 0.000000)
bosco:3458275:3462408 [5] NCCL INFO Bootstrap timings total 0.001033 (create 0.000040, send 0.000171, recv 0.000295, ring 0.000339, delay 0.000000)
bosco:3458272:3462407 [2] NCCL INFO Bootstrap timings total 0.000842 (create 0.000037, send 0.000161, recv 0.000328, ring 0.000148, delay 0.000000)
bosco:3458271:3462406 [1] NCCL INFO Bootstrap timings total 0.001319 (create 0.000041, send 0.000216, recv 0.000662, ring 0.000229, delay 0.000000)
bosco:3458270:3462403 [0] NCCL INFO Bootstrap timings total 0.002264 (create 0.000040, send 0.000181, recv 0.001108, ring 0.000367, delay 0.000000)
bosco:3458272:3462407 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:3458272:3462407 [2] NCCL INFO NVLS multicast support is not available on dev 2
bosco:3458271:3462406 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:3458271:3462406 [1] NCCL INFO NVLS multicast support is not available on dev 1
bosco:3458270:3462403 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:3458270:3462403 [0] NCCL INFO NVLS multicast support is not available on dev 0
bosco:3458275:3462408 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffffff00,00000000,0000ffff,ffffffff,ff000000,00000000
bosco:3458275:3462408 [5] NCCL INFO NVLS multicast support is not available on dev 5
bosco:3458274:3462405 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffffff00,00000000,0000ffff,ffffffff,ff000000,00000000
bosco:3458274:3462405 [4] NCCL INFO NVLS multicast support is not available on dev 4
bosco:3458273:3462404 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:3458273:3462404 [3] NCCL INFO NVLS multicast support is not available on dev 3
bosco:3458275:3462408 [5] NCCL INFO comm 0x55e7b7919f30 rank 5 nRanks 6 nNodes 1 localRanks 6 localRank 5 MNNVL 0
bosco:3458271:3462406 [1] NCCL INFO comm 0x55f4dc235fa0 rank 1 nRanks 6 nNodes 1 localRanks 6 localRank 1 MNNVL 0
bosco:3458273:3462404 [3] NCCL INFO comm 0x564814c0b420 rank 3 nRanks 6 nNodes 1 localRanks 6 localRank 3 MNNVL 0
bosco:3458270:3462403 [0] NCCL INFO comm 0x55fc2f2890f0 rank 0 nRanks 6 nNodes 1 localRanks 6 localRank 0 MNNVL 0
bosco:3458272:3462407 [2] NCCL INFO comm 0x55dcb8a3ac10 rank 2 nRanks 6 nNodes 1 localRanks 6 localRank 2 MNNVL 0
bosco:3458274:3462405 [4] NCCL INFO comm 0x55a93a0253e0 rank 4 nRanks 6 nNodes 1 localRanks 6 localRank 4 MNNVL 0
bosco:3458270:3462403 [0] NCCL INFO Channel 00/04 : 0 1 2 4 5 3
bosco:3458270:3462403 [0] NCCL INFO Channel 01/04 : 0 1 2 4 5 3
bosco:3458270:3462403 [0] NCCL INFO Channel 02/04 : 0 1 2 4 5 3
bosco:3458270:3462403 [0] NCCL INFO Channel 03/04 : 0 1 2 4 5 3
bosco:3458272:3462407 [2] NCCL INFO Trees [0] 4/-1/-1->2->1 [1] 4/-1/-1->2->1 [2] 4/-1/-1->2->1 [3] 4/-1/-1->2->1
bosco:3458271:3462406 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
bosco:3458274:3462405 [4] NCCL INFO Trees [0] 5/-1/-1->4->2 [1] 5/-1/-1->4->2 [2] 5/-1/-1->4->2 [3] 5/-1/-1->4->2
bosco:3458272:3462407 [2] NCCL INFO P2P Chunksize set to 131072
bosco:3458271:3462406 [1] NCCL INFO P2P Chunksize set to 131072
bosco:3458270:3462403 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1
bosco:3458274:3462405 [4] NCCL INFO P2P Chunksize set to 131072
bosco:3458270:3462403 [0] NCCL INFO P2P Chunksize set to 131072
bosco:3458275:3462408 [5] NCCL INFO Trees [0] 3/-1/-1->5->4 [1] 3/-1/-1->5->4 [2] 3/-1/-1->5->4 [3] 3/-1/-1->5->4
bosco:3458275:3462408 [5] NCCL INFO P2P Chunksize set to 131072
bosco:3458270:3462403 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
bosco:3458273:3462404 [3] NCCL INFO Trees [0] -1/-1/-1->3->5 [1] -1/-1/-1->3->5 [2] -1/-1/-1->3->5 [3] -1/-1/-1->3->5
bosco:3458273:3462404 [3] NCCL INFO P2P Chunksize set to 131072
bosco:3458270:3462409 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
bosco:3458271:3462410 [1] NCCL INFO [Proxy Service] Device 1 CPU core 5
bosco:3458272:3462418 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 10
bosco:3458275:3462411 [5] NCCL INFO [Proxy Service] Device 5 CPU core 59
bosco:3458270:3462415 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
bosco:3458271:3462416 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 121
bosco:3458275:3462417 [5] NCCL INFO [Proxy Service UDS] Device 5 CPU core 60
bosco:3458272:3462412 [2] NCCL INFO [Proxy Service] Device 2 CPU core 8
bosco:3458273:3462414 [3] NCCL INFO [Proxy Service] Device 3 CPU core 7
bosco:3458274:3462413 [4] NCCL INFO [Proxy Service] Device 4 CPU core 190
bosco:3458274:3462419 [4] NCCL INFO [Proxy Service UDS] Device 4 CPU core 79
bosco:3458273:3462420 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 10
bosco:3458275:3462408 [5] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458275:3462408 [5] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458274:3462405 [4] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458274:3462405 [4] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458270:3462403 [0] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458270:3462403 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458270:3462403 [0] NCCL INFO CC Off, workFifoBytes 1048576
bosco:3458271:3462406 [1] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458271:3462406 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458272:3462407 [2] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458272:3462407 [2] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458273:3462404 [3] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
bosco:3458273:3462404 [3] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:3458270:3462403 [0] NCCL INFO ncclCommInitRankConfig comm 0x55fc2f2890f0 rank 0 nranks 6 cudaDev 0 nvmlDev 0 busId 2d000 commId 0xcced6bedd3d68269 - Init COMPLETE
bosco:3458270:3462403 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 6 total 0.11 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.02, topo 0.04, graphs 0.03, connections 0.02, rest 0.00)
bosco:3458272:3462407 [2] NCCL INFO ncclCommInitRankConfig comm 0x55dcb8a3ac10 rank 2 nranks 6 cudaDev 2 nvmlDev 2 busId 3b000 commId 0xcced6bedd3d68269 - Init COMPLETE
bosco:3458274:3462405 [4] NCCL INFO ncclCommInitRankConfig comm 0x55a93a0253e0 rank 4 nranks 6 cudaDev 4 nvmlDev 4 busId ad000 commId 0xcced6bedd3d68269 - Init COMPLETE
bosco:3458272:3462407 [2] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 6 total 0.11 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.03, graphs 0.05, connections 0.02, rest 0.00)
bosco:3458274:3462405 [4] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 6 total 0.11 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.04, graphs 0.05, connections 0.02, rest 0.00)
bosco:3458275:3462408 [5] NCCL INFO ncclCommInitRankConfig comm 0x55e7b7919f30 rank 5 nranks 6 cudaDev 5 nvmlDev 5 busId ae000 commId 0xcced6bedd3d68269 - Init COMPLETE
bosco:3458275:3462408 [5] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 6 total 0.11 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.04, graphs 0.04, connections 0.01, rest 0.01)
bosco:3458271:3462406 [1] NCCL INFO ncclCommInitRankConfig comm 0x55f4dc235fa0 rank 1 nranks 6 cudaDev 1 nvmlDev 1 busId 3a000 commId 0xcced6bedd3d68269 - Init COMPLETE
bosco:3458273:3462404 [3] NCCL INFO ncclCommInitRankConfig comm 0x564814c0b420 rank 3 nranks 6 cudaDev 3 nvmlDev 3 busId 3c000 commId 0xcced6bedd3d68269 - Init COMPLETE
bosco:3458271:3462406 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 6 total 0.11 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.01, topo 0.03, graphs 0.04, connections 0.02, rest 0.00)
bosco:3458273:3462404 [3] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 6 total 0.11 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.01, topo 0.04, graphs 0.03, connections 0.02, rest 0.00)
bosco:3458274:3462422 [4] NCCL INFO Channel 00 : 4[4] -> 5[5] via SHM/direct/direct
bosco:3458273:3462425 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
bosco:3458270:3462421 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
bosco:3458274:3462422 [4] NCCL INFO Channel 01 : 4[4] -> 5[5] via SHM/direct/direct
bosco:3458270:3462421 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
bosco:3458273:3462425 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
bosco:3458274:3462422 [4] NCCL INFO Channel 02 : 4[4] -> 5[5] via SHM/direct/direct
bosco:3458270:3462421 [0] NCCL INFO Channel 02 : 0[0] -> 1[1] via SHM/direct/direct
bosco:3458273:3462425 [3] NCCL INFO Channel 02 : 3[3] -> 0[0] via SHM/direct/direct
bosco:3458274:3462422 [4] NCCL INFO Channel 03 : 4[4] -> 5[5] via SHM/direct/direct
bosco:3458270:3462421 [0] NCCL INFO Channel 03 : 0[0] -> 1[1] via SHM/direct/direct
bosco:3458273:3462425 [3] NCCL INFO Channel 03 : 3[3] -> 0[0] via SHM/direct/direct
bosco:3458275:3462423 [5] NCCL INFO Channel 00 : 5[5] -> 3[3] via SHM/direct/direct
bosco:3458275:3462423 [5] NCCL INFO Channel 01 : 5[5] -> 3[3] via SHM/direct/direct
bosco:3458271:3462426 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
bosco:3458271:3462426 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
bosco:3458271:3462426 [1] NCCL INFO Channel 02 : 1[1] -> 2[2] via SHM/direct/direct
bosco:3458275:3462423 [5] NCCL INFO Channel 02 : 5[5] -> 3[3] via SHM/direct/direct
bosco:3458271:3462426 [1] NCCL INFO Channel 03 : 1[1] -> 2[2] via SHM/direct/direct
bosco:3458275:3462423 [5] NCCL INFO Channel 03 : 5[5] -> 3[3] via SHM/direct/direct
bosco:3458272:3462424 [2] NCCL INFO Channel 00 : 2[2] -> 4[4] via SHM/direct/direct
bosco:3458272:3462424 [2] NCCL INFO Channel 01 : 2[2] -> 4[4] via SHM/direct/direct
bosco:3458272:3462424 [2] NCCL INFO Channel 02 : 2[2] -> 4[4] via SHM/direct/direct
bosco:3458272:3462424 [2] NCCL INFO Channel 03 : 2[2] -> 4[4] via SHM/direct/direct
bosco:3458272:3462424 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:3458271:3462426 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:3458270:3462421 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:3458274:3462422 [4] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:3458275:3462423 [5] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:3458273:3462425 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
finish collecting initial predictions before optimization
all entropy: [0.6841421175434236, 0.3329369943751297, 0.4509519323755258, 0.6753695804824225, 0.6866350322126447, 0.6870849202888889, 0.952769835407555, 0.6799671738038009, 0.6423531620946674, 0.6864040100950229, 0.0, 0.681241859470388, 0.6897814027084003, 0.6910977801629237, 0.6567924418440357]
avg entropy: 0.6131685495243221
avg cont entropy: 0.7122594030752551
accuracy: 0.52
precision: 0.34647981796147315
recall: 0.5196963430496364
f1: 0.41576829850613223
posix: 2.72729229927063
finish collecting initial predictions before optimization
all entropy: [0.6841421175434236, 0.3329369943751297, 0.4509519323755258, 0.6753695804824225, 0.6866350322126447, 0.6870849202888889, 0.952769835407555, 0.6799671738038009, 0.6423531620946674, 0.6864040100950229, 0.0, 0.681241859470388, 0.6897814027084003, 0.6910977801629237, 0.6567924418440357]
avg entropy: 0.6131685495243221
avg cont entropy: 0.7122594030752551
accuracy: 0.52
precision: 0.34647981796147315
recall: 0.5196963430496364
f1: 0.41576829850613223
posix: 2.72729229927063
finish collecting initial predictions before optimization
all entropy: [0.6841421175434236, 0.3329369943751297, 0.4509519323755258, 0.6753695804824225, 0.6866350322126447, 0.6870849202888889, 0.952769835407555, 0.6799671738038009, 0.6423531620946674, 0.6864040100950229, 0.0, 0.681241859470388, 0.6897814027084003, 0.6910977801629237, 0.6567924418440357]
avg entropy: 0.6131685495243221
avg cont entropy: 0.7122594030752551
accuracy: 0.52
precision: 0.34647981796147315
recall: 0.5196963430496364
f1: 0.41576829850613223
posix: 2.72729229927063
finish collecting initial predictions before optimization
all entropy: [0.6841421175434236, 0.3329369943751297, 0.4509519323755258, 0.6753695804824225, 0.6866350322126447, 0.6870849202888889, 0.952769835407555, 0.6799671738038009, 0.6423531620946674, 0.6864040100950229, 0.0, 0.681241859470388, 0.6897814027084003, 0.6910977801629237, 0.6567924418440357]
avg entropy: 0.6131685495243221
avg cont entropy: 0.7122594030752551
accuracy: 0.52
precision: 0.34647981796147315
recall: 0.5196963430496364
f1: 0.41576829850613223
posix: 2.72729229927063
finish collecting initial predictions before optimization
all entropy: [0.6841421175434236, 0.3329369943751297, 0.4509519323755258, 0.6753695804824225, 0.6866350322126447, 0.6870849202888889, 0.952769835407555, 0.6799671738038009, 0.6423531620946674, 0.6864040100950229, 0.0, 0.681241859470388, 0.6897814027084003, 0.6910977801629237, 0.6567924418440357]
avg entropy: 0.6131685495243221
avg cont entropy: 0.7122594030752551
accuracy: 0.52
precision: 0.34647981796147315
recall: 0.5196963430496364
f1: 0.41576829850613223
posix: 2.72729229927063
finish collecting initial predictions before optimization
all entropy: [0.6841421175434236, 0.3329369943751297, 0.4509519323755258, 0.6753695804824225, 0.6866350322126447, 0.6870849202888889, 0.952769835407555, 0.6799671738038009, 0.6423531620946674, 0.6864040100950229, 0.0, 0.681241859470388, 0.6897814027084003, 0.6910977801629237, 0.6567924418440357]
avg entropy: 0.6131685495243221
avg cont entropy: 0.7122594030752551
accuracy: 0.52
precision: 0.34647981796147315
recall: 0.5196963430496364
f1: 0.41576829850613223
posix: 2.72729229927063
[INFO|integrations.py:502] 2025-07-07 19:44:50,488 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: parsahejabi (parsahejabi-academic-team) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/exps_ttt/wandb/run-20250707_194450-3kds7ets
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707
wandb: ⭐️ View project at https://wandb.ai/parsahejabi-academic-team/swarm-distillation
wandb: 🚀 View run at https://wandb.ai/parsahejabi-academic-team/swarm-distillation/runs/3kds7ets
{'all entropy': [0.6841421175434236, 0.3329369943751297, 0.4509519323755258, 0.6753695804824225, 0.6866350322126447, 0.6870849202888889, 0.952769835407555, 0.6799671738038009, 0.6423531620946674, 0.6864040100950229, 0.0, 0.681241859470388, 0.6897814027084003, 0.6910977801629237, 0.6567924418440357], 'avg entropy': 0.6131685495243221, 'avg cont entropy': 0.7122594030752551, 'accuracy': 0.52, 'precision': 0.34647981796147315, 'recall': 0.5196963430496364, 'f1': 0.41576829850613223, 'posix': 2.72729229927063, 'unsupervised_dev_runtime': 179.2418, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006}
[INFO|trainer.py:1746] 2025-07-07 19:44:52,079 >> ***** Running training *****
[INFO|trainer.py:1747] 2025-07-07 19:44:52,079 >>   Num examples = 1000
[INFO|trainer.py:1748] 2025-07-07 19:44:52,079 >>   Num Epochs = 25
[INFO|trainer.py:1749] 2025-07-07 19:44:52,079 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1750] 2025-07-07 19:44:52,080 >>   Total train batch size (w. parallel, distributed & accumulation) = 24
[INFO|trainer.py:1751] 2025-07-07 19:44:52,080 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1752] 2025-07-07 19:44:52,080 >>   Total optimization steps = 1000
{'loss': 0.0699, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.24}
[INFO|trainer.py:2661] 2025-07-07 19:47:08,759 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-10
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-07 19:47:08,761 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-10/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 19:47:37,032 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-10/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 19:47:37,036 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 19:47:37,036 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-10/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 19:47:37,065 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-10/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-07 19:47:37,070] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step10 is begin to save!
[2025-07-07 19:47:48,178] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-10/global_step10/mp_rank_00_model_states.pt
[2025-07-07 19:48:40,063] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step10 is begin to save!
[2025-07-07 19:48:51,060] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-10/global_step10/mp_rank_00_model_states.pt
{'loss': 0.0675, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.48}
[INFO|trainer.py:2661] 2025-07-07 19:52:02,655 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-20
[INFO|configuration_utils.py:425] 2025-07-07 19:52:02,657 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-20/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 19:52:29,897 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-20/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 19:52:29,900 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 19:52:29,901 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-20/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 19:52:29,930 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-20/spiece.model
[2025-07-07 19:52:29,935] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step20 is begin to save!
[2025-07-07 19:52:40,905] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-20/global_step20/mp_rank_00_model_states.pt
[2025-07-07 19:53:34,572] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step20 is begin to save!
[2025-07-07 19:53:45,366] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-20/global_step20/mp_rank_00_model_states.pt
{'loss': 0.0676, 'learning_rate': 6e-06, 'epoch': 0.72}
[INFO|trainer.py:2661] 2025-07-07 19:56:58,569 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-30
[INFO|configuration_utils.py:425] 2025-07-07 19:56:58,570 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-30/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 19:57:24,963 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-30/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 19:57:24,967 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 19:57:24,968 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-30/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 19:57:24,998 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-30/spiece.model
[2025-07-07 19:57:25,002] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step30 is begin to save!
[2025-07-07 19:57:35,564] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-30/global_step30/mp_rank_00_model_states.pt
[2025-07-07 19:58:28,788] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step30 is begin to save!
[2025-07-07 19:58:39,541] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-30/global_step30/mp_rank_00_model_states.pt
{'loss': 0.0669, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.96}
[INFO|trainer.py:2661] 2025-07-07 20:01:48,735 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-40
[INFO|configuration_utils.py:425] 2025-07-07 20:01:48,736 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-40/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 20:02:14,651 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-40/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 20:02:14,654 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 20:02:14,655 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-40/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 20:02:14,684 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-40/spiece.model
[2025-07-07 20:02:14,689] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step40 is begin to save!
[2025-07-07 20:02:25,431] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-40/global_step40/mp_rank_00_model_states.pt
[2025-07-07 20:03:15,553] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step40 is begin to save!
[2025-07-07 20:03:26,369] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-40/global_step40/mp_rank_00_model_states.pt
{'loss': 0.075, 'learning_rate': 1e-05, 'epoch': 1.22}
[INFO|trainer.py:2925] 2025-07-07 20:06:45,974 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 20:06:45,975 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 20:06:45,975 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.05), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.0, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.34), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.65, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.03), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 51.97, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.28), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.55), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.58, 'vote_ensemble_f1': 41.42, 'posix': 2.7246594429016113}
{'max_accuracy': 52.3, 'median_accuracy': 49.6, 'mean_accuracy': 47.05, 'min_accuracy': 33.3, 'std_accuracy': 6.11, 'avg_ensemble_accuracy': 52.0, 'vote_ensemble_accuracy': 51.8, 'max_precision': 48.34, 'median_precision': 34.28, 'mean_precision': 35.33, 'min_precision': 11.1, 'std_precision': 8.45, 'avg_ensemble_precision': 34.65, 'vote_ensemble_precision': 34.52, 'max_recall': 52.26, 'median_recall': 49.58, 'mean_recall': 47.03, 'min_recall': 33.33, 'std_recall': 6.1, 'avg_ensemble_recall': 51.97, 'vote_ensemble_recall': 51.77, 'max_f1': 45.28, 'median_f1': 41.04, 'mean_f1': 39.55, 'min_f1': 16.65, 'std_f1': 6.27, 'avg_ensemble_f1': 41.58, 'vote_ensemble_f1': 41.42, 'posix': 2.7246594429016113, 'eval_runtime': 180.7487, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 1.22}
[INFO|trainer.py:2925] 2025-07-07 20:09:46,725 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 20:09:46,725 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 20:09:46,725 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.05), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.0, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.34), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.65, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.03), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 51.97, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.28), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.55), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.58, 'vote_ensemble_f1': 41.42, 'posix': 2.7246594429016113}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.05), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.0, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.34), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.65, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.03), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 51.97, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.28), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.55), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.58, 'vote_ensemble_f1': 41.42, 'posix': 2.7246594429016113}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.05), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.0, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.34), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.65, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.03), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 51.97, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.28), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.55), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.58, 'vote_ensemble_f1': 41.42, 'posix': 2.7246594429016113}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.05), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.0, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.34), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.65, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.03), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 51.97, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.28), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.55), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.58, 'vote_ensemble_f1': 41.42, 'posix': 2.7246594429016113}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.05), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.0, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.34), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.65, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.03), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 51.97, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.28), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.55), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.58, 'vote_ensemble_f1': 41.42, 'posix': 2.7246594429016113}
all entropy: [0.6835947251295394, 0.3260696801570385, 0.45880793684908494, 0.6749869805043389, 0.6870849202888889, 0.6877293893152671, 0.9508100090917699, 0.6796382396070921, 0.6430053354598306, 0.6864040100950229, 0.0, 0.6809293403663059, 0.6896150198775521, 0.6909675965703669, 0.658421318067677]
avg entropy: 0.613204300091985
avg cont entropy: 0.7121533771392038
accuracy: 0.52
precision: 0.34646616541353376
recall: 0.5196933460406514
f1: 0.4157575098178833
posix: 2.7246594429016113
delta all entropy: [-0.0005473924138841291, -0.0068673142180911695, 0.007856004473559164, -0.00038259997808354207, 0.00044988807624424254, 0.0006444690263781583, -0.0019598263157850626, -0.00032893419670876334, 0.0006521733651632111, 0.0, 0.0, -0.0003125191040820319, -0.000166382830848133, -0.00013018359255678735, 0.0016288762236412735]
delta avg entropy: 3.5750567663095365e-05
{'all entropy': [0.6835947251295394, 0.3260696801570385, 0.45880793684908494, 0.6749869805043389, 0.6870849202888889, 0.6877293893152671, 0.9508100090917699, 0.6796382396070921, 0.6430053354598306, 0.6864040100950229, 0.0, 0.6809293403663059, 0.6896150198775521, 0.6909675965703669, 0.658421318067677], 'avg entropy': 0.613204300091985, 'avg cont entropy': 0.7121533771392038, 'accuracy': 0.52, 'precision': 0.34646616541353376, 'recall': 0.5196933460406514, 'f1': 0.4157575098178833, 'posix': 2.7246594429016113, 'delta all entropy': [-0.0005473924138841291, -0.0068673142180911695, 0.007856004473559164, -0.00038259997808354207, 0.00044988807624424254, 0.0006444690263781583, -0.0019598263157850626, -0.00032893419670876334, 0.0006521733651632111, 0.0, 0.0, -0.0003125191040820319, -0.000166382830848133, -0.00013018359255678735, 0.0016288762236412735], 'delta avg entropy': 3.5750567663095365e-05, 'unsupervised_dev_runtime': 181.3552, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 1.22}
all entropy: [0.6835947251295394, 0.3260696801570385, 0.45880793684908494, 0.6749869805043389, 0.6870849202888889, 0.6877293893152671, 0.9508100090917699, 0.6796382396070921, 0.6430053354598306, 0.6864040100950229, 0.0, 0.6809293403663059, 0.6896150198775521, 0.6909675965703669, 0.658421318067677]
avg entropy: 0.613204300091985
avg cont entropy: 0.7121533771392038
accuracy: 0.52
precision: 0.34646616541353376
recall: 0.5196933460406514
f1: 0.4157575098178833
posix: 2.7246594429016113
delta all entropy: [-0.0005473924138841291, -0.0068673142180911695, 0.007856004473559164, -0.00038259997808354207, 0.00044988807624424254, 0.0006444690263781583, -0.0019598263157850626, -0.00032893419670876334, 0.0006521733651632111, 0.0, 0.0, -0.0003125191040820319, -0.000166382830848133, -0.00013018359255678735, 0.0016288762236412735]
delta avg entropy: 3.5750567663095365e-05
all entropy: [0.6835947251295394, 0.3260696801570385, 0.45880793684908494, 0.6749869805043389, 0.6870849202888889, 0.6877293893152671, 0.9508100090917699, 0.6796382396070921, 0.6430053354598306, 0.6864040100950229, 0.0, 0.6809293403663059, 0.6896150198775521, 0.6909675965703669, 0.658421318067677]
avg entropy: 0.613204300091985
avg cont entropy: 0.7121533771392038
accuracy: 0.52
precision: 0.34646616541353376
recall: 0.5196933460406514
f1: 0.4157575098178833
posix: 2.7246594429016113
delta all entropy: [-0.0005473924138841291, -0.0068673142180911695, 0.007856004473559164, -0.00038259997808354207, 0.00044988807624424254, 0.0006444690263781583, -0.0019598263157850626, -0.00032893419670876334, 0.0006521733651632111, 0.0, 0.0, -0.0003125191040820319, -0.000166382830848133, -0.00013018359255678735, 0.0016288762236412735]
delta avg entropy: 3.5750567663095365e-05
all entropy: [0.6835947251295394, 0.3260696801570385, 0.45880793684908494, 0.6749869805043389, 0.6870849202888889, 0.6877293893152671, 0.9508100090917699, 0.6796382396070921, 0.6430053354598306, 0.6864040100950229, 0.0, 0.6809293403663059, 0.6896150198775521, 0.6909675965703669, 0.658421318067677]
avg entropy: 0.613204300091985
avg cont entropy: 0.7121533771392038
accuracy: 0.52
precision: 0.34646616541353376
recall: 0.5196933460406514
f1: 0.4157575098178833
posix: 2.7246594429016113
delta all entropy: [-0.0005473924138841291, -0.0068673142180911695, 0.007856004473559164, -0.00038259997808354207, 0.00044988807624424254, 0.0006444690263781583, -0.0019598263157850626, -0.00032893419670876334, 0.0006521733651632111, 0.0, 0.0, -0.0003125191040820319, -0.000166382830848133, -0.00013018359255678735, 0.0016288762236412735]
delta avg entropy: 3.5750567663095365e-05
all entropy: [0.6835947251295394, 0.3260696801570385, 0.45880793684908494, 0.6749869805043389, 0.6870849202888889, 0.6877293893152671, 0.9508100090917699, 0.6796382396070921, 0.6430053354598306, 0.6864040100950229, 0.0, 0.6809293403663059, 0.6896150198775521, 0.6909675965703669, 0.658421318067677]
avg entropy: 0.613204300091985
avg cont entropy: 0.7121533771392038
accuracy: 0.52
precision: 0.34646616541353376
recall: 0.5196933460406514
f1: 0.4157575098178833
posix: 2.7246594429016113
delta all entropy: [-0.0005473924138841291, -0.0068673142180911695, 0.007856004473559164, -0.00038259997808354207, 0.00044988807624424254, 0.0006444690263781583, -0.0019598263157850626, -0.00032893419670876334, 0.0006521733651632111, 0.0, 0.0, -0.0003125191040820319, -0.000166382830848133, -0.00013018359255678735, 0.0016288762236412735]
delta avg entropy: 3.5750567663095365e-05
all entropy: [0.6835947251295394, 0.3260696801570385, 0.45880793684908494, 0.6749869805043389, 0.6870849202888889, 0.6877293893152671, 0.9508100090917699, 0.6796382396070921, 0.6430053354598306, 0.6864040100950229, 0.0, 0.6809293403663059, 0.6896150198775521, 0.6909675965703669, 0.658421318067677]
avg entropy: 0.613204300091985
avg cont entropy: 0.7121533771392038
accuracy: 0.52
precision: 0.34646616541353376
recall: 0.5196933460406514
f1: 0.4157575098178833
posix: 2.7246594429016113
delta all entropy: [-0.0005473924138841291, -0.0068673142180911695, 0.007856004473559164, -0.00038259997808354207, 0.00044988807624424254, 0.0006444690263781583, -0.0019598263157850626, -0.00032893419670876334, 0.0006521733651632111, 0.0, 0.0, -0.0003125191040820319, -0.000166382830848133, -0.00013018359255678735, 0.0016288762236412735]
delta avg entropy: 3.5750567663095365e-05
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-07 20:12:48,297 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-50
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-07 20:12:48,298 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-50/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 20:13:14,250 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-50/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 20:13:14,254 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-50/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 20:13:14,255 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-50/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 20:13:14,287 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-50/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-07 20:13:14,292] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is begin to save!
[2025-07-07 20:13:25,143] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-50/global_step50/mp_rank_00_model_states.pt
[2025-07-07 20:14:17,395] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is begin to save!
[2025-07-07 20:14:28,382] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-50/global_step50/mp_rank_00_model_states.pt
{'loss': 0.0684, 'learning_rate': 1.2e-05, 'epoch': 1.46}
[INFO|trainer.py:2661] 2025-07-07 20:17:46,309 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-60
[INFO|configuration_utils.py:425] 2025-07-07 20:17:46,310 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-60/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 20:18:13,341 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-60/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 20:18:13,345 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-60/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 20:18:13,346 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-60/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 20:18:13,381 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-60/spiece.model
[2025-07-07 20:18:13,385] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step60 is begin to save!
[2025-07-07 20:18:23,986] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-60/global_step60/mp_rank_00_model_states.pt
[2025-07-07 20:19:12,693] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step60 is begin to save!
[2025-07-07 20:19:23,250] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-60/global_step60/mp_rank_00_model_states.pt
{'loss': 0.0699, 'learning_rate': 1.4e-05, 'epoch': 1.69}
[INFO|trainer.py:2661] 2025-07-07 20:22:26,894 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-70
[INFO|configuration_utils.py:425] 2025-07-07 20:22:26,896 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-70/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 20:22:53,522 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-70/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 20:22:53,526 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 20:22:53,527 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-70/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 20:22:53,566 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-70/spiece.model
[2025-07-07 20:22:53,571] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step70 is begin to save!
[2025-07-07 20:23:04,346] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-70/global_step70/mp_rank_00_model_states.pt
[2025-07-07 20:23:55,797] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step70 is begin to save!
[2025-07-07 20:24:06,530] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-70/global_step70/mp_rank_00_model_states.pt
{'loss': 0.0681, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.93}
[INFO|trainer.py:2661] 2025-07-07 20:27:21,680 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-80
[INFO|configuration_utils.py:425] 2025-07-07 20:27:21,681 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-80/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 20:27:47,951 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 20:27:47,955 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 20:27:47,955 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-80/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 20:27:47,990 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-80/spiece.model
[2025-07-07 20:27:47,994] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step80 is begin to save!
[2025-07-07 20:27:58,889] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-80/global_step80/mp_rank_00_model_states.pt
[2025-07-07 20:28:52,421] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step80 is begin to save!
[2025-07-07 20:29:03,489] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-80/global_step80/mp_rank_00_model_states.pt
{'loss': 0.0757, 'learning_rate': 1.8200000000000002e-05, 'epoch': 2.19}
[INFO|trainer.py:2661] 2025-07-07 20:32:26,741 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-90
[INFO|configuration_utils.py:425] 2025-07-07 20:32:26,743 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-90/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 20:32:53,099 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-90/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 20:32:53,103 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-90/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 20:32:53,103 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-90/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 20:32:53,136 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-90/spiece.model
[2025-07-07 20:32:53,141] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step91 is begin to save!
[2025-07-07 20:33:03,980] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-90/global_step91/mp_rank_00_model_states.pt
[2025-07-07 20:33:59,700] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step91 is begin to save!
[2025-07-07 20:34:10,713] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-90/global_step91/mp_rank_00_model_states.pt
{'loss': 0.0684, 'learning_rate': 1.9977888888888892e-05, 'epoch': 2.43}
[INFO|trainer.py:2925] 2025-07-07 20:37:22,837 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 20:37:22,837 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 20:37:22,838 >>   Batch size = 100
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.5), 'mean_accuracy': np.float64(47.02), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.13), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.16), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.31), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.43), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.48), 'mean_recall': np.float64(47.0), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.12), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.09), 'median_f1': np.float64(40.89), 'mean_f1': np.float64(39.53), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.72481107711792}
{'max_accuracy': 52.4, 'median_accuracy': 49.5, 'mean_accuracy': 47.02, 'min_accuracy': 33.3, 'std_accuracy': 6.13, 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': 48.16, 'median_precision': 34.22, 'mean_precision': 35.31, 'min_precision': 11.1, 'std_precision': 8.43, 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': 52.37, 'median_recall': 49.48, 'mean_recall': 47.0, 'min_recall': 33.33, 'std_recall': 6.12, 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': 45.09, 'median_f1': 40.89, 'mean_f1': 39.53, 'min_f1': 16.65, 'std_f1': 6.26, 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.72481107711792, 'eval_runtime': 180.8103, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 2.43}
[INFO|trainer.py:2925] 2025-07-07 20:40:23,650 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 20:40:23,650 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 20:40:23,650 >>   Batch size = 100
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.5), 'mean_accuracy': np.float64(47.02), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.13), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.16), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.31), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.43), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.48), 'mean_recall': np.float64(47.0), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.12), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.09), 'median_f1': np.float64(40.89), 'mean_f1': np.float64(39.53), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.72481107711792}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.5), 'mean_accuracy': np.float64(47.02), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.13), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.16), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.31), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.43), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.48), 'mean_recall': np.float64(47.0), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.12), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.09), 'median_f1': np.float64(40.89), 'mean_f1': np.float64(39.53), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.72481107711792}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.5), 'mean_accuracy': np.float64(47.02), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.13), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.16), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.31), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.43), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.48), 'mean_recall': np.float64(47.0), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.12), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.09), 'median_f1': np.float64(40.89), 'mean_f1': np.float64(39.53), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.72481107711792}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.5), 'mean_accuracy': np.float64(47.02), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.13), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.16), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.31), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.43), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.48), 'mean_recall': np.float64(47.0), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.12), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.09), 'median_f1': np.float64(40.89), 'mean_f1': np.float64(39.53), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.72481107711792}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.5), 'mean_accuracy': np.float64(47.02), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.13), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.16), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.31), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.43), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.48), 'mean_recall': np.float64(47.0), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.12), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(45.09), 'median_f1': np.float64(40.89), 'mean_f1': np.float64(39.53), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.72481107711792}
all entropy: [0.6841421175434236, 0.32103990173601904, 0.4559539571964927, 0.6753695804824225, 0.6866350322126447, 0.6877293893152671, 0.949496412572556, 0.6796382396070921, 0.6423531620946674, 0.6861689334163774, 0.0, 0.6809293403663059, 0.6897814027084003, 0.6909675965703669, 0.6573397035288462]
avg entropy: 0.612502984623392
avg cont entropy: 0.7120911580963837
accuracy: 0.522
precision: 0.3478159786107404
recall: 0.5216953480426535
f1: 0.41737001567193577
posix: 2.72481107711792
delta all entropy: [0.0, -0.011897092639110651, 0.005002024820966899, 0.0, 0.0, 0.0006444690263781583, -0.0032734228349989847, -0.00032893419670876334, 0.0, -0.0002350766786455294, 0.0, -0.0003125191040820319, 0.0, -0.00013018359255678735, 0.0005472616848104517]
delta avg entropy: -0.0006655649009298159
{'all entropy': [0.6841421175434236, 0.32103990173601904, 0.4559539571964927, 0.6753695804824225, 0.6866350322126447, 0.6877293893152671, 0.949496412572556, 0.6796382396070921, 0.6423531620946674, 0.6861689334163774, 0.0, 0.6809293403663059, 0.6897814027084003, 0.6909675965703669, 0.6573397035288462], 'avg entropy': 0.612502984623392, 'avg cont entropy': 0.7120911580963837, 'accuracy': 0.522, 'precision': 0.3478159786107404, 'recall': 0.5216953480426535, 'f1': 0.41737001567193577, 'posix': 2.72481107711792, 'delta all entropy': [0.0, -0.011897092639110651, 0.005002024820966899, 0.0, 0.0, 0.0006444690263781583, -0.0032734228349989847, -0.00032893419670876334, 0.0, -0.0002350766786455294, 0.0, -0.0003125191040820319, 0.0, -0.00013018359255678735, 0.0005472616848104517], 'delta avg entropy': -0.0006655649009298159, 'unsupervised_dev_runtime': 181.1884, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 2.43}
all entropy: [0.6841421175434236, 0.32103990173601904, 0.4559539571964927, 0.6753695804824225, 0.6866350322126447, 0.6877293893152671, 0.949496412572556, 0.6796382396070921, 0.6423531620946674, 0.6861689334163774, 0.0, 0.6809293403663059, 0.6897814027084003, 0.6909675965703669, 0.6573397035288462]
avg entropy: 0.612502984623392
avg cont entropy: 0.7120911580963837
accuracy: 0.522
precision: 0.3478159786107404
recall: 0.5216953480426535
f1: 0.41737001567193577
posix: 2.72481107711792
delta all entropy: [0.0, -0.011897092639110651, 0.005002024820966899, 0.0, 0.0, 0.0006444690263781583, -0.0032734228349989847, -0.00032893419670876334, 0.0, -0.0002350766786455294, 0.0, -0.0003125191040820319, 0.0, -0.00013018359255678735, 0.0005472616848104517]
delta avg entropy: -0.0006655649009298159
all entropy: [0.6841421175434236, 0.32103990173601904, 0.4559539571964927, 0.6753695804824225, 0.6866350322126447, 0.6877293893152671, 0.949496412572556, 0.6796382396070921, 0.6423531620946674, 0.6861689334163774, 0.0, 0.6809293403663059, 0.6897814027084003, 0.6909675965703669, 0.6573397035288462]
avg entropy: 0.612502984623392
avg cont entropy: 0.7120911580963837
accuracy: 0.522
precision: 0.3478159786107404
recall: 0.5216953480426535
f1: 0.41737001567193577
posix: 2.72481107711792
delta all entropy: [0.0, -0.011897092639110651, 0.005002024820966899, 0.0, 0.0, 0.0006444690263781583, -0.0032734228349989847, -0.00032893419670876334, 0.0, -0.0002350766786455294, 0.0, -0.0003125191040820319, 0.0, -0.00013018359255678735, 0.0005472616848104517]
delta avg entropy: -0.0006655649009298159
all entropy: [0.6841421175434236, 0.32103990173601904, 0.4559539571964927, 0.6753695804824225, 0.6866350322126447, 0.6877293893152671, 0.949496412572556, 0.6796382396070921, 0.6423531620946674, 0.6861689334163774, 0.0, 0.6809293403663059, 0.6897814027084003, 0.6909675965703669, 0.6573397035288462]
avg entropy: 0.612502984623392
avg cont entropy: 0.7120911580963837
accuracy: 0.522
precision: 0.3478159786107404
recall: 0.5216953480426535
f1: 0.41737001567193577
posix: 2.72481107711792
delta all entropy: [0.0, -0.011897092639110651, 0.005002024820966899, 0.0, 0.0, 0.0006444690263781583, -0.0032734228349989847, -0.00032893419670876334, 0.0, -0.0002350766786455294, 0.0, -0.0003125191040820319, 0.0, -0.00013018359255678735, 0.0005472616848104517]
delta avg entropy: -0.0006655649009298159
all entropy: [0.6841421175434236, 0.32103990173601904, 0.4559539571964927, 0.6753695804824225, 0.6866350322126447, 0.6877293893152671, 0.949496412572556, 0.6796382396070921, 0.6423531620946674, 0.6861689334163774, 0.0, 0.6809293403663059, 0.6897814027084003, 0.6909675965703669, 0.6573397035288462]
avg entropy: 0.612502984623392
avg cont entropy: 0.7120911580963837
accuracy: 0.522
precision: 0.3478159786107404
recall: 0.5216953480426535
f1: 0.41737001567193577
posix: 2.72481107711792
delta all entropy: [0.0, -0.011897092639110651, 0.005002024820966899, 0.0, 0.0, 0.0006444690263781583, -0.0032734228349989847, -0.00032893419670876334, 0.0, -0.0002350766786455294, 0.0, -0.0003125191040820319, 0.0, -0.00013018359255678735, 0.0005472616848104517]
delta avg entropy: -0.0006655649009298159
all entropy: [0.6841421175434236, 0.32103990173601904, 0.4559539571964927, 0.6753695804824225, 0.6866350322126447, 0.6877293893152671, 0.949496412572556, 0.6796382396070921, 0.6423531620946674, 0.6861689334163774, 0.0, 0.6809293403663059, 0.6897814027084003, 0.6909675965703669, 0.6573397035288462]
avg entropy: 0.612502984623392
avg cont entropy: 0.7120911580963837
accuracy: 0.522
precision: 0.3478159786107404
recall: 0.5216953480426535
f1: 0.41737001567193577
posix: 2.72481107711792
delta all entropy: [0.0, -0.011897092639110651, 0.005002024820966899, 0.0, 0.0, 0.0006444690263781583, -0.0032734228349989847, -0.00032893419670876334, 0.0, -0.0002350766786455294, 0.0, -0.0003125191040820319, 0.0, -0.00013018359255678735, 0.0005472616848104517]
delta avg entropy: -0.0006655649009298159
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-07 20:43:25,126 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-100
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-07 20:43:25,126 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-100/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 20:43:51,890 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-100/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 20:43:51,895 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 20:43:51,895 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-100/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 20:43:51,927 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-100/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-07 20:43:51,932] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step101 is begin to save!
[2025-07-07 20:44:02,740] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-100/global_step101/mp_rank_00_model_states.pt
[2025-07-07 20:44:56,717] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step101 is begin to save!
[2025-07-07 20:45:07,851] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-100/global_step101/mp_rank_00_model_states.pt
{'loss': 0.0673, 'learning_rate': 1.975677777777778e-05, 'epoch': 2.67}
[INFO|trainer.py:2661] 2025-07-07 20:48:23,135 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-110
[INFO|configuration_utils.py:425] 2025-07-07 20:48:23,136 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-110/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 20:48:48,831 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-110/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 20:48:48,835 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-110/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 20:48:48,835 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-110/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 20:48:48,868 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-110/spiece.model
[2025-07-07 20:48:48,872] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step111 is begin to save!
[2025-07-07 20:48:59,718] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-110/global_step111/mp_rank_00_model_states.pt
[2025-07-07 20:49:50,257] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step111 is begin to save!
[2025-07-07 20:50:10,791] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-110/global_step111/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 20:51:07,738 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-10] due to args.save_total_limit
{'loss': 0.0681, 'learning_rate': 1.953566666666667e-05, 'epoch': 2.91}
[INFO|trainer.py:2661] 2025-07-07 20:53:23,521 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-120
[INFO|configuration_utils.py:425] 2025-07-07 20:53:23,522 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-120/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 20:53:49,683 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 20:53:49,686 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 20:53:49,686 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-120/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 20:53:49,724 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-120/spiece.model
[2025-07-07 20:53:49,730] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step121 is begin to save!
[2025-07-07 20:54:00,272] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-120/global_step121/mp_rank_00_model_states.pt
[2025-07-07 20:54:52,577] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step121 is begin to save!
[2025-07-07 20:55:03,104] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-120/global_step121/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 20:56:03,123 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-20] due to args.save_total_limit
{'loss': 0.0747, 'learning_rate': 1.9292444444444448e-05, 'epoch': 3.17}
[INFO|trainer.py:2661] 2025-07-07 20:58:29,950 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-130
[INFO|configuration_utils.py:425] 2025-07-07 20:58:29,951 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-130/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 20:58:57,512 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-130/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 20:58:57,516 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-130/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 20:58:57,516 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-130/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 20:58:57,555 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-130/spiece.model
[2025-07-07 20:58:57,560] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step132 is begin to save!
[2025-07-07 20:59:08,284] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-130/global_step132/mp_rank_00_model_states.pt
[2025-07-07 20:59:58,126] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step132 is begin to save!
[2025-07-07 21:00:08,828] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-130/global_step132/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 21:01:06,639 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-30] due to args.save_total_limit
{'loss': 0.0676, 'learning_rate': 1.9071333333333335e-05, 'epoch': 3.41}
[INFO|trainer.py:2661] 2025-07-07 21:03:25,881 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-140
[INFO|configuration_utils.py:425] 2025-07-07 21:03:25,882 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-140/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 21:03:53,569 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-140/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 21:03:53,573 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 21:03:53,573 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-140/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 21:03:53,610 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-140/spiece.model
[2025-07-07 21:03:53,615] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step142 is begin to save!
[2025-07-07 21:04:04,483] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-140/global_step142/mp_rank_00_model_states.pt
[2025-07-07 21:04:56,946] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step142 is begin to save!
[2025-07-07 21:05:08,130] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-140/global_step142/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 21:06:08,669 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-40] due to args.save_total_limit
{'loss': 0.0691, 'learning_rate': 1.8850222222222222e-05, 'epoch': 3.65}
[INFO|trainer.py:2925] 2025-07-07 21:08:24,979 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 21:08:24,979 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 21:08:24,979 >>   Batch size = 100
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(48.53), 'median_precision': np.float64(34.21), 'mean_precision': np.float64(35.35), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(45.58), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.58), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.7246758937835693}
{'max_accuracy': 52.2, 'median_accuracy': 49.7, 'mean_accuracy': 47.08, 'min_accuracy': 33.3, 'std_accuracy': 6.09, 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': 48.53, 'median_precision': 34.21, 'mean_precision': 35.35, 'min_precision': 11.1, 'std_precision': 8.47, 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.59, 'max_recall': 52.17, 'median_recall': 49.68, 'mean_recall': 47.06, 'min_recall': 33.33, 'std_recall': 6.08, 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': 45.58, 'median_f1': 41.04, 'mean_f1': 39.58, 'min_f1': 16.65, 'std_f1': 6.29, 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.7246758937835693, 'eval_runtime': 180.393, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 3.65}
[INFO|trainer.py:2925] 2025-07-07 21:11:25,374 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 21:11:25,374 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 21:11:25,374 >>   Batch size = 100
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(48.53), 'median_precision': np.float64(34.21), 'mean_precision': np.float64(35.35), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(45.58), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.58), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.7246758937835693}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(48.53), 'median_precision': np.float64(34.21), 'mean_precision': np.float64(35.35), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(45.58), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.58), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.7246758937835693}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(48.53), 'median_precision': np.float64(34.21), 'mean_precision': np.float64(35.35), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(45.58), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.58), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.7246758937835693}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(48.53), 'median_precision': np.float64(34.21), 'mean_precision': np.float64(35.35), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(45.58), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.58), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.7246758937835693}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(48.53), 'median_precision': np.float64(34.21), 'mean_precision': np.float64(35.35), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(45.58), 'median_f1': np.float64(41.04), 'mean_f1': np.float64(39.58), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.7246758937835693}
all entropy: [0.6846732173608203, 0.3185029183530207, 0.4604744499500426, 0.674600230714884, 0.6866350322126447, 0.6873037899231933, 0.9532575851208384, 0.6799671738038009, 0.6423531620946674, 0.6859298002523728, 0.0, 0.6815502814029536, 0.6901020884407325, 0.6909675965703669, 0.6589556806830628]
avg entropy: 0.6130182004588934
avg cont entropy: 0.7122468679296586
accuracy: 0.521
precision: 0.3471411465349498
recall: 0.5206943470416524
f1: 0.4165638164472042
posix: 2.7246758937835693
delta all entropy: [0.0005310998173967407, -0.014434076022109, 0.009522517574516831, -0.0007693497675385075, 0.0, 0.0002188696343043972, 0.0004877497132833586, 0.0, 0.0, -0.00047420984265011956, 0.0, 0.000308421932565639, 0.00032068573233223763, -0.00013018359255678735, 0.0021632388390270263]
delta avg entropy: -0.00015034906542854554
{'all entropy': [0.6846732173608203, 0.3185029183530207, 0.4604744499500426, 0.674600230714884, 0.6866350322126447, 0.6873037899231933, 0.9532575851208384, 0.6799671738038009, 0.6423531620946674, 0.6859298002523728, 0.0, 0.6815502814029536, 0.6901020884407325, 0.6909675965703669, 0.6589556806830628], 'avg entropy': 0.6130182004588934, 'avg cont entropy': 0.7122468679296586, 'accuracy': 0.521, 'precision': 0.3471411465349498, 'recall': 0.5206943470416524, 'f1': 0.4165638164472042, 'posix': 2.7246758937835693, 'delta all entropy': [0.0005310998173967407, -0.014434076022109, 0.009522517574516831, -0.0007693497675385075, 0.0, 0.0002188696343043972, 0.0004877497132833586, 0.0, 0.0, -0.00047420984265011956, 0.0, 0.000308421932565639, 0.00032068573233223763, -0.00013018359255678735, 0.0021632388390270263], 'delta avg entropy': -0.00015034906542854554, 'unsupervised_dev_runtime': 180.8612, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 3.65}
all entropy: [0.6846732173608203, 0.3185029183530207, 0.4604744499500426, 0.674600230714884, 0.6866350322126447, 0.6873037899231933, 0.9532575851208384, 0.6799671738038009, 0.6423531620946674, 0.6859298002523728, 0.0, 0.6815502814029536, 0.6901020884407325, 0.6909675965703669, 0.6589556806830628]
avg entropy: 0.6130182004588934
avg cont entropy: 0.7122468679296586
accuracy: 0.521
precision: 0.3471411465349498
recall: 0.5206943470416524
f1: 0.4165638164472042
posix: 2.7246758937835693
delta all entropy: [0.0005310998173967407, -0.014434076022109, 0.009522517574516831, -0.0007693497675385075, 0.0, 0.0002188696343043972, 0.0004877497132833586, 0.0, 0.0, -0.00047420984265011956, 0.0, 0.000308421932565639, 0.00032068573233223763, -0.00013018359255678735, 0.0021632388390270263]
delta avg entropy: -0.00015034906542854554
all entropy: [0.6846732173608203, 0.3185029183530207, 0.4604744499500426, 0.674600230714884, 0.6866350322126447, 0.6873037899231933, 0.9532575851208384, 0.6799671738038009, 0.6423531620946674, 0.6859298002523728, 0.0, 0.6815502814029536, 0.6901020884407325, 0.6909675965703669, 0.6589556806830628]
avg entropy: 0.6130182004588934
avg cont entropy: 0.7122468679296586
accuracy: 0.521
precision: 0.3471411465349498
recall: 0.5206943470416524
f1: 0.4165638164472042
posix: 2.7246758937835693
delta all entropy: [0.0005310998173967407, -0.014434076022109, 0.009522517574516831, -0.0007693497675385075, 0.0, 0.0002188696343043972, 0.0004877497132833586, 0.0, 0.0, -0.00047420984265011956, 0.0, 0.000308421932565639, 0.00032068573233223763, -0.00013018359255678735, 0.0021632388390270263]
delta avg entropy: -0.00015034906542854554
all entropy: [0.6846732173608203, 0.3185029183530207, 0.4604744499500426, 0.674600230714884, 0.6866350322126447, 0.6873037899231933, 0.9532575851208384, 0.6799671738038009, 0.6423531620946674, 0.6859298002523728, 0.0, 0.6815502814029536, 0.6901020884407325, 0.6909675965703669, 0.6589556806830628]
avg entropy: 0.6130182004588934
avg cont entropy: 0.7122468679296586
accuracy: 0.521
precision: 0.3471411465349498
recall: 0.5206943470416524
f1: 0.4165638164472042
posix: 2.7246758937835693
delta all entropy: [0.0005310998173967407, -0.014434076022109, 0.009522517574516831, -0.0007693497675385075, 0.0, 0.0002188696343043972, 0.0004877497132833586, 0.0, 0.0, -0.00047420984265011956, 0.0, 0.000308421932565639, 0.00032068573233223763, -0.00013018359255678735, 0.0021632388390270263]
delta avg entropy: -0.00015034906542854554
all entropy: [0.6846732173608203, 0.3185029183530207, 0.4604744499500426, 0.674600230714884, 0.6866350322126447, 0.6873037899231933, 0.9532575851208384, 0.6799671738038009, 0.6423531620946674, 0.6859298002523728, 0.0, 0.6815502814029536, 0.6901020884407325, 0.6909675965703669, 0.6589556806830628]
avg entropy: 0.6130182004588934
avg cont entropy: 0.7122468679296586
accuracy: 0.521
precision: 0.3471411465349498
recall: 0.5206943470416524
f1: 0.4165638164472042
posix: 2.7246758937835693
delta all entropy: [0.0005310998173967407, -0.014434076022109, 0.009522517574516831, -0.0007693497675385075, 0.0, 0.0002188696343043972, 0.0004877497132833586, 0.0, 0.0, -0.00047420984265011956, 0.0, 0.000308421932565639, 0.00032068573233223763, -0.00013018359255678735, 0.0021632388390270263]
delta avg entropy: -0.00015034906542854554
all entropy: [0.6846732173608203, 0.3185029183530207, 0.4604744499500426, 0.674600230714884, 0.6866350322126447, 0.6873037899231933, 0.9532575851208384, 0.6799671738038009, 0.6423531620946674, 0.6859298002523728, 0.0, 0.6815502814029536, 0.6901020884407325, 0.6909675965703669, 0.6589556806830628]
avg entropy: 0.6130182004588934
avg cont entropy: 0.7122468679296586
accuracy: 0.521
precision: 0.3471411465349498
recall: 0.5206943470416524
f1: 0.4165638164472042
posix: 2.7246758937835693
delta all entropy: [0.0005310998173967407, -0.014434076022109, 0.009522517574516831, -0.0007693497675385075, 0.0, 0.0002188696343043972, 0.0004877497132833586, 0.0, 0.0, -0.00047420984265011956, 0.0, 0.000308421932565639, 0.00032068573233223763, -0.00013018359255678735, 0.0021632388390270263]
delta avg entropy: -0.00015034906542854554
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-07 21:14:26,468 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-150
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-07 21:14:26,469 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-150/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 21:14:53,666 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-150/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 21:14:53,672 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 21:14:53,673 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-150/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 21:14:53,706 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-150/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-07 21:14:53,712] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step152 is begin to save!
[2025-07-07 21:15:04,541] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-150/global_step152/mp_rank_00_model_states.pt
[2025-07-07 21:15:59,293] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step152 is begin to save!
[2025-07-07 21:16:10,534] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-150/global_step152/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 21:17:09,860 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-50] due to args.save_total_limit
{'loss': 0.068, 'learning_rate': 1.8629111111111113e-05, 'epoch': 3.89}
[INFO|trainer.py:2661] 2025-07-07 21:19:21,312 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-160
[INFO|configuration_utils.py:425] 2025-07-07 21:19:21,313 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-160/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 21:19:48,315 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 21:19:48,319 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 21:19:48,319 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-160/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 21:19:48,352 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-160/spiece.model
[2025-07-07 21:19:48,356] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step162 is begin to save!
[2025-07-07 21:19:59,516] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-160/global_step162/mp_rank_00_model_states.pt
[2025-07-07 21:20:54,727] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step162 is begin to save!
[2025-07-07 21:21:05,950] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-160/global_step162/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 21:22:06,840 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-60] due to args.save_total_limit
{'loss': 0.0748, 'learning_rate': 1.838588888888889e-05, 'epoch': 4.14}
[INFO|trainer.py:2661] 2025-07-07 21:24:40,678 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-170
[INFO|configuration_utils.py:425] 2025-07-07 21:24:40,680 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-170/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 21:25:09,217 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-170/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 21:25:09,221 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-170/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 21:25:09,221 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-170/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 21:25:09,263 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-170/spiece.model
[2025-07-07 21:25:09,267] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step173 is begin to save!
[2025-07-07 21:25:20,210] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-170/global_step173/mp_rank_00_model_states.pt
[2025-07-07 21:26:13,544] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step173 is begin to save!
[2025-07-07 21:26:24,440] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-170/global_step173/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 21:27:17,094 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-70] due to args.save_total_limit
{'loss': 0.0671, 'learning_rate': 1.8164777777777778e-05, 'epoch': 4.38}
[INFO|trainer.py:2661] 2025-07-07 21:29:31,609 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-180
[INFO|configuration_utils.py:425] 2025-07-07 21:29:31,611 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-180/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 21:29:57,713 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-180/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 21:29:57,717 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-180/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 21:29:57,718 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-180/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 21:29:57,758 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-180/spiece.model
[2025-07-07 21:29:57,763] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step183 is begin to save!
[2025-07-07 21:30:08,293] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-180/global_step183/mp_rank_00_model_states.pt
[2025-07-07 21:30:56,885] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step183 is begin to save!
[2025-07-07 21:31:07,409] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-180/global_step183/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 21:32:05,123 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-80] due to args.save_total_limit
{'loss': 0.0683, 'learning_rate': 1.794366666666667e-05, 'epoch': 4.62}
[INFO|trainer.py:2661] 2025-07-07 21:34:25,011 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-190
[INFO|configuration_utils.py:425] 2025-07-07 21:34:25,012 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-190/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 21:34:51,560 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-190/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 21:34:51,563 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-190/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 21:34:51,564 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-190/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 21:34:51,607 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-190/spiece.model
[2025-07-07 21:34:51,612] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step193 is begin to save!
[2025-07-07 21:35:02,153] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-190/global_step193/mp_rank_00_model_states.pt
[2025-07-07 21:35:52,965] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step193 is begin to save!
[2025-07-07 21:36:03,484] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-190/global_step193/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 21:37:02,854 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-90] due to args.save_total_limit
{'loss': 0.0699, 'learning_rate': 1.7722555555555556e-05, 'epoch': 4.86}
[INFO|trainer.py:2925] 2025-07-07 21:39:23,352 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 21:39:23,352 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 21:39:23,353 >>   Batch size = 100
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.03), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.14), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.08), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.37), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.48), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.01), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.13), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(44.76), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.56), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.7261979579925537}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.03), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.14), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.08), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.37), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.48), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.01), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.13), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(44.76), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.56), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.7261979579925537}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.03), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.14), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.08), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.37), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.48), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.01), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.13), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(44.76), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.56), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.7261979579925537}
{'max_accuracy': 52.2, 'median_accuracy': 49.7, 'mean_accuracy': 47.03, 'min_accuracy': 33.3, 'std_accuracy': 6.14, 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': 48.08, 'median_precision': 34.28, 'mean_precision': 35.37, 'min_precision': 11.1, 'std_precision': 8.48, 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': 52.17, 'median_recall': 49.68, 'mean_recall': 47.01, 'min_recall': 33.33, 'std_recall': 6.13, 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': 44.76, 'median_f1': 41.06, 'mean_f1': 39.56, 'min_f1': 16.65, 'std_f1': 6.26, 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.7261979579925537, 'eval_runtime': 181.1224, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 4.86}
[INFO|trainer.py:2925] 2025-07-07 21:42:24,478 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 21:42:24,478 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 21:42:24,478 >>   Batch size = 100
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.03), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.14), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.08), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.37), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.48), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.01), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.13), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(44.76), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.56), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.7261979579925537}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.03), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.14), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.08), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.37), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.48), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.01), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.13), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(44.76), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.56), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.7261979579925537}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.03), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.14), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 51.8, 'max_precision': np.float64(48.08), 'median_precision': np.float64(34.28), 'mean_precision': np.float64(35.37), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.48), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.52, 'max_recall': np.float64(52.17), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.01), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.13), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 51.77, 'max_f1': np.float64(44.76), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.56), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.42, 'posix': 2.7261979579925537}
all entropy: [0.6841421175434236, 0.32160229980987126, 0.4682024625803113, 0.6753695804824225, 0.6868620016589055, 0.6875186123480206, 0.954179836397482, 0.6796382396070921, 0.6403699544381998, 0.6854393565969992, 0.0, 0.6815502814029536, 0.6892701675539441, 0.690833395474842, 0.6578826606351565]
avg entropy: 0.6135240644353084
avg cont entropy: 0.7123924204186267
accuracy: 0.522
precision: 0.3478028404344194
recall: 0.5216923510336684
f1: 0.4173595973913549
posix: 2.7261979579925537
delta all entropy: [0.0, -0.01133469456525843, 0.017250530204785508, 0.0, 0.0002269694462608829, 0.00043369205913168507, 0.0014100009899270516, -0.00032893419670876334, -0.001983207656467645, -0.0009646534980237709, 0.0, 0.000308421932565639, -0.0005112351544561688, -0.0002643846880816536, 0.0010902187911208117]
delta avg entropy: 0.00035551491098634314
all entropy: [0.6841421175434236, 0.32160229980987126, 0.4682024625803113, 0.6753695804824225, 0.6868620016589055, 0.6875186123480206, 0.954179836397482, 0.6796382396070921, 0.6403699544381998, 0.6854393565969992, 0.0, 0.6815502814029536, 0.6892701675539441, 0.690833395474842, 0.6578826606351565]
avg entropy: 0.6135240644353084
avg cont entropy: 0.7123924204186267
accuracy: 0.522
precision: 0.3478028404344194
recall: 0.5216923510336684
f1: 0.4173595973913549
posix: 2.7261979579925537
delta all entropy: [0.0, -0.01133469456525843, 0.017250530204785508, 0.0, 0.0002269694462608829, 0.00043369205913168507, 0.0014100009899270516, -0.00032893419670876334, -0.001983207656467645, -0.0009646534980237709, 0.0, 0.000308421932565639, -0.0005112351544561688, -0.0002643846880816536, 0.0010902187911208117]
delta avg entropy: 0.00035551491098634314
all entropy: [0.6841421175434236, 0.32160229980987126, 0.4682024625803113, 0.6753695804824225, 0.6868620016589055, 0.6875186123480206, 0.954179836397482, 0.6796382396070921, 0.6403699544381998, 0.6854393565969992, 0.0, 0.6815502814029536, 0.6892701675539441, 0.690833395474842, 0.6578826606351565]
avg entropy: 0.6135240644353084
avg cont entropy: 0.7123924204186267
accuracy: 0.522
precision: 0.3478028404344194
recall: 0.5216923510336684
f1: 0.4173595973913549
posix: 2.7261979579925537
delta all entropy: [0.0, -0.01133469456525843, 0.017250530204785508, 0.0, 0.0002269694462608829, 0.00043369205913168507, 0.0014100009899270516, -0.00032893419670876334, -0.001983207656467645, -0.0009646534980237709, 0.0, 0.000308421932565639, -0.0005112351544561688, -0.0002643846880816536, 0.0010902187911208117]
delta avg entropy: 0.00035551491098634314
all entropy: [0.6841421175434236, 0.32160229980987126, 0.4682024625803113, 0.6753695804824225, 0.6868620016589055, 0.6875186123480206, 0.954179836397482, 0.6796382396070921, 0.6403699544381998, 0.6854393565969992, 0.0, 0.6815502814029536, 0.6892701675539441, 0.690833395474842, 0.6578826606351565]
avg entropy: 0.6135240644353084
avg cont entropy: 0.7123924204186267
accuracy: 0.522
precision: 0.3478028404344194
recall: 0.5216923510336684
f1: 0.4173595973913549
posix: 2.7261979579925537
delta all entropy: [0.0, -0.01133469456525843, 0.017250530204785508, 0.0, 0.0002269694462608829, 0.00043369205913168507, 0.0014100009899270516, -0.00032893419670876334, -0.001983207656467645, -0.0009646534980237709, 0.0, 0.000308421932565639, -0.0005112351544561688, -0.0002643846880816536, 0.0010902187911208117]
delta avg entropy: 0.00035551491098634314
all entropy: [0.6841421175434236, 0.32160229980987126, 0.4682024625803113, 0.6753695804824225, 0.6868620016589055, 0.6875186123480206, 0.954179836397482, 0.6796382396070921, 0.6403699544381998, 0.6854393565969992, 0.0, 0.6815502814029536, 0.6892701675539441, 0.690833395474842, 0.6578826606351565]
avg entropy: 0.6135240644353084
avg cont entropy: 0.7123924204186267
accuracy: 0.522
precision: 0.3478028404344194
recall: 0.5216923510336684
f1: 0.4173595973913549
posix: 2.7261979579925537
delta all entropy: [0.0, -0.01133469456525843, 0.017250530204785508, 0.0, 0.0002269694462608829, 0.00043369205913168507, 0.0014100009899270516, -0.00032893419670876334, -0.001983207656467645, -0.0009646534980237709, 0.0, 0.000308421932565639, -0.0005112351544561688, -0.0002643846880816536, 0.0010902187911208117]
delta avg entropy: 0.00035551491098634314
{'all entropy': [0.6841421175434236, 0.32160229980987126, 0.4682024625803113, 0.6753695804824225, 0.6868620016589055, 0.6875186123480206, 0.954179836397482, 0.6796382396070921, 0.6403699544381998, 0.6854393565969992, 0.0, 0.6815502814029536, 0.6892701675539441, 0.690833395474842, 0.6578826606351565], 'avg entropy': 0.6135240644353084, 'avg cont entropy': 0.7123924204186267, 'accuracy': 0.522, 'precision': 0.3478028404344194, 'recall': 0.5216923510336684, 'f1': 0.4173595973913549, 'posix': 2.7261979579925537, 'delta all entropy': [0.0, -0.01133469456525843, 0.017250530204785508, 0.0, 0.0002269694462608829, 0.00043369205913168507, 0.0014100009899270516, -0.00032893419670876334, -0.001983207656467645, -0.0009646534980237709, 0.0, 0.000308421932565639, -0.0005112351544561688, -0.0002643846880816536, 0.0010902187911208117], 'delta avg entropy': 0.00035551491098634314, 'unsupervised_dev_runtime': 181.7052, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 4.86}
all entropy: [0.6841421175434236, 0.32160229980987126, 0.4682024625803113, 0.6753695804824225, 0.6868620016589055, 0.6875186123480206, 0.954179836397482, 0.6796382396070921, 0.6403699544381998, 0.6854393565969992, 0.0, 0.6815502814029536, 0.6892701675539441, 0.690833395474842, 0.6578826606351565]
avg entropy: 0.6135240644353084
avg cont entropy: 0.7123924204186267
accuracy: 0.522
precision: 0.3478028404344194
recall: 0.5216923510336684
f1: 0.4173595973913549
posix: 2.7261979579925537
delta all entropy: [0.0, -0.01133469456525843, 0.017250530204785508, 0.0, 0.0002269694462608829, 0.00043369205913168507, 0.0014100009899270516, -0.00032893419670876334, -0.001983207656467645, -0.0009646534980237709, 0.0, 0.000308421932565639, -0.0005112351544561688, -0.0002643846880816536, 0.0010902187911208117]
delta avg entropy: 0.00035551491098634314
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-07 21:45:26,224 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-200
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-07 21:45:26,226 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-200/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 21:45:52,879 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-200/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 21:45:52,884 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 21:45:52,884 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-200/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 21:45:52,927 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-200/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-07 21:45:52,932] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step203 is begin to save!
[2025-07-07 21:46:03,915] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-200/global_step203/mp_rank_00_model_states.pt
[2025-07-07 21:46:52,790] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step203 is begin to save!
[2025-07-07 21:47:04,000] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-200/global_step203/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 21:48:01,305 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-100] due to args.save_total_limit
{'loss': 0.0695, 'learning_rate': 1.7501444444444447e-05, 'epoch': 5.12}
[INFO|trainer.py:2661] 2025-07-07 21:50:26,227 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-210
[INFO|configuration_utils.py:425] 2025-07-07 21:50:26,228 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-210/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 21:50:53,001 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-210/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 21:50:53,005 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-210/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 21:50:53,006 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-210/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 21:50:53,041 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-210/spiece.model
[2025-07-07 21:50:53,046] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step213 is begin to save!
[2025-07-07 21:51:03,711] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-210/global_step213/mp_rank_00_model_states.pt
[2025-07-07 21:51:55,630] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step213 is begin to save!
[2025-07-07 21:52:06,616] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-210/global_step213/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 21:53:06,846 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-110] due to args.save_total_limit
{'loss': 0.0659, 'learning_rate': 1.7280333333333335e-05, 'epoch': 5.36}
[INFO|trainer.py:2661] 2025-07-07 21:55:20,940 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-220
[INFO|configuration_utils.py:425] 2025-07-07 21:55:20,941 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-220/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 21:55:47,100 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-220/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 21:55:47,104 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-220/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 21:55:47,104 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-220/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 21:55:47,137 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-220/spiece.model
[2025-07-07 21:55:47,142] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step223 is begin to save!
[2025-07-07 21:55:58,070] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-220/global_step223/mp_rank_00_model_states.pt
[2025-07-07 21:56:48,513] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step223 is begin to save!
[2025-07-07 21:56:59,450] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-220/global_step223/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 21:57:55,575 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-120] due to args.save_total_limit
{'loss': 0.0646, 'learning_rate': 1.7059222222222222e-05, 'epoch': 5.6}
[INFO|trainer.py:2661] 2025-07-07 22:00:18,312 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-230
[INFO|configuration_utils.py:425] 2025-07-07 22:00:18,313 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-230/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 22:00:46,626 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-230/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 22:00:46,631 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-230/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 22:00:46,631 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-230/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 22:00:46,674 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-230/spiece.model
[2025-07-07 22:00:46,679] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step233 is begin to save!
[2025-07-07 22:00:57,429] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-230/global_step233/mp_rank_00_model_states.pt
[2025-07-07 22:01:44,981] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step233 is begin to save!
[2025-07-07 22:01:55,655] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-230/global_step233/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 22:02:46,431 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-130] due to args.save_total_limit
{'loss': 0.065, 'learning_rate': 1.6838111111111113e-05, 'epoch': 5.84}
[INFO|trainer.py:2661] 2025-07-07 22:05:05,183 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-240
[INFO|configuration_utils.py:425] 2025-07-07 22:05:05,184 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-240/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 22:05:32,063 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 22:05:32,067 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 22:05:32,067 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-240/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 22:05:32,111 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-240/spiece.model
[2025-07-07 22:05:32,116] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step243 is begin to save!
[2025-07-07 22:05:42,822] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-240/global_step243/mp_rank_00_model_states.pt
[2025-07-07 22:06:35,431] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step243 is begin to save!
[2025-07-07 22:06:46,146] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-240/global_step243/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 22:07:45,790 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-140] due to args.save_total_limit
{'loss': 0.066, 'learning_rate': 1.659488888888889e-05, 'epoch': 6.1}
[INFO|trainer.py:2925] 2025-07-07 22:10:17,326 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 22:10:17,327 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 22:10:17,327 >>   Batch size = 100
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.07), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.17), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.54), 'median_precision': np.float64(34.35), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.51), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.05), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(44.81), 'median_f1': np.float64(41.2), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.7328245639801025}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.07), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.17), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.54), 'median_precision': np.float64(34.35), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.51), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.05), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(44.81), 'median_f1': np.float64(41.2), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.7328245639801025}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.07), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.17), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.54), 'median_precision': np.float64(34.35), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.51), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.05), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(44.81), 'median_f1': np.float64(41.2), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.7328245639801025}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.07), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.17), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.54), 'median_precision': np.float64(34.35), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.51), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.05), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(44.81), 'median_f1': np.float64(41.2), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.7328245639801025}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.07), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.17), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.54), 'median_precision': np.float64(34.35), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.51), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.05), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(44.81), 'median_f1': np.float64(41.2), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.7328245639801025}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.07), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.17), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.54), 'median_precision': np.float64(34.35), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.51), 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.05), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(44.81), 'median_f1': np.float64(41.2), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.7328245639801025}
{'max_accuracy': 52.4, 'median_accuracy': 49.6, 'mean_accuracy': 47.07, 'min_accuracy': 33.3, 'std_accuracy': 6.17, 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': 48.54, 'median_precision': 34.35, 'mean_precision': 35.41, 'min_precision': 11.1, 'std_precision': 8.51, 'avg_ensemble_precision': 34.71, 'vote_ensemble_precision': 34.72, 'max_recall': 52.36, 'median_recall': 49.58, 'mean_recall': 47.05, 'min_recall': 33.33, 'std_recall': 6.16, 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': 44.81, 'median_f1': 41.2, 'mean_f1': 39.59, 'min_f1': 16.65, 'std_f1': 6.27, 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.7328245639801025, 'eval_runtime': 182.2079, 'eval_samples_per_second': 0.005, 'eval_steps_per_second': 0.005, 'epoch': 6.1}
[INFO|trainer.py:2925] 2025-07-07 22:13:19,536 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 22:13:19,537 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 22:13:19,537 >>   Batch size = 100
all entropy: [0.6838704590270674, 0.32541734025374647, 0.4706095865273051, 0.6742093278222404, 0.6864040100950229, 0.687936122542609, 0.9478201771779563, 0.6806127214870903, 0.6423531620946674, 0.6856866086439042, 0.0, 0.6815502814029536, 0.6890916952679865, 0.6905529364152048, 0.6594857532902015]
avg entropy: 0.6137066788031972
avg cont entropy: 0.7129240129478672
accuracy: 0.521
precision: 0.34712796735706686
recall: 0.5206913500326674
f1: 0.41655336850740854
posix: 2.7328245639801025
delta all entropy: [-0.0002716585163561325, -0.0075196541213832235, 0.019657654151779336, -0.0011602526601820484, -0.00023102211762171532, 0.0008512022537201513, -0.004949658229598697, 0.0006455476832893936, 0.0, -0.0007174014511187599, 0.0, 0.000308421932565639, -0.0006897074404137449, -0.0005448437477189039, 0.0026933114461658025]
delta avg entropy: 0.0005381292788751398
{'all entropy': [0.6838704590270674, 0.32541734025374647, 0.4706095865273051, 0.6742093278222404, 0.6864040100950229, 0.687936122542609, 0.9478201771779563, 0.6806127214870903, 0.6423531620946674, 0.6856866086439042, 0.0, 0.6815502814029536, 0.6890916952679865, 0.6905529364152048, 0.6594857532902015], 'avg entropy': 0.6137066788031972, 'avg cont entropy': 0.7129240129478672, 'accuracy': 0.521, 'precision': 0.34712796735706686, 'recall': 0.5206913500326674, 'f1': 0.41655336850740854, 'posix': 2.7328245639801025, 'delta all entropy': [-0.0002716585163561325, -0.0075196541213832235, 0.019657654151779336, -0.0011602526601820484, -0.00023102211762171532, 0.0008512022537201513, -0.004949658229598697, 0.0006455476832893936, 0.0, -0.0007174014511187599, 0.0, 0.000308421932565639, -0.0006897074404137449, -0.0005448437477189039, 0.0026933114461658025], 'delta avg entropy': 0.0005381292788751398, 'unsupervised_dev_runtime': 180.3198, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 6.1}
all entropy: [0.6838704590270674, 0.32541734025374647, 0.4706095865273051, 0.6742093278222404, 0.6864040100950229, 0.687936122542609, 0.9478201771779563, 0.6806127214870903, 0.6423531620946674, 0.6856866086439042, 0.0, 0.6815502814029536, 0.6890916952679865, 0.6905529364152048, 0.6594857532902015]
avg entropy: 0.6137066788031972
avg cont entropy: 0.7129240129478672
accuracy: 0.521
precision: 0.34712796735706686
recall: 0.5206913500326674
f1: 0.41655336850740854
posix: 2.7328245639801025
delta all entropy: [-0.0002716585163561325, -0.0075196541213832235, 0.019657654151779336, -0.0011602526601820484, -0.00023102211762171532, 0.0008512022537201513, -0.004949658229598697, 0.0006455476832893936, 0.0, -0.0007174014511187599, 0.0, 0.000308421932565639, -0.0006897074404137449, -0.0005448437477189039, 0.0026933114461658025]
delta avg entropy: 0.0005381292788751398
all entropy: [0.6838704590270674, 0.32541734025374647, 0.4706095865273051, 0.6742093278222404, 0.6864040100950229, 0.687936122542609, 0.9478201771779563, 0.6806127214870903, 0.6423531620946674, 0.6856866086439042, 0.0, 0.6815502814029536, 0.6890916952679865, 0.6905529364152048, 0.6594857532902015]
avg entropy: 0.6137066788031972
avg cont entropy: 0.7129240129478672
accuracy: 0.521
precision: 0.34712796735706686
recall: 0.5206913500326674
f1: 0.41655336850740854
posix: 2.7328245639801025
delta all entropy: [-0.0002716585163561325, -0.0075196541213832235, 0.019657654151779336, -0.0011602526601820484, -0.00023102211762171532, 0.0008512022537201513, -0.004949658229598697, 0.0006455476832893936, 0.0, -0.0007174014511187599, 0.0, 0.000308421932565639, -0.0006897074404137449, -0.0005448437477189039, 0.0026933114461658025]
delta avg entropy: 0.0005381292788751398
all entropy: [0.6838704590270674, 0.32541734025374647, 0.4706095865273051, 0.6742093278222404, 0.6864040100950229, 0.687936122542609, 0.9478201771779563, 0.6806127214870903, 0.6423531620946674, 0.6856866086439042, 0.0, 0.6815502814029536, 0.6890916952679865, 0.6905529364152048, 0.6594857532902015]
avg entropy: 0.6137066788031972
avg cont entropy: 0.7129240129478672
accuracy: 0.521
precision: 0.34712796735706686
recall: 0.5206913500326674
f1: 0.41655336850740854
posix: 2.7328245639801025
delta all entropy: [-0.0002716585163561325, -0.0075196541213832235, 0.019657654151779336, -0.0011602526601820484, -0.00023102211762171532, 0.0008512022537201513, -0.004949658229598697, 0.0006455476832893936, 0.0, -0.0007174014511187599, 0.0, 0.000308421932565639, -0.0006897074404137449, -0.0005448437477189039, 0.0026933114461658025]
delta avg entropy: 0.0005381292788751398
all entropy: [0.6838704590270674, 0.32541734025374647, 0.4706095865273051, 0.6742093278222404, 0.6864040100950229, 0.687936122542609, 0.9478201771779563, 0.6806127214870903, 0.6423531620946674, 0.6856866086439042, 0.0, 0.6815502814029536, 0.6890916952679865, 0.6905529364152048, 0.6594857532902015]
avg entropy: 0.6137066788031972
avg cont entropy: 0.7129240129478672
accuracy: 0.521
precision: 0.34712796735706686
recall: 0.5206913500326674
f1: 0.41655336850740854
posix: 2.7328245639801025
delta all entropy: [-0.0002716585163561325, -0.0075196541213832235, 0.019657654151779336, -0.0011602526601820484, -0.00023102211762171532, 0.0008512022537201513, -0.004949658229598697, 0.0006455476832893936, 0.0, -0.0007174014511187599, 0.0, 0.000308421932565639, -0.0006897074404137449, -0.0005448437477189039, 0.0026933114461658025]
delta avg entropy: 0.0005381292788751398
all entropy: [0.6838704590270674, 0.32541734025374647, 0.4706095865273051, 0.6742093278222404, 0.6864040100950229, 0.687936122542609, 0.9478201771779563, 0.6806127214870903, 0.6423531620946674, 0.6856866086439042, 0.0, 0.6815502814029536, 0.6890916952679865, 0.6905529364152048, 0.6594857532902015]
avg entropy: 0.6137066788031972
avg cont entropy: 0.7129240129478672
accuracy: 0.521
precision: 0.34712796735706686
recall: 0.5206913500326674
f1: 0.41655336850740854
posix: 2.7328245639801025
delta all entropy: [-0.0002716585163561325, -0.0075196541213832235, 0.019657654151779336, -0.0011602526601820484, -0.00023102211762171532, 0.0008512022537201513, -0.004949658229598697, 0.0006455476832893936, 0.0, -0.0007174014511187599, 0.0, 0.000308421932565639, -0.0006897074404137449, -0.0005448437477189039, 0.0026933114461658025]
delta avg entropy: 0.0005381292788751398
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-07 22:16:20,895 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-250
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-07 22:16:20,896 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-250/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 22:16:47,496 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 22:16:47,501 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 22:16:47,501 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-250/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 22:16:47,542 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-250/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-07 22:16:47,547] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step254 is begin to save!
[2025-07-07 22:16:58,315] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-250/global_step254/mp_rank_00_model_states.pt
[2025-07-07 22:17:48,220] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step254 is begin to save!
[2025-07-07 22:17:58,942] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-250/global_step254/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 22:18:57,149 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-150] due to args.save_total_limit
{'loss': 0.0618, 'learning_rate': 1.6373777777777778e-05, 'epoch': 6.34}
[INFO|trainer.py:2661] 2025-07-07 22:21:19,260 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-260
[INFO|configuration_utils.py:425] 2025-07-07 22:21:19,261 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-260/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 22:21:46,159 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-260/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 22:21:46,163 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-260/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 22:21:46,163 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-260/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 22:21:46,204 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-260/spiece.model
[2025-07-07 22:21:46,209] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step264 is begin to save!
[2025-07-07 22:21:56,870] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-260/global_step264/mp_rank_00_model_states.pt
[2025-07-07 22:22:49,222] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step264 is begin to save!
[2025-07-07 22:22:59,941] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-260/global_step264/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 22:24:00,176 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-160] due to args.save_total_limit
{'loss': 0.0594, 'learning_rate': 1.615266666666667e-05, 'epoch': 6.57}
[INFO|trainer.py:2661] 2025-07-07 22:26:15,503 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-270
[INFO|configuration_utils.py:425] 2025-07-07 22:26:15,504 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-270/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 22:26:43,069 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-270/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 22:26:43,073 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-270/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 22:26:43,074 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-270/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 22:26:43,112 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-270/spiece.model
[2025-07-07 22:26:43,116] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step274 is begin to save!
[2025-07-07 22:26:53,880] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-270/global_step274/mp_rank_00_model_states.pt
[2025-07-07 22:27:47,800] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step274 is begin to save!
[2025-07-07 22:27:58,996] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-270/global_step274/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 22:29:02,233 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-170] due to args.save_total_limit
{'loss': 0.0596, 'learning_rate': 1.5931555555555556e-05, 'epoch': 6.81}
[INFO|trainer.py:2661] 2025-07-07 22:31:17,123 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-280
[INFO|configuration_utils.py:425] 2025-07-07 22:31:17,124 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-280/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 22:31:44,904 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-280/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 22:31:44,908 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 22:31:44,909 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-280/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 22:31:44,950 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-280/spiece.model
[2025-07-07 22:31:44,955] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step284 is begin to save!
[2025-07-07 22:31:55,887] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-280/global_step284/mp_rank_00_model_states.pt
[2025-07-07 22:32:51,122] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step284 is begin to save!
[2025-07-07 22:33:02,248] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-280/global_step284/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 22:34:02,348 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-180] due to args.save_total_limit
{'loss': 0.0618, 'learning_rate': 1.5688333333333334e-05, 'epoch': 7.07}
[INFO|trainer.py:2661] 2025-07-07 22:36:36,001 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-290
[INFO|configuration_utils.py:425] 2025-07-07 22:36:36,002 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-290/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 22:37:04,126 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-290/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 22:37:04,130 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-290/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 22:37:04,130 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-290/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 22:37:04,176 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-290/spiece.model
[2025-07-07 22:37:04,181] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step295 is begin to save!
[2025-07-07 22:37:15,195] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-290/global_step295/mp_rank_00_model_states.pt
[2025-07-07 22:38:06,345] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step295 is begin to save!
[2025-07-07 22:38:17,276] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-290/global_step295/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 22:39:13,940 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-190] due to args.save_total_limit
{'loss': 0.0556, 'learning_rate': 1.5467222222222224e-05, 'epoch': 7.31}
[INFO|trainer.py:2925] 2025-07-07 22:41:31,315 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 22:41:31,315 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 22:41:31,315 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(46.98), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.16), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(47.78), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.2), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.3), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.27), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(46.96), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.15), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(44.64), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.46), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.23), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.745359420776367}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(46.98), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.16), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(47.78), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.2), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.3), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.27), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(46.96), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.15), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(44.64), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.46), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.23), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.745359420776367}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(46.98), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.16), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(47.78), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.2), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.3), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.27), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(46.96), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.15), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(44.64), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.46), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.23), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.745359420776367}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(46.98), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.16), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(47.78), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.2), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.3), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.27), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(46.96), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.15), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(44.64), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.46), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.23), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.745359420776367}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(46.98), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.16), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(47.78), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.2), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.3), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.27), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(46.96), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.15), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(44.64), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.46), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.23), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.745359420776367}
{'max_accuracy': 52.3, 'median_accuracy': 49.6, 'mean_accuracy': 46.98, 'min_accuracy': 33.3, 'std_accuracy': 6.16, 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': 47.78, 'median_precision': 34.22, 'mean_precision': 35.2, 'min_precision': 11.1, 'std_precision': 8.3, 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.59, 'max_recall': 52.27, 'median_recall': 49.58, 'mean_recall': 46.96, 'min_recall': 33.33, 'std_recall': 6.15, 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': 44.64, 'median_f1': 41.05, 'mean_f1': 39.46, 'min_f1': 16.65, 'std_f1': 6.23, 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.745359420776367, 'eval_runtime': 181.8563, 'eval_samples_per_second': 0.005, 'eval_steps_per_second': 0.005, 'epoch': 7.31}
[INFO|trainer.py:2925] 2025-07-07 22:44:33,173 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 22:44:33,174 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 22:44:33,174 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(46.98), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.16), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 51.9, 'max_precision': np.float64(47.78), 'median_precision': np.float64(34.22), 'mean_precision': np.float64(35.2), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.3), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.59, 'max_recall': np.float64(52.27), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(46.96), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.15), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.87, 'max_f1': np.float64(44.64), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.46), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.23), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.5, 'posix': 2.745359420776367}
all entropy: [0.6838704590270674, 0.3185029183530207, 0.4922201220781442, 0.6753695804824225, 0.6870849202888889, 0.687936122542609, 0.9424197936185839, 0.6796382396070921, 0.6423531620946674, 0.6864040100950229, 0.0, 0.6815502814029536, 0.6896150198775521, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.614431549771086
avg cont entropy: 0.713788238411264
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.745359420776367
delta all entropy: [-0.0002716585163561325, -0.014434076022109, 0.04126818970261842, 0.0, 0.00044988807624424254, 0.0008512022537201513, -0.010350041788971143, -0.00032893419670876334, 0.0, 0.0, 0.0, 0.000308421932565639, -0.000166382830848133, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.001263000246764227
all entropy: [0.6838704590270674, 0.3185029183530207, 0.4922201220781442, 0.6753695804824225, 0.6870849202888889, 0.687936122542609, 0.9424197936185839, 0.6796382396070921, 0.6423531620946674, 0.6864040100950229, 0.0, 0.6815502814029536, 0.6896150198775521, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.614431549771086
avg cont entropy: 0.713788238411264
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.745359420776367
delta all entropy: [-0.0002716585163561325, -0.014434076022109, 0.04126818970261842, 0.0, 0.00044988807624424254, 0.0008512022537201513, -0.010350041788971143, -0.00032893419670876334, 0.0, 0.0, 0.0, 0.000308421932565639, -0.000166382830848133, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.001263000246764227
all entropy: [0.6838704590270674, 0.3185029183530207, 0.4922201220781442, 0.6753695804824225, 0.6870849202888889, 0.687936122542609, 0.9424197936185839, 0.6796382396070921, 0.6423531620946674, 0.6864040100950229, 0.0, 0.6815502814029536, 0.6896150198775521, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.614431549771086
avg cont entropy: 0.713788238411264
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.745359420776367
delta all entropy: [-0.0002716585163561325, -0.014434076022109, 0.04126818970261842, 0.0, 0.00044988807624424254, 0.0008512022537201513, -0.010350041788971143, -0.00032893419670876334, 0.0, 0.0, 0.0, 0.000308421932565639, -0.000166382830848133, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.001263000246764227
{'all entropy': [0.6838704590270674, 0.3185029183530207, 0.4922201220781442, 0.6753695804824225, 0.6870849202888889, 0.687936122542609, 0.9424197936185839, 0.6796382396070921, 0.6423531620946674, 0.6864040100950229, 0.0, 0.6815502814029536, 0.6896150198775521, 0.6905529364152048, 0.6589556806830628], 'avg entropy': 0.614431549771086, 'avg cont entropy': 0.713788238411264, 'accuracy': 0.521, 'precision': 0.34715764502622976, 'recall': 0.5206913500326674, 'f1': 0.4165747356787607, 'posix': 2.745359420776367, 'delta all entropy': [-0.0002716585163561325, -0.014434076022109, 0.04126818970261842, 0.0, 0.00044988807624424254, 0.0008512022537201513, -0.010350041788971143, -0.00032893419670876334, 0.0, 0.0, 0.0, 0.000308421932565639, -0.000166382830848133, -0.0005448437477189039, 0.0021632388390270263], 'delta avg entropy': 0.001263000246764227, 'unsupervised_dev_runtime': 179.9227, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 7.31}
all entropy: [0.6838704590270674, 0.3185029183530207, 0.4922201220781442, 0.6753695804824225, 0.6870849202888889, 0.687936122542609, 0.9424197936185839, 0.6796382396070921, 0.6423531620946674, 0.6864040100950229, 0.0, 0.6815502814029536, 0.6896150198775521, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.614431549771086
avg cont entropy: 0.713788238411264
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.745359420776367
delta all entropy: [-0.0002716585163561325, -0.014434076022109, 0.04126818970261842, 0.0, 0.00044988807624424254, 0.0008512022537201513, -0.010350041788971143, -0.00032893419670876334, 0.0, 0.0, 0.0, 0.000308421932565639, -0.000166382830848133, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.001263000246764227
all entropy: [0.6838704590270674, 0.3185029183530207, 0.4922201220781442, 0.6753695804824225, 0.6870849202888889, 0.687936122542609, 0.9424197936185839, 0.6796382396070921, 0.6423531620946674, 0.6864040100950229, 0.0, 0.6815502814029536, 0.6896150198775521, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.614431549771086
avg cont entropy: 0.713788238411264
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.745359420776367
delta all entropy: [-0.0002716585163561325, -0.014434076022109, 0.04126818970261842, 0.0, 0.00044988807624424254, 0.0008512022537201513, -0.010350041788971143, -0.00032893419670876334, 0.0, 0.0, 0.0, 0.000308421932565639, -0.000166382830848133, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.001263000246764227
all entropy: [0.6838704590270674, 0.3185029183530207, 0.4922201220781442, 0.6753695804824225, 0.6870849202888889, 0.687936122542609, 0.9424197936185839, 0.6796382396070921, 0.6423531620946674, 0.6864040100950229, 0.0, 0.6815502814029536, 0.6896150198775521, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.614431549771086
avg cont entropy: 0.713788238411264
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.745359420776367
delta all entropy: [-0.0002716585163561325, -0.014434076022109, 0.04126818970261842, 0.0, 0.00044988807624424254, 0.0008512022537201513, -0.010350041788971143, -0.00032893419670876334, 0.0, 0.0, 0.0, 0.000308421932565639, -0.000166382830848133, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.001263000246764227
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-07 22:47:33,267 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-300
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-07 22:47:33,268 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-300/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 22:47:59,231 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-300/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 22:47:59,235 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 22:47:59,236 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-300/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 22:47:59,279 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-300/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-07 22:47:59,284] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step305 is begin to save!
[2025-07-07 22:48:10,026] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-300/global_step305/mp_rank_00_model_states.pt
[2025-07-07 22:49:03,659] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step305 is begin to save!
[2025-07-07 22:49:14,464] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-300/global_step305/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 22:50:11,658 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-200] due to args.save_total_limit
{'loss': 0.0532, 'learning_rate': 1.5246111111111112e-05, 'epoch': 7.55}
[INFO|trainer.py:2661] 2025-07-07 22:52:32,628 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-310
[INFO|configuration_utils.py:425] 2025-07-07 22:52:32,629 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-310/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 22:53:00,106 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-310/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 22:53:00,109 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-310/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 22:53:00,110 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-310/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 22:53:00,154 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-310/spiece.model
[2025-07-07 22:53:00,159] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step315 is begin to save!
[2025-07-07 22:53:11,176] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-310/global_step315/mp_rank_00_model_states.pt
[2025-07-07 22:54:02,793] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step315 is begin to save!
[2025-07-07 22:54:13,705] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-310/global_step315/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 22:55:12,558 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-210] due to args.save_total_limit
{'loss': 0.056, 'learning_rate': 1.5025000000000003e-05, 'epoch': 7.79}
[INFO|trainer.py:2661] 2025-07-07 22:57:31,755 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-320
[INFO|configuration_utils.py:425] 2025-07-07 22:57:31,756 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-320/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 22:57:58,548 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 22:57:58,552 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 22:57:58,552 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-320/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 22:57:58,595 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-320/spiece.model
[2025-07-07 22:57:58,600] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step325 is begin to save!
[2025-07-07 22:58:09,534] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-320/global_step325/mp_rank_00_model_states.pt
[2025-07-07 22:59:00,459] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step325 is begin to save!
[2025-07-07 22:59:11,329] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-320/global_step325/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 23:00:12,563 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-220] due to args.save_total_limit
{'loss': 0.0564, 'learning_rate': 1.4781777777777779e-05, 'epoch': 8.05}
[INFO|trainer.py:2661] 2025-07-07 23:02:40,306 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-330
[INFO|configuration_utils.py:425] 2025-07-07 23:02:40,307 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-330/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 23:03:08,348 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-330/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 23:03:08,352 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-330/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 23:03:08,353 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-330/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 23:03:08,392 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-330/spiece.model
[2025-07-07 23:03:08,397] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step336 is begin to save!
[2025-07-07 23:03:19,340] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-330/global_step336/mp_rank_00_model_states.pt
[2025-07-07 23:04:08,804] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step336 is begin to save!
[2025-07-07 23:04:20,092] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-330/global_step336/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 23:05:20,663 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-230] due to args.save_total_limit
{'loss': 0.0519, 'learning_rate': 1.456066666666667e-05, 'epoch': 8.29}
[INFO|trainer.py:2661] 2025-07-07 23:07:46,455 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-340
[INFO|configuration_utils.py:425] 2025-07-07 23:07:46,457 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-340/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 23:08:13,541 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-340/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 23:08:13,545 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 23:08:13,546 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-340/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 23:08:13,592 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-340/spiece.model
[2025-07-07 23:08:13,598] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step346 is begin to save!
[2025-07-07 23:08:24,257] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-340/global_step346/mp_rank_00_model_states.pt
[2025-07-07 23:09:15,886] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step346 is begin to save!
[2025-07-07 23:09:26,489] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-340/global_step346/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 23:10:19,913 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-240] due to args.save_total_limit
{'loss': 0.0517, 'learning_rate': 1.4339555555555558e-05, 'epoch': 8.53}
[INFO|trainer.py:2925] 2025-07-07 23:12:39,249 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 23:12:39,250 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 23:12:39,250 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.44), 'median_precision': np.float64(34.34), 'mean_precision': np.float64(35.11), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.12), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.44), 'median_f1': np.float64(40.63), 'mean_f1': np.float64(39.48), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.74, 'posix': 2.7644641399383545}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.44), 'median_precision': np.float64(34.34), 'mean_precision': np.float64(35.11), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.12), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.44), 'median_f1': np.float64(40.63), 'mean_f1': np.float64(39.48), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.74, 'posix': 2.7644641399383545}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.44), 'median_precision': np.float64(34.34), 'mean_precision': np.float64(35.11), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.12), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.44), 'median_f1': np.float64(40.63), 'mean_f1': np.float64(39.48), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.74, 'posix': 2.7644641399383545}
{'max_accuracy': 52.3, 'median_accuracy': 49.8, 'mean_accuracy': 47.08, 'min_accuracy': 33.3, 'std_accuracy': 6.19, 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.2, 'max_precision': 48.44, 'median_precision': 34.34, 'mean_precision': 35.11, 'min_precision': 11.1, 'std_precision': 8.12, 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.78, 'max_recall': 52.26, 'median_recall': 49.78, 'mean_recall': 47.06, 'min_recall': 33.33, 'std_recall': 6.17, 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.17, 'max_f1': 45.44, 'median_f1': 40.63, 'mean_f1': 39.48, 'min_f1': 16.65, 'std_f1': 6.26, 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.74, 'posix': 2.7644641399383545, 'eval_runtime': 181.3471, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 8.53}
[INFO|trainer.py:2925] 2025-07-07 23:15:40,600 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 23:15:40,600 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 23:15:40,601 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.44), 'median_precision': np.float64(34.34), 'mean_precision': np.float64(35.11), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.12), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.44), 'median_f1': np.float64(40.63), 'mean_f1': np.float64(39.48), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.74, 'posix': 2.7644641399383545}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.44), 'median_precision': np.float64(34.34), 'mean_precision': np.float64(35.11), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.12), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.44), 'median_f1': np.float64(40.63), 'mean_f1': np.float64(39.48), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.74, 'posix': 2.7644641399383545}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.08), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.44), 'median_precision': np.float64(34.34), 'mean_precision': np.float64(35.11), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.12), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.06), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.44), 'median_f1': np.float64(40.63), 'mean_f1': np.float64(39.48), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.26), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.74, 'posix': 2.7644641399383545}
all entropy: [0.6838704590270674, 0.31147921033738674, 0.5029260796091797, 0.6749869805043389, 0.6870849202888889, 0.6873037899231933, 0.9379131486430685, 0.6806127214870903, 0.6436530712983656, 0.6866350322126447, 0.0, 0.6818546087307834, 0.6890916952679865, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6144769243037019
avg cont entropy: 0.7148608311809184
accuracy: 0.522
precision: 0.3478312022375169
recall: 0.5216923510336684
f1: 0.41738001685059334
posix: 2.7644641399383545
delta all entropy: [-0.0002716585163561325, -0.021457784037742955, 0.0519741472336539, -0.00038259997808354207, 0.00044988807624424254, 0.0002188696343043972, -0.014856686764486482, 0.0006455476832893936, 0.0012999092036981574, 0.00023102211762171532, 0.0, 0.000612749260395451, -0.0006897074404137449, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0013083747793800134
all entropy: [0.6838704590270674, 0.31147921033738674, 0.5029260796091797, 0.6749869805043389, 0.6870849202888889, 0.6873037899231933, 0.9379131486430685, 0.6806127214870903, 0.6436530712983656, 0.6866350322126447, 0.0, 0.6818546087307834, 0.6890916952679865, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6144769243037019
avg cont entropy: 0.7148608311809184
accuracy: 0.522
precision: 0.3478312022375169
recall: 0.5216923510336684
f1: 0.41738001685059334
posix: 2.7644641399383545
delta all entropy: [-0.0002716585163561325, -0.021457784037742955, 0.0519741472336539, -0.00038259997808354207, 0.00044988807624424254, 0.0002188696343043972, -0.014856686764486482, 0.0006455476832893936, 0.0012999092036981574, 0.00023102211762171532, 0.0, 0.000612749260395451, -0.0006897074404137449, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0013083747793800134
all entropy: [0.6838704590270674, 0.31147921033738674, 0.5029260796091797, 0.6749869805043389, 0.6870849202888889, 0.6873037899231933, 0.9379131486430685, 0.6806127214870903, 0.6436530712983656, 0.6866350322126447, 0.0, 0.6818546087307834, 0.6890916952679865, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6144769243037019
avg cont entropy: 0.7148608311809184
accuracy: 0.522
precision: 0.3478312022375169
recall: 0.5216923510336684
f1: 0.41738001685059334
posix: 2.7644641399383545
delta all entropy: [-0.0002716585163561325, -0.021457784037742955, 0.0519741472336539, -0.00038259997808354207, 0.00044988807624424254, 0.0002188696343043972, -0.014856686764486482, 0.0006455476832893936, 0.0012999092036981574, 0.00023102211762171532, 0.0, 0.000612749260395451, -0.0006897074404137449, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0013083747793800134
all entropy: [0.6838704590270674, 0.31147921033738674, 0.5029260796091797, 0.6749869805043389, 0.6870849202888889, 0.6873037899231933, 0.9379131486430685, 0.6806127214870903, 0.6436530712983656, 0.6866350322126447, 0.0, 0.6818546087307834, 0.6890916952679865, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6144769243037019
avg cont entropy: 0.7148608311809184
accuracy: 0.522
precision: 0.3478312022375169
recall: 0.5216923510336684
f1: 0.41738001685059334
posix: 2.7644641399383545
delta all entropy: [-0.0002716585163561325, -0.021457784037742955, 0.0519741472336539, -0.00038259997808354207, 0.00044988807624424254, 0.0002188696343043972, -0.014856686764486482, 0.0006455476832893936, 0.0012999092036981574, 0.00023102211762171532, 0.0, 0.000612749260395451, -0.0006897074404137449, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0013083747793800134
all entropy: [0.6838704590270674, 0.31147921033738674, 0.5029260796091797, 0.6749869805043389, 0.6870849202888889, 0.6873037899231933, 0.9379131486430685, 0.6806127214870903, 0.6436530712983656, 0.6866350322126447, 0.0, 0.6818546087307834, 0.6890916952679865, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6144769243037019
avg cont entropy: 0.7148608311809184
accuracy: 0.522
precision: 0.3478312022375169
recall: 0.5216923510336684
f1: 0.41738001685059334
posix: 2.7644641399383545
delta all entropy: [-0.0002716585163561325, -0.021457784037742955, 0.0519741472336539, -0.00038259997808354207, 0.00044988807624424254, 0.0002188696343043972, -0.014856686764486482, 0.0006455476832893936, 0.0012999092036981574, 0.00023102211762171532, 0.0, 0.000612749260395451, -0.0006897074404137449, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0013083747793800134
{'all entropy': [0.6838704590270674, 0.31147921033738674, 0.5029260796091797, 0.6749869805043389, 0.6870849202888889, 0.6873037899231933, 0.9379131486430685, 0.6806127214870903, 0.6436530712983656, 0.6866350322126447, 0.0, 0.6818546087307834, 0.6890916952679865, 0.6902563939353337, 0.6594857532902015], 'avg entropy': 0.6144769243037019, 'avg cont entropy': 0.7148608311809184, 'accuracy': 0.522, 'precision': 0.3478312022375169, 'recall': 0.5216923510336684, 'f1': 0.41738001685059334, 'posix': 2.7644641399383545, 'delta all entropy': [-0.0002716585163561325, -0.021457784037742955, 0.0519741472336539, -0.00038259997808354207, 0.00044988807624424254, 0.0002188696343043972, -0.014856686764486482, 0.0006455476832893936, 0.0012999092036981574, 0.00023102211762171532, 0.0, 0.000612749260395451, -0.0006897074404137449, -0.0008413862275900019, 0.0026933114461658025], 'delta avg entropy': 0.0013083747793800134, 'unsupervised_dev_runtime': 180.7582, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 8.53}
all entropy: [0.6838704590270674, 0.31147921033738674, 0.5029260796091797, 0.6749869805043389, 0.6870849202888889, 0.6873037899231933, 0.9379131486430685, 0.6806127214870903, 0.6436530712983656, 0.6866350322126447, 0.0, 0.6818546087307834, 0.6890916952679865, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6144769243037019
avg cont entropy: 0.7148608311809184
accuracy: 0.522
precision: 0.3478312022375169
recall: 0.5216923510336684
f1: 0.41738001685059334
posix: 2.7644641399383545
delta all entropy: [-0.0002716585163561325, -0.021457784037742955, 0.0519741472336539, -0.00038259997808354207, 0.00044988807624424254, 0.0002188696343043972, -0.014856686764486482, 0.0006455476832893936, 0.0012999092036981574, 0.00023102211762171532, 0.0, 0.000612749260395451, -0.0006897074404137449, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0013083747793800134
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-07 23:18:41,412 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-350
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-07 23:18:41,414 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-350/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 23:19:08,526 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 23:19:08,530 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 23:19:08,531 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-350/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 23:19:08,576 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-350/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-07 23:19:08,581] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step356 is begin to save!
[2025-07-07 23:19:19,355] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-350/global_step356/mp_rank_00_model_states.pt
[2025-07-07 23:20:15,229] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step356 is begin to save!
[2025-07-07 23:20:26,036] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-350/global_step356/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 23:21:24,033 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-250] due to args.save_total_limit
{'loss': 0.0506, 'learning_rate': 1.4118444444444446e-05, 'epoch': 8.77}
[INFO|trainer.py:2661] 2025-07-07 23:23:42,489 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-360
[INFO|configuration_utils.py:425] 2025-07-07 23:23:42,491 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-360/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 23:24:09,735 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-360/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 23:24:09,739 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 23:24:09,739 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-360/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 23:24:09,784 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-360/spiece.model
[2025-07-07 23:24:09,789] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step366 is begin to save!
[2025-07-07 23:24:20,634] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-360/global_step366/mp_rank_00_model_states.pt
[2025-07-07 23:25:10,124] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step366 is begin to save!
[2025-07-07 23:25:20,918] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-360/global_step366/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 23:26:20,107 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-260] due to args.save_total_limit
{'loss': 0.0512, 'learning_rate': 1.3897333333333337e-05, 'epoch': 9.02}
[INFO|trainer.py:2661] 2025-07-07 23:28:49,445 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-370
[INFO|configuration_utils.py:425] 2025-07-07 23:28:49,447 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-370/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 23:29:17,651 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-370/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 23:29:17,655 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-370/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 23:29:17,656 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-370/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 23:29:17,699 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-370/spiece.model
[2025-07-07 23:29:17,704] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step376 is begin to save!
[2025-07-07 23:29:28,512] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-370/global_step376/mp_rank_00_model_states.pt
[2025-07-07 23:30:22,183] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step376 is begin to save!
[2025-07-07 23:30:32,965] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-370/global_step376/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 23:31:34,868 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-270] due to args.save_total_limit
{'loss': 0.0475, 'learning_rate': 1.3676222222222226e-05, 'epoch': 9.26}
[INFO|trainer.py:2661] 2025-07-07 23:33:54,126 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-380
[INFO|configuration_utils.py:425] 2025-07-07 23:33:54,127 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-380/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 23:34:20,983 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-380/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 23:34:20,988 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-380/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 23:34:20,988 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-380/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 23:34:21,030 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-380/spiece.model
[2025-07-07 23:34:21,035] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step386 is begin to save!
[2025-07-07 23:34:31,845] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-380/global_step386/mp_rank_00_model_states.pt
[2025-07-07 23:35:21,869] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step386 is begin to save!
[2025-07-07 23:35:32,842] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-380/global_step386/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 23:36:27,972 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-280] due to args.save_total_limit
{'loss': 0.0465, 'learning_rate': 1.3455111111111115e-05, 'epoch': 9.5}
[INFO|trainer.py:2661] 2025-07-07 23:38:45,880 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-390
[INFO|configuration_utils.py:425] 2025-07-07 23:38:45,881 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-390/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 23:39:12,409 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 23:39:12,413 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 23:39:12,414 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-390/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 23:39:12,458 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-390/spiece.model
[2025-07-07 23:39:12,462] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step396 is begin to save!
[2025-07-07 23:39:23,357] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-390/global_step396/mp_rank_00_model_states.pt
[2025-07-07 23:40:16,079] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step396 is begin to save!
[2025-07-07 23:40:27,333] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-390/global_step396/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 23:41:25,715 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-290] due to args.save_total_limit
{'loss': 0.0454, 'learning_rate': 1.3234e-05, 'epoch': 9.74}
[INFO|trainer.py:2925] 2025-07-07 23:43:46,993 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 23:43:46,993 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 23:43:46,993 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.15), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.89), 'median_precision': np.float64(34.24), 'mean_precision': np.float64(35.47), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.6), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.13), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.45), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.66), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.7876265048980713}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.15), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.89), 'median_precision': np.float64(34.24), 'mean_precision': np.float64(35.47), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.6), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.13), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.45), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.66), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.7876265048980713}
{'max_accuracy': 52.3, 'median_accuracy': 49.6, 'mean_accuracy': 47.15, 'min_accuracy': 33.3, 'std_accuracy': 6.11, 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': 48.89, 'median_precision': 34.24, 'mean_precision': 35.47, 'min_precision': 11.1, 'std_precision': 8.6, 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': 52.26, 'median_recall': 49.58, 'mean_recall': 47.13, 'min_recall': 33.33, 'std_recall': 6.1, 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': 45.45, 'median_f1': 41.06, 'mean_f1': 39.66, 'min_f1': 16.65, 'std_f1': 6.31, 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.7876265048980713, 'eval_runtime': 181.1768, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 9.74}
[INFO|trainer.py:2925] 2025-07-07 23:46:48,172 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-07 23:46:48,172 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-07 23:46:48,172 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.15), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.89), 'median_precision': np.float64(34.24), 'mean_precision': np.float64(35.47), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.6), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.13), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.45), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.66), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.7876265048980713}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.15), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.89), 'median_precision': np.float64(34.24), 'mean_precision': np.float64(35.47), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.6), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.13), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.45), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.66), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.7876265048980713}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.15), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.89), 'median_precision': np.float64(34.24), 'mean_precision': np.float64(35.47), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.6), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.13), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.45), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.66), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.7876265048980713}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.6), 'mean_accuracy': np.float64(47.15), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.89), 'median_precision': np.float64(34.24), 'mean_precision': np.float64(35.47), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.6), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.58), 'mean_recall': np.float64(47.13), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.45), 'median_f1': np.float64(41.06), 'mean_f1': np.float64(39.66), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.7876265048980713}
all entropy: [0.6830310220484359, 0.30632738584384805, 0.5210089039360535, 0.6757480339013497, 0.6870849202888889, 0.6875186123480206, 0.9364671497017745, 0.681241859470388, 0.6436530712983656, 0.6870849202888889, 0.0, 0.6824509896559686, 0.6892701675539441, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6153752789040976
avg cont entropy: 0.7159022502884721
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.7876265048980713
delta all entropy: [-0.0011110954949876684, -0.026609608531281637, 0.0700569715605277, 0.00037845341892728257, 0.00044988807624424254, 0.00043369205913168507, -0.01630268570578053, 0.001274685666587061, 0.0012999092036981574, 0.0006809101938659579, 0.0, 0.0012091301855806647, -0.0005112351544561688, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0022067293797755033
all entropy: [0.6830310220484359, 0.30632738584384805, 0.5210089039360535, 0.6757480339013497, 0.6870849202888889, 0.6875186123480206, 0.9364671497017745, 0.681241859470388, 0.6436530712983656, 0.6870849202888889, 0.0, 0.6824509896559686, 0.6892701675539441, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6153752789040976
avg cont entropy: 0.7159022502884721
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.7876265048980713
delta all entropy: [-0.0011110954949876684, -0.026609608531281637, 0.0700569715605277, 0.00037845341892728257, 0.00044988807624424254, 0.00043369205913168507, -0.01630268570578053, 0.001274685666587061, 0.0012999092036981574, 0.0006809101938659579, 0.0, 0.0012091301855806647, -0.0005112351544561688, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0022067293797755033
all entropy: [0.6830310220484359, 0.30632738584384805, 0.5210089039360535, 0.6757480339013497, 0.6870849202888889, 0.6875186123480206, 0.9364671497017745, 0.681241859470388, 0.6436530712983656, 0.6870849202888889, 0.0, 0.6824509896559686, 0.6892701675539441, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6153752789040976
avg cont entropy: 0.7159022502884721
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.7876265048980713
delta all entropy: [-0.0011110954949876684, -0.026609608531281637, 0.0700569715605277, 0.00037845341892728257, 0.00044988807624424254, 0.00043369205913168507, -0.01630268570578053, 0.001274685666587061, 0.0012999092036981574, 0.0006809101938659579, 0.0, 0.0012091301855806647, -0.0005112351544561688, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0022067293797755033
all entropy: [0.6830310220484359, 0.30632738584384805, 0.5210089039360535, 0.6757480339013497, 0.6870849202888889, 0.6875186123480206, 0.9364671497017745, 0.681241859470388, 0.6436530712983656, 0.6870849202888889, 0.0, 0.6824509896559686, 0.6892701675539441, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6153752789040976
avg cont entropy: 0.7159022502884721
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.7876265048980713
delta all entropy: [-0.0011110954949876684, -0.026609608531281637, 0.0700569715605277, 0.00037845341892728257, 0.00044988807624424254, 0.00043369205913168507, -0.01630268570578053, 0.001274685666587061, 0.0012999092036981574, 0.0006809101938659579, 0.0, 0.0012091301855806647, -0.0005112351544561688, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0022067293797755033
all entropy: [0.6830310220484359, 0.30632738584384805, 0.5210089039360535, 0.6757480339013497, 0.6870849202888889, 0.6875186123480206, 0.9364671497017745, 0.681241859470388, 0.6436530712983656, 0.6870849202888889, 0.0, 0.6824509896559686, 0.6892701675539441, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6153752789040976
avg cont entropy: 0.7159022502884721
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.7876265048980713
delta all entropy: [-0.0011110954949876684, -0.026609608531281637, 0.0700569715605277, 0.00037845341892728257, 0.00044988807624424254, 0.00043369205913168507, -0.01630268570578053, 0.001274685666587061, 0.0012999092036981574, 0.0006809101938659579, 0.0, 0.0012091301855806647, -0.0005112351544561688, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0022067293797755033
{'all entropy': [0.6830310220484359, 0.30632738584384805, 0.5210089039360535, 0.6757480339013497, 0.6870849202888889, 0.6875186123480206, 0.9364671497017745, 0.681241859470388, 0.6436530712983656, 0.6870849202888889, 0.0, 0.6824509896559686, 0.6892701675539441, 0.6902563939353337, 0.6594857532902015], 'avg entropy': 0.6153752789040976, 'avg cont entropy': 0.7159022502884721, 'accuracy': 0.521, 'precision': 0.34715764502622976, 'recall': 0.5206913500326674, 'f1': 0.4165747356787607, 'posix': 2.7876265048980713, 'delta all entropy': [-0.0011110954949876684, -0.026609608531281637, 0.0700569715605277, 0.00037845341892728257, 0.00044988807624424254, 0.00043369205913168507, -0.01630268570578053, 0.001274685666587061, 0.0012999092036981574, 0.0006809101938659579, 0.0, 0.0012091301855806647, -0.0005112351544561688, -0.0008413862275900019, 0.0026933114461658025], 'delta avg entropy': 0.0022067293797755033, 'unsupervised_dev_runtime': 181.3997, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 9.74}
all entropy: [0.6830310220484359, 0.30632738584384805, 0.5210089039360535, 0.6757480339013497, 0.6870849202888889, 0.6875186123480206, 0.9364671497017745, 0.681241859470388, 0.6436530712983656, 0.6870849202888889, 0.0, 0.6824509896559686, 0.6892701675539441, 0.6902563939353337, 0.6594857532902015]
avg entropy: 0.6153752789040976
avg cont entropy: 0.7159022502884721
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.7876265048980713
delta all entropy: [-0.0011110954949876684, -0.026609608531281637, 0.0700569715605277, 0.00037845341892728257, 0.00044988807624424254, 0.00043369205913168507, -0.01630268570578053, 0.001274685666587061, 0.0012999092036981574, 0.0006809101938659579, 0.0, 0.0012091301855806647, -0.0005112351544561688, -0.0008413862275900019, 0.0026933114461658025]
delta avg entropy: 0.0022067293797755033
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-07 23:49:49,613 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-400
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-07 23:49:49,614 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-400/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 23:50:18,588 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 23:50:18,592 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 23:50:18,593 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-400/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 23:50:18,642 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-400/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-07 23:50:18,647] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step406 is begin to save!
[2025-07-07 23:50:29,655] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-400/global_step406/mp_rank_00_model_states.pt
[2025-07-07 23:51:24,188] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step406 is begin to save!
[2025-07-07 23:51:35,069] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-400/global_step406/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 23:52:29,223 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-300] due to args.save_total_limit
{'loss': 0.0454, 'learning_rate': 1.301288888888889e-05, 'epoch': 9.98}
[INFO|trainer.py:2661] 2025-07-07 23:54:45,578 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-410
[INFO|configuration_utils.py:425] 2025-07-07 23:54:45,579 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-410/config.json
[INFO|modeling_utils.py:1070] 2025-07-07 23:55:12,303 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-07 23:55:12,306 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-07 23:55:12,307 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-410/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-07 23:55:12,354 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-410/spiece.model
[2025-07-07 23:55:12,359] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step416 is begin to save!
[2025-07-07 23:55:23,130] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-410/global_step416/mp_rank_00_model_states.pt
[2025-07-07 23:56:13,703] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step416 is begin to save!
[2025-07-07 23:56:24,508] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-410/global_step416/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-07 23:57:22,920 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-310] due to args.save_total_limit
{'loss': 0.0463, 'learning_rate': 1.276966666666667e-05, 'epoch': 10.24}
[INFO|trainer.py:2661] 2025-07-07 23:59:54,754 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-420
[INFO|configuration_utils.py:425] 2025-07-07 23:59:54,755 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-420/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 00:00:22,717 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-420/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 00:00:22,721 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 00:00:22,721 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-420/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 00:00:22,767 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-420/spiece.model
[2025-07-08 00:00:22,772] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step427 is begin to save!
[2025-07-08 00:00:33,710] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-420/global_step427/mp_rank_00_model_states.pt
[2025-07-08 00:01:27,962] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step427 is begin to save!
[2025-07-08 00:01:38,843] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-420/global_step427/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 00:02:39,953 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-320] due to args.save_total_limit
{'loss': 0.0422, 'learning_rate': 1.2548555555555556e-05, 'epoch': 10.48}
[INFO|trainer.py:2661] 2025-07-08 00:04:57,822 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-430
[INFO|configuration_utils.py:425] 2025-07-08 00:04:57,823 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-430/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 00:05:25,255 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-430/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 00:05:25,259 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-430/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 00:05:25,259 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-430/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 00:05:25,302 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-430/spiece.model
[2025-07-08 00:05:25,306] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step437 is begin to save!
[2025-07-08 00:05:36,115] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-430/global_step437/mp_rank_00_model_states.pt
[2025-07-08 00:06:27,827] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step437 is begin to save!
[2025-07-08 00:06:38,563] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-430/global_step437/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 00:07:37,600 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-330] due to args.save_total_limit
{'loss': 0.0425, 'learning_rate': 1.2327444444444447e-05, 'epoch': 10.72}
[INFO|trainer.py:2661] 2025-07-08 00:09:56,835 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-440
[INFO|configuration_utils.py:425] 2025-07-08 00:09:56,837 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-440/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 00:10:24,297 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-440/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 00:10:24,301 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-440/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 00:10:24,302 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-440/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 00:10:24,343 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-440/spiece.model
[2025-07-08 00:10:24,348] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step447 is begin to save!
[2025-07-08 00:10:35,339] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-440/global_step447/mp_rank_00_model_states.pt
[2025-07-08 00:11:27,640] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step447 is begin to save!
[2025-07-08 00:11:38,890] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-440/global_step447/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 00:12:38,101 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-340] due to args.save_total_limit
{'loss': 0.0424, 'learning_rate': 1.2106333333333335e-05, 'epoch': 10.96}
[INFO|trainer.py:2925] 2025-07-08 00:14:52,336 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 00:14:52,337 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 00:14:52,337 >>   Batch size = 100
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.21), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.83), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.19), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.09), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.77), 'median_f1': np.float64(41.28), 'mean_f1': np.float64(39.67), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.8158764839172363}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.21), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.83), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.19), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.09), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.77), 'median_f1': np.float64(41.28), 'mean_f1': np.float64(39.67), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.8158764839172363}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.21), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.83), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.19), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.09), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.77), 'median_f1': np.float64(41.28), 'mean_f1': np.float64(39.67), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.8158764839172363}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.21), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.83), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.19), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.09), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.77), 'median_f1': np.float64(41.28), 'mean_f1': np.float64(39.67), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.8158764839172363}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.21), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.83), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.19), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.09), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.77), 'median_f1': np.float64(41.28), 'mean_f1': np.float64(39.67), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.8158764839172363}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.21), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(48.83), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.41), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.36), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.19), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.09), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.77), 'median_f1': np.float64(41.28), 'mean_f1': np.float64(39.67), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.31), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.8158764839172363}
{'max_accuracy': 52.4, 'median_accuracy': 49.8, 'mean_accuracy': 47.21, 'min_accuracy': 33.3, 'std_accuracy': 6.11, 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.1, 'max_precision': 48.83, 'median_precision': 34.42, 'mean_precision': 35.41, 'min_precision': 11.1, 'std_precision': 8.47, 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.72, 'max_recall': 52.36, 'median_recall': 49.78, 'mean_recall': 47.19, 'min_recall': 33.33, 'std_recall': 6.09, 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.07, 'max_f1': 45.77, 'median_f1': 41.28, 'mean_f1': 39.67, 'min_f1': 16.65, 'std_f1': 6.31, 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.66, 'posix': 2.8158764839172363, 'eval_runtime': 188.7866, 'eval_samples_per_second': 0.005, 'eval_steps_per_second': 0.005, 'epoch': 10.96}
[INFO|trainer.py:2925] 2025-07-08 00:18:01,125 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 00:18:01,126 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 00:18:01,126 >>   Batch size = 100
all entropy: [0.6835947251295394, 0.31147921033738674, 0.5343616232537988, 0.6757480339013497, 0.6875186123480206, 0.6873037899231933, 0.9344296954774023, 0.6809293403663059, 0.6423531620946674, 0.6873037899231933, 0.0, 0.683314913574166, 0.6894446086193584, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.6164860081364433
avg cont entropy: 0.7172522143346707
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8158764839172363
delta all entropy: [-0.0005473924138841291, -0.021457784037742955, 0.08340969087827299, 0.00037845341892728257, 0.0008835801353759276, 0.0002188696343043972, -0.01834013993015271, 0.0009621665625050291, 0.0, 0.0008997798281703551, 0.0, 0.002073054103778005, -0.0003367940890418586, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.0033174586121213637
all entropy: [0.6835947251295394, 0.31147921033738674, 0.5343616232537988, 0.6757480339013497, 0.6875186123480206, 0.6873037899231933, 0.9344296954774023, 0.6809293403663059, 0.6423531620946674, 0.6873037899231933, 0.0, 0.683314913574166, 0.6894446086193584, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.6164860081364433
avg cont entropy: 0.7172522143346707
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8158764839172363
delta all entropy: [-0.0005473924138841291, -0.021457784037742955, 0.08340969087827299, 0.00037845341892728257, 0.0008835801353759276, 0.0002188696343043972, -0.01834013993015271, 0.0009621665625050291, 0.0, 0.0008997798281703551, 0.0, 0.002073054103778005, -0.0003367940890418586, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.0033174586121213637
all entropy: [0.6835947251295394, 0.31147921033738674, 0.5343616232537988, 0.6757480339013497, 0.6875186123480206, 0.6873037899231933, 0.9344296954774023, 0.6809293403663059, 0.6423531620946674, 0.6873037899231933, 0.0, 0.683314913574166, 0.6894446086193584, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.6164860081364433
avg cont entropy: 0.7172522143346707
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8158764839172363
delta all entropy: [-0.0005473924138841291, -0.021457784037742955, 0.08340969087827299, 0.00037845341892728257, 0.0008835801353759276, 0.0002188696343043972, -0.01834013993015271, 0.0009621665625050291, 0.0, 0.0008997798281703551, 0.0, 0.002073054103778005, -0.0003367940890418586, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.0033174586121213637
all entropy: [0.6835947251295394, 0.31147921033738674, 0.5343616232537988, 0.6757480339013497, 0.6875186123480206, 0.6873037899231933, 0.9344296954774023, 0.6809293403663059, 0.6423531620946674, 0.6873037899231933, 0.0, 0.683314913574166, 0.6894446086193584, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.6164860081364433
avg cont entropy: 0.7172522143346707
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8158764839172363
delta all entropy: [-0.0005473924138841291, -0.021457784037742955, 0.08340969087827299, 0.00037845341892728257, 0.0008835801353759276, 0.0002188696343043972, -0.01834013993015271, 0.0009621665625050291, 0.0, 0.0008997798281703551, 0.0, 0.002073054103778005, -0.0003367940890418586, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.0033174586121213637
all entropy: [0.6835947251295394, 0.31147921033738674, 0.5343616232537988, 0.6757480339013497, 0.6875186123480206, 0.6873037899231933, 0.9344296954774023, 0.6809293403663059, 0.6423531620946674, 0.6873037899231933, 0.0, 0.683314913574166, 0.6894446086193584, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.6164860081364433
avg cont entropy: 0.7172522143346707
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8158764839172363
delta all entropy: [-0.0005473924138841291, -0.021457784037742955, 0.08340969087827299, 0.00037845341892728257, 0.0008835801353759276, 0.0002188696343043972, -0.01834013993015271, 0.0009621665625050291, 0.0, 0.0008997798281703551, 0.0, 0.002073054103778005, -0.0003367940890418586, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.0033174586121213637
{'all entropy': [0.6835947251295394, 0.31147921033738674, 0.5343616232537988, 0.6757480339013497, 0.6875186123480206, 0.6873037899231933, 0.9344296954774023, 0.6809293403663059, 0.6423531620946674, 0.6873037899231933, 0.0, 0.683314913574166, 0.6894446086193584, 0.6905529364152048, 0.6589556806830628], 'avg entropy': 0.6164860081364433, 'avg cont entropy': 0.7172522143346707, 'accuracy': 0.521, 'precision': 0.34715764502622976, 'recall': 0.5206913500326674, 'f1': 0.4165747356787607, 'posix': 2.8158764839172363, 'delta all entropy': [-0.0005473924138841291, -0.021457784037742955, 0.08340969087827299, 0.00037845341892728257, 0.0008835801353759276, 0.0002188696343043972, -0.01834013993015271, 0.0009621665625050291, 0.0, 0.0008997798281703551, 0.0, 0.002073054103778005, -0.0003367940890418586, -0.0005448437477189039, 0.0021632388390270263], 'delta avg entropy': 0.0033174586121213637, 'unsupervised_dev_runtime': 179.8339, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 10.96}
all entropy: [0.6835947251295394, 0.31147921033738674, 0.5343616232537988, 0.6757480339013497, 0.6875186123480206, 0.6873037899231933, 0.9344296954774023, 0.6809293403663059, 0.6423531620946674, 0.6873037899231933, 0.0, 0.683314913574166, 0.6894446086193584, 0.6905529364152048, 0.6589556806830628]
avg entropy: 0.6164860081364433
avg cont entropy: 0.7172522143346707
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8158764839172363
delta all entropy: [-0.0005473924138841291, -0.021457784037742955, 0.08340969087827299, 0.00037845341892728257, 0.0008835801353759276, 0.0002188696343043972, -0.01834013993015271, 0.0009621665625050291, 0.0, 0.0008997798281703551, 0.0, 0.002073054103778005, -0.0003367940890418586, -0.0005448437477189039, 0.0021632388390270263]
delta avg entropy: 0.0033174586121213637
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-08 00:21:01,018 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-450
[INFO|configuration_utils.py:425] 2025-07-08 00:21:01,020 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-450/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 00:21:28,802 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-450/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 00:21:28,805 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-450/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 00:21:28,805 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-450/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 00:21:28,854 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-450/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-08 00:21:28,859] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step457 is begin to save!
[2025-07-08 00:21:39,865] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-450/global_step457/mp_rank_00_model_states.pt
[2025-07-08 00:22:28,150] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step457 is begin to save!
[2025-07-08 00:22:39,027] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-450/global_step457/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 00:23:32,565 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-350] due to args.save_total_limit
{'loss': 0.0466, 'learning_rate': 1.1863111111111114e-05, 'epoch': 11.22}
[INFO|trainer.py:2661] 2025-07-08 00:26:02,400 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-460
[INFO|configuration_utils.py:425] 2025-07-08 00:26:02,401 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-460/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 00:26:29,293 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-460/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 00:26:29,297 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-460/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 00:26:29,298 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-460/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 00:26:29,345 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-460/spiece.model
[2025-07-08 00:26:29,350] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step468 is begin to save!
[2025-07-08 00:26:40,114] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-460/global_step468/mp_rank_00_model_states.pt
[2025-07-08 00:27:32,289] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step468 is begin to save!
[2025-07-08 00:27:43,058] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-460/global_step468/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 00:28:43,372 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-360] due to args.save_total_limit
{'loss': 0.0401, 'learning_rate': 1.1642000000000001e-05, 'epoch': 11.46}
[INFO|trainer.py:2661] 2025-07-08 00:30:58,913 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-470
[INFO|configuration_utils.py:425] 2025-07-08 00:30:58,914 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-470/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 00:31:26,915 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 00:31:26,919 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 00:31:26,919 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-470/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 00:31:26,966 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-470/spiece.model
[2025-07-08 00:31:26,971] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step478 is begin to save!
[2025-07-08 00:31:37,671] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-470/global_step478/mp_rank_00_model_states.pt
[2025-07-08 00:32:31,506] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step478 is begin to save!
[2025-07-08 00:32:42,214] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-470/global_step478/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 00:33:43,133 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-370] due to args.save_total_limit
{'loss': 0.0402, 'learning_rate': 1.142088888888889e-05, 'epoch': 11.69}
[INFO|trainer.py:2661] 2025-07-08 00:36:05,087 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-480
[INFO|configuration_utils.py:425] 2025-07-08 00:36:05,088 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-480/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 00:36:33,412 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-480/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 00:36:33,415 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 00:36:33,416 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-480/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 00:36:33,459 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-480/spiece.model
[2025-07-08 00:36:33,464] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step488 is begin to save!
[2025-07-08 00:36:44,306] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-480/global_step488/mp_rank_00_model_states.pt
[2025-07-08 00:37:37,530] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step488 is begin to save!
[2025-07-08 00:37:48,227] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-480/global_step488/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 00:38:47,439 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-380] due to args.save_total_limit
{'loss': 0.0399, 'learning_rate': 1.1199777777777778e-05, 'epoch': 11.93}
[INFO|trainer.py:2661] 2025-07-08 00:41:09,499 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-490
[INFO|configuration_utils.py:425] 2025-07-08 00:41:09,500 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-490/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 00:41:37,200 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-490/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 00:41:37,204 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-490/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 00:41:37,204 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-490/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 00:41:37,246 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-490/spiece.model
[2025-07-08 00:41:37,251] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step498 is begin to save!
[2025-07-08 00:41:48,216] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-490/global_step498/mp_rank_00_model_states.pt
[2025-07-08 00:42:39,796] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step498 is begin to save!
[2025-07-08 00:42:50,935] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-490/global_step498/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 00:43:51,191 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-390] due to args.save_total_limit
{'loss': 0.0395, 'learning_rate': 1.0956555555555559e-05, 'epoch': 12.19}
[INFO|trainer.py:2925] 2025-07-08 00:46:14,650 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 00:46:14,650 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 00:46:14,650 >>   Batch size = 100
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.12), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(49.9), 'median_precision': np.float64(34.3), 'mean_precision': np.float64(35.38), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.49), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.79, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.1), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(44.8), 'median_f1': np.float64(41.14), 'mean_f1': np.float64(39.6), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.839383363723755}
{'max_accuracy': 52.4, 'median_accuracy': 49.7, 'mean_accuracy': 47.12, 'min_accuracy': 33.3, 'std_accuracy': 6.19, 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': 49.9, 'median_precision': 34.3, 'mean_precision': 35.38, 'min_precision': 11.1, 'std_precision': 8.49, 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.79, 'max_recall': 52.37, 'median_recall': 49.68, 'mean_recall': 47.1, 'min_recall': 33.33, 'std_recall': 6.17, 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': 44.8, 'median_f1': 41.14, 'mean_f1': 39.6, 'min_f1': 16.65, 'std_f1': 6.27, 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.839383363723755, 'eval_runtime': 180.6831, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 12.19}
[INFO|trainer.py:2925] 2025-07-08 00:49:15,335 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 00:49:15,335 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 00:49:15,335 >>   Batch size = 100
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.12), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(49.9), 'median_precision': np.float64(34.3), 'mean_precision': np.float64(35.38), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.49), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.79, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.1), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(44.8), 'median_f1': np.float64(41.14), 'mean_f1': np.float64(39.6), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.839383363723755}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.12), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(49.9), 'median_precision': np.float64(34.3), 'mean_precision': np.float64(35.38), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.49), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.79, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.1), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(44.8), 'median_f1': np.float64(41.14), 'mean_f1': np.float64(39.6), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.839383363723755}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.12), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(49.9), 'median_precision': np.float64(34.3), 'mean_precision': np.float64(35.38), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.49), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.79, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.1), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(44.8), 'median_f1': np.float64(41.14), 'mean_f1': np.float64(39.6), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.839383363723755}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.12), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(49.9), 'median_precision': np.float64(34.3), 'mean_precision': np.float64(35.38), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.49), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.79, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.1), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(44.8), 'median_f1': np.float64(41.14), 'mean_f1': np.float64(39.6), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.839383363723755}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.12), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.19), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(49.9), 'median_precision': np.float64(34.3), 'mean_precision': np.float64(35.38), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.49), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.79, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.1), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.17), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(44.8), 'median_f1': np.float64(41.14), 'mean_f1': np.float64(39.6), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.27), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.839383363723755}
all entropy: [0.6824509896559686, 0.29390383761337363, 0.5609199711525013, 0.6761223439739077, 0.6873037899231933, 0.6877293893152671, 0.9328410535373317, 0.6815502814029536, 0.6455697149230675, 0.6875186123480206, 0.0, 0.683314913574166, 0.6883374644776967, 0.6902563939353337, 0.6589556806830628]
avg entropy: 0.6171182957677229
avg cont entropy: 0.7183230295223252
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.839383363723755
delta all entropy: [-0.0016911278874549573, -0.03903315676175606, 0.10996803877697547, 0.000752763491485231, 0.0006687577105486397, 0.0006444690263781583, -0.01992878187022329, 0.0015831075991527, 0.0032165528284000544, 0.001114602252997643, 0.0, 0.002073054103778005, -0.0014439382307035453, -0.0008413862275900019, 0.0021632388390270263]
delta avg entropy: 0.003949746243401006
all entropy: [0.6824509896559686, 0.29390383761337363, 0.5609199711525013, 0.6761223439739077, 0.6873037899231933, 0.6877293893152671, 0.9328410535373317, 0.6815502814029536, 0.6455697149230675, 0.6875186123480206, 0.0, 0.683314913574166, 0.6883374644776967, 0.6902563939353337, 0.6589556806830628]
avg entropy: 0.6171182957677229
avg cont entropy: 0.7183230295223252
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.839383363723755
delta all entropy: [-0.0016911278874549573, -0.03903315676175606, 0.10996803877697547, 0.000752763491485231, 0.0006687577105486397, 0.0006444690263781583, -0.01992878187022329, 0.0015831075991527, 0.0032165528284000544, 0.001114602252997643, 0.0, 0.002073054103778005, -0.0014439382307035453, -0.0008413862275900019, 0.0021632388390270263]
delta avg entropy: 0.003949746243401006
all entropy: [0.6824509896559686, 0.29390383761337363, 0.5609199711525013, 0.6761223439739077, 0.6873037899231933, 0.6877293893152671, 0.9328410535373317, 0.6815502814029536, 0.6455697149230675, 0.6875186123480206, 0.0, 0.683314913574166, 0.6883374644776967, 0.6902563939353337, 0.6589556806830628]
avg entropy: 0.6171182957677229
avg cont entropy: 0.7183230295223252
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.839383363723755
delta all entropy: [-0.0016911278874549573, -0.03903315676175606, 0.10996803877697547, 0.000752763491485231, 0.0006687577105486397, 0.0006444690263781583, -0.01992878187022329, 0.0015831075991527, 0.0032165528284000544, 0.001114602252997643, 0.0, 0.002073054103778005, -0.0014439382307035453, -0.0008413862275900019, 0.0021632388390270263]
delta avg entropy: 0.003949746243401006
all entropy: [0.6824509896559686, 0.29390383761337363, 0.5609199711525013, 0.6761223439739077, 0.6873037899231933, 0.6877293893152671, 0.9328410535373317, 0.6815502814029536, 0.6455697149230675, 0.6875186123480206, 0.0, 0.683314913574166, 0.6883374644776967, 0.6902563939353337, 0.6589556806830628]
avg entropy: 0.6171182957677229
avg cont entropy: 0.7183230295223252
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.839383363723755
delta all entropy: [-0.0016911278874549573, -0.03903315676175606, 0.10996803877697547, 0.000752763491485231, 0.0006687577105486397, 0.0006444690263781583, -0.01992878187022329, 0.0015831075991527, 0.0032165528284000544, 0.001114602252997643, 0.0, 0.002073054103778005, -0.0014439382307035453, -0.0008413862275900019, 0.0021632388390270263]
delta avg entropy: 0.003949746243401006
{'all entropy': [0.6824509896559686, 0.29390383761337363, 0.5609199711525013, 0.6761223439739077, 0.6873037899231933, 0.6877293893152671, 0.9328410535373317, 0.6815502814029536, 0.6455697149230675, 0.6875186123480206, 0.0, 0.683314913574166, 0.6883374644776967, 0.6902563939353337, 0.6589556806830628], 'avg entropy': 0.6171182957677229, 'avg cont entropy': 0.7183230295223252, 'accuracy': 0.521, 'precision': 0.34715764502622976, 'recall': 0.5206913500326674, 'f1': 0.4165747356787607, 'posix': 2.839383363723755, 'delta all entropy': [-0.0016911278874549573, -0.03903315676175606, 0.10996803877697547, 0.000752763491485231, 0.0006687577105486397, 0.0006444690263781583, -0.01992878187022329, 0.0015831075991527, 0.0032165528284000544, 0.001114602252997643, 0.0, 0.002073054103778005, -0.0014439382307035453, -0.0008413862275900019, 0.0021632388390270263], 'delta avg entropy': 0.003949746243401006, 'unsupervised_dev_runtime': 181.4131, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 12.19}
all entropy: [0.6824509896559686, 0.29390383761337363, 0.5609199711525013, 0.6761223439739077, 0.6873037899231933, 0.6877293893152671, 0.9328410535373317, 0.6815502814029536, 0.6455697149230675, 0.6875186123480206, 0.0, 0.683314913574166, 0.6883374644776967, 0.6902563939353337, 0.6589556806830628]
avg entropy: 0.6171182957677229
avg cont entropy: 0.7183230295223252
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.839383363723755
delta all entropy: [-0.0016911278874549573, -0.03903315676175606, 0.10996803877697547, 0.000752763491485231, 0.0006687577105486397, 0.0006444690263781583, -0.01992878187022329, 0.0015831075991527, 0.0032165528284000544, 0.001114602252997643, 0.0, 0.002073054103778005, -0.0014439382307035453, -0.0008413862275900019, 0.0021632388390270263]
delta avg entropy: 0.003949746243401006
all entropy: [0.6824509896559686, 0.29390383761337363, 0.5609199711525013, 0.6761223439739077, 0.6873037899231933, 0.6877293893152671, 0.9328410535373317, 0.6815502814029536, 0.6455697149230675, 0.6875186123480206, 0.0, 0.683314913574166, 0.6883374644776967, 0.6902563939353337, 0.6589556806830628]
avg entropy: 0.6171182957677229
avg cont entropy: 0.7183230295223252
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.839383363723755
delta all entropy: [-0.0016911278874549573, -0.03903315676175606, 0.10996803877697547, 0.000752763491485231, 0.0006687577105486397, 0.0006444690263781583, -0.01992878187022329, 0.0015831075991527, 0.0032165528284000544, 0.001114602252997643, 0.0, 0.002073054103778005, -0.0014439382307035453, -0.0008413862275900019, 0.0021632388390270263]
delta avg entropy: 0.003949746243401006
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-08 00:52:17,533 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-500
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-08 00:52:17,534 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-500/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 00:52:45,086 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 00:52:45,091 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 00:52:45,091 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-500/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 00:52:45,131 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-500/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-08 00:52:45,136] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step509 is begin to save!
[2025-07-08 00:52:56,387] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-500/global_step509/mp_rank_00_model_states.pt
[2025-07-08 00:53:51,151] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step509 is begin to save!
[2025-07-08 00:54:12,169] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-500/global_step509/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 00:55:12,789 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-400] due to args.save_total_limit
{'loss': 0.0385, 'learning_rate': 1.0735444444444448e-05, 'epoch': 12.43}
[INFO|trainer.py:2661] 2025-07-08 00:57:30,032 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-510
[INFO|configuration_utils.py:425] 2025-07-08 00:57:30,034 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-510/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 00:57:56,905 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-510/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 00:57:56,909 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-510/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 00:57:56,909 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-510/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 00:57:56,958 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-510/spiece.model
[2025-07-08 00:57:56,963] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step519 is begin to save!
[2025-07-08 00:58:07,776] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-510/global_step519/mp_rank_00_model_states.pt
[2025-07-08 00:58:56,300] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step519 is begin to save!
[2025-07-08 00:59:07,088] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-510/global_step519/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 01:00:03,505 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-410] due to args.save_total_limit
{'loss': 0.0378, 'learning_rate': 1.0514333333333335e-05, 'epoch': 12.67}
[INFO|trainer.py:2661] 2025-07-08 01:02:24,626 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-520
[INFO|configuration_utils.py:425] 2025-07-08 01:02:24,628 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-520/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 01:02:51,593 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-520/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 01:02:51,596 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-520/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 01:02:51,597 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-520/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 01:02:51,645 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-520/spiece.model
[2025-07-08 01:02:51,650] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step529 is begin to save!
[2025-07-08 01:03:02,370] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-520/global_step529/mp_rank_00_model_states.pt
[2025-07-08 01:03:50,976] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step529 is begin to save!
[2025-07-08 01:04:01,723] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-520/global_step529/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 01:04:57,237 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-420] due to args.save_total_limit
{'loss': 0.0392, 'learning_rate': 1.0293222222222226e-05, 'epoch': 12.91}
[INFO|trainer.py:2661] 2025-07-08 01:07:16,213 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-530
[INFO|configuration_utils.py:425] 2025-07-08 01:07:16,215 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-530/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 01:07:43,494 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-530/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 01:07:43,498 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-530/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 01:07:43,499 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-530/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 01:07:43,544 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-530/spiece.model
[2025-07-08 01:07:43,549] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step539 is begin to save!
[2025-07-08 01:07:54,385] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-530/global_step539/mp_rank_00_model_states.pt
[2025-07-08 01:08:44,081] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step539 is begin to save!
[2025-07-08 01:08:54,869] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-530/global_step539/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 01:09:50,909 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-430] due to args.save_total_limit
{'loss': 0.0414, 'learning_rate': 1.0072111111111112e-05, 'epoch': 13.17}
[INFO|trainer.py:2661] 2025-07-08 01:12:22,160 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-540
[INFO|configuration_utils.py:425] 2025-07-08 01:12:22,161 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-540/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 01:12:49,029 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-540/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 01:12:49,033 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-540/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 01:12:49,034 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-540/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 01:12:49,077 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-540/spiece.model
[2025-07-08 01:12:49,082] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step549 is begin to save!
[2025-07-08 01:12:59,871] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-540/global_step549/mp_rank_00_model_states.pt
[2025-07-08 01:13:49,338] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step549 is begin to save!
[2025-07-08 01:14:00,344] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-540/global_step549/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 01:14:58,095 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-440] due to args.save_total_limit
{'loss': 0.0363, 'learning_rate': 9.851000000000001e-06, 'epoch': 13.41}
[INFO|trainer.py:2925] 2025-07-08 01:17:15,686 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 01:17:15,687 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 01:17:15,687 >>   Batch size = 100
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(50.0), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.0, 'max_precision': np.float64(48.9), 'median_precision': np.float64(34.23), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.42), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.65, 'max_recall': np.float64(52.16), 'median_recall': np.float64(49.98), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.97, 'max_f1': np.float64(45.69), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.58, 'posix': 2.862457275390625}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(50.0), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.0, 'max_precision': np.float64(48.9), 'median_precision': np.float64(34.23), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.42), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.65, 'max_recall': np.float64(52.16), 'median_recall': np.float64(49.98), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.97, 'max_f1': np.float64(45.69), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.58, 'posix': 2.862457275390625}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(50.0), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.0, 'max_precision': np.float64(48.9), 'median_precision': np.float64(34.23), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.42), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.65, 'max_recall': np.float64(52.16), 'median_recall': np.float64(49.98), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.97, 'max_f1': np.float64(45.69), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.58, 'posix': 2.862457275390625}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(50.0), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.0, 'max_precision': np.float64(48.9), 'median_precision': np.float64(34.23), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.42), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.65, 'max_recall': np.float64(52.16), 'median_recall': np.float64(49.98), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.97, 'max_f1': np.float64(45.69), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.58, 'posix': 2.862457275390625}
{'max_accuracy': 52.2, 'median_accuracy': 50.0, 'mean_accuracy': 47.13, 'min_accuracy': 33.3, 'std_accuracy': 6.11, 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.0, 'max_precision': 48.9, 'median_precision': 34.23, 'mean_precision': 35.33, 'min_precision': 11.1, 'std_precision': 8.42, 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.65, 'max_recall': 52.16, 'median_recall': 49.98, 'mean_recall': 47.11, 'min_recall': 33.33, 'std_recall': 6.1, 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.97, 'max_f1': 45.69, 'median_f1': 41.05, 'mean_f1': 39.59, 'min_f1': 16.65, 'std_f1': 6.29, 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.58, 'posix': 2.862457275390625, 'eval_runtime': 181.2507, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 13.41}
[INFO|trainer.py:2925] 2025-07-08 01:20:16,941 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 01:20:16,941 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 01:20:16,941 >>   Batch size = 100
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(50.0), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.0, 'max_precision': np.float64(48.9), 'median_precision': np.float64(34.23), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.42), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.65, 'max_recall': np.float64(52.16), 'median_recall': np.float64(49.98), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.97, 'max_f1': np.float64(45.69), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.58, 'posix': 2.862457275390625}
{'max_accuracy': np.float64(52.2), 'median_accuracy': np.float64(50.0), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.11), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.0, 'max_precision': np.float64(48.9), 'median_precision': np.float64(34.23), 'mean_precision': np.float64(35.33), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.42), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.65, 'max_recall': np.float64(52.16), 'median_recall': np.float64(49.98), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 51.97, 'max_f1': np.float64(45.69), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.59), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.58, 'posix': 2.862457275390625}
all entropy: [0.6824509896559686, 0.28121524052797575, 0.5643232015448391, 0.6768585467349507, 0.6873037899231933, 0.6881388137135884, 0.9284446054973319, 0.6815502814029536, 0.6430053354598306, 0.6881388137135884, 0.0, 0.6838704590270674, 0.6892701675539441, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6161489538732525
avg cont entropy: 0.719273714001109
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.862457275390625
delta all entropy: [-0.0016911278874549573, -0.05172175384715394, 0.11337126916931328, 0.0014889662525282565, 0.0006687577105486397, 0.0010538934246995257, -0.024325229910223123, 0.0015831075991527, 0.0006521733651632111, 0.0017348036185654836, 0.0, 0.0026285995566794895, -0.0005112351544561688, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.002980404348930654
all entropy: [0.6824509896559686, 0.28121524052797575, 0.5643232015448391, 0.6768585467349507, 0.6873037899231933, 0.6881388137135884, 0.9284446054973319, 0.6815502814029536, 0.6430053354598306, 0.6881388137135884, 0.0, 0.6838704590270674, 0.6892701675539441, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6161489538732525
avg cont entropy: 0.719273714001109
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.862457275390625
delta all entropy: [-0.0016911278874549573, -0.05172175384715394, 0.11337126916931328, 0.0014889662525282565, 0.0006687577105486397, 0.0010538934246995257, -0.024325229910223123, 0.0015831075991527, 0.0006521733651632111, 0.0017348036185654836, 0.0, 0.0026285995566794895, -0.0005112351544561688, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.002980404348930654
{'all entropy': [0.6824509896559686, 0.28121524052797575, 0.5643232015448391, 0.6768585467349507, 0.6873037899231933, 0.6881388137135884, 0.9284446054973319, 0.6815502814029536, 0.6430053354598306, 0.6881388137135884, 0.0, 0.6838704590270674, 0.6892701675539441, 0.6897814027084003, 0.6578826606351565], 'avg entropy': 0.6161489538732525, 'avg cont entropy': 0.719273714001109, 'accuracy': 0.521, 'precision': 0.34715764502622976, 'recall': 0.5206913500326674, 'f1': 0.4165747356787607, 'posix': 2.862457275390625, 'delta all entropy': [-0.0016911278874549573, -0.05172175384715394, 0.11337126916931328, 0.0014889662525282565, 0.0006687577105486397, 0.0010538934246995257, -0.024325229910223123, 0.0015831075991527, 0.0006521733651632111, 0.0017348036185654836, 0.0, 0.0026285995566794895, -0.0005112351544561688, -0.0013163774545233986, 0.0010902187911208117], 'delta avg entropy': 0.002980404348930654, 'unsupervised_dev_runtime': 180.6312, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 13.41}
all entropy: [0.6824509896559686, 0.28121524052797575, 0.5643232015448391, 0.6768585467349507, 0.6873037899231933, 0.6881388137135884, 0.9284446054973319, 0.6815502814029536, 0.6430053354598306, 0.6881388137135884, 0.0, 0.6838704590270674, 0.6892701675539441, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6161489538732525
avg cont entropy: 0.719273714001109
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.862457275390625
delta all entropy: [-0.0016911278874549573, -0.05172175384715394, 0.11337126916931328, 0.0014889662525282565, 0.0006687577105486397, 0.0010538934246995257, -0.024325229910223123, 0.0015831075991527, 0.0006521733651632111, 0.0017348036185654836, 0.0, 0.0026285995566794895, -0.0005112351544561688, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.002980404348930654
all entropy: [0.6824509896559686, 0.28121524052797575, 0.5643232015448391, 0.6768585467349507, 0.6873037899231933, 0.6881388137135884, 0.9284446054973319, 0.6815502814029536, 0.6430053354598306, 0.6881388137135884, 0.0, 0.6838704590270674, 0.6892701675539441, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6161489538732525
avg cont entropy: 0.719273714001109
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.862457275390625
delta all entropy: [-0.0016911278874549573, -0.05172175384715394, 0.11337126916931328, 0.0014889662525282565, 0.0006687577105486397, 0.0010538934246995257, -0.024325229910223123, 0.0015831075991527, 0.0006521733651632111, 0.0017348036185654836, 0.0, 0.0026285995566794895, -0.0005112351544561688, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.002980404348930654
all entropy: [0.6824509896559686, 0.28121524052797575, 0.5643232015448391, 0.6768585467349507, 0.6873037899231933, 0.6881388137135884, 0.9284446054973319, 0.6815502814029536, 0.6430053354598306, 0.6881388137135884, 0.0, 0.6838704590270674, 0.6892701675539441, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6161489538732525
avg cont entropy: 0.719273714001109
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.862457275390625
delta all entropy: [-0.0016911278874549573, -0.05172175384715394, 0.11337126916931328, 0.0014889662525282565, 0.0006687577105486397, 0.0010538934246995257, -0.024325229910223123, 0.0015831075991527, 0.0006521733651632111, 0.0017348036185654836, 0.0, 0.0026285995566794895, -0.0005112351544561688, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.002980404348930654
all entropy: [0.6824509896559686, 0.28121524052797575, 0.5643232015448391, 0.6768585467349507, 0.6873037899231933, 0.6881388137135884, 0.9284446054973319, 0.6815502814029536, 0.6430053354598306, 0.6881388137135884, 0.0, 0.6838704590270674, 0.6892701675539441, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6161489538732525
avg cont entropy: 0.719273714001109
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.862457275390625
delta all entropy: [-0.0016911278874549573, -0.05172175384715394, 0.11337126916931328, 0.0014889662525282565, 0.0006687577105486397, 0.0010538934246995257, -0.024325229910223123, 0.0015831075991527, 0.0006521733651632111, 0.0017348036185654836, 0.0, 0.0026285995566794895, -0.0005112351544561688, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.002980404348930654
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-08 01:23:17,842 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-550
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-08 01:23:17,843 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-550/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 01:23:45,887 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-550/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 01:23:45,891 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-550/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 01:23:45,892 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-550/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 01:23:45,933 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-550/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-08 01:23:45,938] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step559 is begin to save!
[2025-07-08 01:23:56,998] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-550/global_step559/mp_rank_00_model_states.pt
[2025-07-08 01:24:50,904] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step559 is begin to save!
[2025-07-08 01:25:02,316] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-550/global_step559/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 01:26:00,600 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-450] due to args.save_total_limit
{'loss': 0.0365, 'learning_rate': 9.629888888888892e-06, 'epoch': 13.65}
[INFO|trainer.py:2661] 2025-07-08 01:28:15,940 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-560
[INFO|configuration_utils.py:425] 2025-07-08 01:28:15,941 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-560/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 01:28:43,598 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-560/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 01:28:43,602 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-560/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 01:28:43,602 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-560/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 01:28:43,644 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-560/spiece.model
[2025-07-08 01:28:43,649] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step569 is begin to save!
[2025-07-08 01:28:54,807] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-560/global_step569/mp_rank_00_model_states.pt
[2025-07-08 01:30:03,635] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step569 is begin to save!
[2025-07-08 01:30:14,709] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-560/global_step569/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 01:31:18,529 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-460] due to args.save_total_limit
{'loss': 0.0358, 'learning_rate': 9.40877777777778e-06, 'epoch': 13.89}
[INFO|trainer.py:2661] 2025-07-08 01:33:36,185 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-570
[INFO|configuration_utils.py:425] 2025-07-08 01:33:36,186 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-570/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 01:34:02,381 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-570/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 01:34:02,385 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-570/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 01:34:02,386 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-570/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 01:34:02,435 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-570/spiece.model
[2025-07-08 01:34:02,440] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step579 is begin to save!
[2025-07-08 01:34:13,237] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-570/global_step579/mp_rank_00_model_states.pt
[2025-07-08 01:35:05,262] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step579 is begin to save!
[2025-07-08 01:35:16,044] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-570/global_step579/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 01:36:15,372 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-470] due to args.save_total_limit
{'loss': 0.0375, 'learning_rate': 9.165555555555559e-06, 'epoch': 14.14}
[INFO|trainer.py:2661] 2025-07-08 01:38:44,740 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-580
[INFO|configuration_utils.py:425] 2025-07-08 01:38:44,741 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-580/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 01:39:12,641 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-580/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 01:39:12,644 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-580/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 01:39:12,645 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-580/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 01:39:12,695 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-580/spiece.model
[2025-07-08 01:39:12,700] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step590 is begin to save!
[2025-07-08 01:39:23,632] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-580/global_step590/mp_rank_00_model_states.pt
[2025-07-08 01:40:14,157] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step590 is begin to save!
[2025-07-08 01:40:25,086] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-580/global_step590/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 01:41:23,099 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-480] due to args.save_total_limit
{'loss': 0.0356, 'learning_rate': 8.944444444444446e-06, 'epoch': 14.38}
[INFO|trainer.py:2661] 2025-07-08 01:43:43,314 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-590
[INFO|configuration_utils.py:425] 2025-07-08 01:43:43,315 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-590/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 01:44:11,943 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 01:44:11,947 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 01:44:11,948 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-590/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 01:44:12,001 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-590/spiece.model
[2025-07-08 01:44:12,006] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is begin to save!
[2025-07-08 01:44:23,125] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-590/global_step600/mp_rank_00_model_states.pt
[2025-07-08 01:45:17,988] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is begin to save!
[2025-07-08 01:45:29,000] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-590/global_step600/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 01:46:29,091 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-490] due to args.save_total_limit
{'loss': 0.034, 'learning_rate': 8.723333333333335e-06, 'epoch': 14.62}
[INFO|trainer.py:2925] 2025-07-08 01:48:51,800 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 01:48:51,800 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 01:48:51,800 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.8), 'median_precision': np.float64(34.36), 'mean_precision': np.float64(35.21), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.26), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.11), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.59), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.54), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.28), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.8836567401885986}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.8), 'median_precision': np.float64(34.36), 'mean_precision': np.float64(35.21), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.26), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.11), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.59), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.54), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.28), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.8836567401885986}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.8), 'median_precision': np.float64(34.36), 'mean_precision': np.float64(35.21), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.26), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.11), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.59), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.54), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.28), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.8836567401885986}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.8), 'median_precision': np.float64(34.36), 'mean_precision': np.float64(35.21), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.26), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.11), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.59), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.54), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.28), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.8836567401885986}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.8), 'median_precision': np.float64(34.36), 'mean_precision': np.float64(35.21), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.26), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.11), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.59), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.54), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.28), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.8836567401885986}
{'max_accuracy': 52.3, 'median_accuracy': 49.7, 'mean_accuracy': 47.13, 'min_accuracy': 33.3, 'std_accuracy': 6.12, 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': 48.8, 'median_precision': 34.36, 'mean_precision': 35.21, 'min_precision': 11.1, 'std_precision': 8.26, 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': 52.26, 'median_recall': 49.68, 'mean_recall': 47.11, 'min_recall': 33.33, 'std_recall': 6.11, 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': 45.59, 'median_f1': 41.05, 'mean_f1': 39.54, 'min_f1': 16.65, 'std_f1': 6.28, 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.8836567401885986, 'eval_runtime': 182.1326, 'eval_samples_per_second': 0.005, 'eval_steps_per_second': 0.005, 'epoch': 14.62}
[INFO|trainer.py:2925] 2025-07-08 01:51:53,936 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 01:51:53,936 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 01:51:53,936 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.7), 'mean_accuracy': np.float64(47.13), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.2, 'max_precision': np.float64(48.8), 'median_precision': np.float64(34.36), 'mean_precision': np.float64(35.21), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.26), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.78, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.68), 'mean_recall': np.float64(47.11), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.11), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.17, 'max_f1': np.float64(45.59), 'median_f1': np.float64(41.05), 'mean_f1': np.float64(39.54), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.28), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.74, 'posix': 2.8836567401885986}
all entropy: [0.6818546087307834, 0.2804952869545485, 0.5799600959621822, 0.6768585467349507, 0.6875186123480206, 0.6877293893152671, 0.9235676415560239, 0.6815502814029536, 0.6436530712983656, 0.6881388137135884, 0.0, 0.6846732173608203, 0.6889091903146469, 0.6899437584583995, 0.6589556806830628]
avg entropy: 0.6169205463222409
avg cont entropy: 0.7201636638905465
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8836567401885986
delta all entropy: [-0.002287508812640171, -0.052441707420581185, 0.12900816358665645, 0.0014889662525282565, 0.0008835801353759276, 0.0006444690263781583, -0.02920219385153111, 0.0015831075991527, 0.0012999092036981574, 0.0017348036185654836, 0.0, 0.0034313578904323627, -0.0008722123937533466, -0.0011540217045241663, 0.0021632388390270263]
delta avg entropy: 0.0037519967979189697
all entropy: [0.6818546087307834, 0.2804952869545485, 0.5799600959621822, 0.6768585467349507, 0.6875186123480206, 0.6877293893152671, 0.9235676415560239, 0.6815502814029536, 0.6436530712983656, 0.6881388137135884, 0.0, 0.6846732173608203, 0.6889091903146469, 0.6899437584583995, 0.6589556806830628]
avg entropy: 0.6169205463222409
avg cont entropy: 0.7201636638905465
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8836567401885986
delta all entropy: [-0.002287508812640171, -0.052441707420581185, 0.12900816358665645, 0.0014889662525282565, 0.0008835801353759276, 0.0006444690263781583, -0.02920219385153111, 0.0015831075991527, 0.0012999092036981574, 0.0017348036185654836, 0.0, 0.0034313578904323627, -0.0008722123937533466, -0.0011540217045241663, 0.0021632388390270263]
delta avg entropy: 0.0037519967979189697
{'all entropy': [0.6818546087307834, 0.2804952869545485, 0.5799600959621822, 0.6768585467349507, 0.6875186123480206, 0.6877293893152671, 0.9235676415560239, 0.6815502814029536, 0.6436530712983656, 0.6881388137135884, 0.0, 0.6846732173608203, 0.6889091903146469, 0.6899437584583995, 0.6589556806830628], 'avg entropy': 0.6169205463222409, 'avg cont entropy': 0.7201636638905465, 'accuracy': 0.521, 'precision': 0.34715764502622976, 'recall': 0.5206913500326674, 'f1': 0.4165747356787607, 'posix': 2.8836567401885986, 'delta all entropy': [-0.002287508812640171, -0.052441707420581185, 0.12900816358665645, 0.0014889662525282565, 0.0008835801353759276, 0.0006444690263781583, -0.02920219385153111, 0.0015831075991527, 0.0012999092036981574, 0.0017348036185654836, 0.0, 0.0034313578904323627, -0.0008722123937533466, -0.0011540217045241663, 0.0021632388390270263], 'delta avg entropy': 0.0037519967979189697, 'unsupervised_dev_runtime': 180.3685, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 14.62}
all entropy: [0.6818546087307834, 0.2804952869545485, 0.5799600959621822, 0.6768585467349507, 0.6875186123480206, 0.6877293893152671, 0.9235676415560239, 0.6815502814029536, 0.6436530712983656, 0.6881388137135884, 0.0, 0.6846732173608203, 0.6889091903146469, 0.6899437584583995, 0.6589556806830628]
avg entropy: 0.6169205463222409
avg cont entropy: 0.7201636638905465
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8836567401885986
delta all entropy: [-0.002287508812640171, -0.052441707420581185, 0.12900816358665645, 0.0014889662525282565, 0.0008835801353759276, 0.0006444690263781583, -0.02920219385153111, 0.0015831075991527, 0.0012999092036981574, 0.0017348036185654836, 0.0, 0.0034313578904323627, -0.0008722123937533466, -0.0011540217045241663, 0.0021632388390270263]
delta avg entropy: 0.0037519967979189697
all entropy: [0.6818546087307834, 0.2804952869545485, 0.5799600959621822, 0.6768585467349507, 0.6875186123480206, 0.6877293893152671, 0.9235676415560239, 0.6815502814029536, 0.6436530712983656, 0.6881388137135884, 0.0, 0.6846732173608203, 0.6889091903146469, 0.6899437584583995, 0.6589556806830628]
avg entropy: 0.6169205463222409
avg cont entropy: 0.7201636638905465
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8836567401885986
delta all entropy: [-0.002287508812640171, -0.052441707420581185, 0.12900816358665645, 0.0014889662525282565, 0.0008835801353759276, 0.0006444690263781583, -0.02920219385153111, 0.0015831075991527, 0.0012999092036981574, 0.0017348036185654836, 0.0, 0.0034313578904323627, -0.0008722123937533466, -0.0011540217045241663, 0.0021632388390270263]
delta avg entropy: 0.0037519967979189697
all entropy: [0.6818546087307834, 0.2804952869545485, 0.5799600959621822, 0.6768585467349507, 0.6875186123480206, 0.6877293893152671, 0.9235676415560239, 0.6815502814029536, 0.6436530712983656, 0.6881388137135884, 0.0, 0.6846732173608203, 0.6889091903146469, 0.6899437584583995, 0.6589556806830628]
avg entropy: 0.6169205463222409
avg cont entropy: 0.7201636638905465
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8836567401885986
delta all entropy: [-0.002287508812640171, -0.052441707420581185, 0.12900816358665645, 0.0014889662525282565, 0.0008835801353759276, 0.0006444690263781583, -0.02920219385153111, 0.0015831075991527, 0.0012999092036981574, 0.0017348036185654836, 0.0, 0.0034313578904323627, -0.0008722123937533466, -0.0011540217045241663, 0.0021632388390270263]
delta avg entropy: 0.0037519967979189697
all entropy: [0.6818546087307834, 0.2804952869545485, 0.5799600959621822, 0.6768585467349507, 0.6875186123480206, 0.6877293893152671, 0.9235676415560239, 0.6815502814029536, 0.6436530712983656, 0.6881388137135884, 0.0, 0.6846732173608203, 0.6889091903146469, 0.6899437584583995, 0.6589556806830628]
avg entropy: 0.6169205463222409
avg cont entropy: 0.7201636638905465
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.8836567401885986
delta all entropy: [-0.002287508812640171, -0.052441707420581185, 0.12900816358665645, 0.0014889662525282565, 0.0008835801353759276, 0.0006444690263781583, -0.02920219385153111, 0.0015831075991527, 0.0012999092036981574, 0.0017348036185654836, 0.0, 0.0034313578904323627, -0.0008722123937533466, -0.0011540217045241663, 0.0021632388390270263]
delta avg entropy: 0.0037519967979189697
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-08 01:54:54,498 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-600
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-08 01:54:54,499 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-600/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 01:55:22,961 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-600/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 01:55:22,966 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 01:55:22,966 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-600/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 01:55:23,010 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-600/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-08 01:55:23,015] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step610 is begin to save!
[2025-07-08 01:55:33,870] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-600/global_step610/mp_rank_00_model_states.pt
[2025-07-08 01:56:28,729] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step610 is begin to save!
[2025-07-08 01:56:40,184] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-600/global_step610/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 01:57:38,637 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-500] due to args.save_total_limit
{'loss': 0.0352, 'learning_rate': 8.502222222222226e-06, 'epoch': 14.86}
[INFO|trainer.py:2661] 2025-07-08 01:59:55,105 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-610
[INFO|configuration_utils.py:425] 2025-07-08 01:59:55,106 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-610/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 02:00:23,697 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-610/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 02:00:23,701 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-610/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 02:00:23,701 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-610/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 02:00:23,745 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-610/spiece.model
[2025-07-08 02:00:23,749] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step620 is begin to save!
[2025-07-08 02:00:34,531] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-610/global_step620/mp_rank_00_model_states.pt
[2025-07-08 02:01:26,818] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step620 is begin to save!
[2025-07-08 02:01:44,308] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-610/global_step620/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 02:02:52,987 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-510] due to args.save_total_limit
{'loss': 0.037, 'learning_rate': 8.259000000000002e-06, 'epoch': 15.12}
[INFO|trainer.py:2661] 2025-07-08 02:05:34,724 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-620
[INFO|configuration_utils.py:425] 2025-07-08 02:05:34,725 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-620/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 02:06:02,713 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-620/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 02:06:02,718 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-620/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 02:06:02,718 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-620/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 02:06:02,792 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-620/spiece.model
[2025-07-08 02:06:02,798] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step631 is begin to save!
[2025-07-08 02:06:13,651] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-620/global_step631/mp_rank_00_model_states.pt
[2025-07-08 02:06:59,433] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step631 is begin to save!
[2025-07-08 02:07:10,222] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-620/global_step631/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 02:08:06,195 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-520] due to args.save_total_limit
{'loss': 0.034, 'learning_rate': 8.03788888888889e-06, 'epoch': 15.36}
[INFO|trainer.py:2661] 2025-07-08 02:10:27,965 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-630
[INFO|configuration_utils.py:425] 2025-07-08 02:10:27,966 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-630/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 02:10:55,605 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-630/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 02:10:55,607 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-630/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 02:10:55,607 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-630/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 02:10:55,668 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-630/spiece.model
[2025-07-08 02:10:55,673] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step641 is begin to save!
[2025-07-08 02:11:06,298] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-630/global_step641/mp_rank_00_model_states.pt
[2025-07-08 02:12:00,001] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step641 is begin to save!
[2025-07-08 02:12:10,669] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-630/global_step641/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 02:13:11,813 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-530] due to args.save_total_limit
{'loss': 0.0343, 'learning_rate': 7.81677777777778e-06, 'epoch': 15.6}
[INFO|trainer.py:2661] 2025-07-08 02:15:31,873 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-640
[INFO|configuration_utils.py:425] 2025-07-08 02:15:31,874 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-640/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 02:15:59,918 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-640/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 02:15:59,922 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-640/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 02:15:59,923 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-640/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 02:15:59,970 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-640/spiece.model
[2025-07-08 02:15:59,976] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step651 is begin to save!
[2025-07-08 02:16:10,722] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-640/global_step651/mp_rank_00_model_states.pt
[2025-07-08 02:17:01,534] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step651 is begin to save!
[2025-07-08 02:17:12,202] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-640/global_step651/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 02:18:14,076 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-540] due to args.save_total_limit
{'loss': 0.0339, 'learning_rate': 7.595666666666668e-06, 'epoch': 15.84}
[INFO|trainer.py:2925] 2025-07-08 02:20:30,990 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 02:20:30,991 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 02:20:30,991 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.24), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.15), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.77), 'median_precision': np.float64(34.5), 'mean_precision': np.float64(35.48), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.59), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.22), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.14), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.79), 'median_f1': np.float64(41.37), 'mean_f1': np.float64(39.7), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.82, 'posix': 2.90234637260437}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.24), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.15), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.77), 'median_precision': np.float64(34.5), 'mean_precision': np.float64(35.48), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.59), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.22), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.14), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.79), 'median_f1': np.float64(41.37), 'mean_f1': np.float64(39.7), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.82, 'posix': 2.90234637260437}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.24), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.15), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.77), 'median_precision': np.float64(34.5), 'mean_precision': np.float64(35.48), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.59), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.22), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.14), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.79), 'median_f1': np.float64(41.37), 'mean_f1': np.float64(39.7), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.82, 'posix': 2.90234637260437}
{'max_accuracy': 52.3, 'median_accuracy': 49.8, 'mean_accuracy': 47.24, 'min_accuracy': 33.3, 'std_accuracy': 6.15, 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.3, 'max_precision': 49.77, 'median_precision': 34.5, 'mean_precision': 35.48, 'min_precision': 11.1, 'std_precision': 8.59, 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.85, 'max_recall': 52.26, 'median_recall': 49.78, 'mean_recall': 47.22, 'min_recall': 33.33, 'std_recall': 6.14, 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.27, 'max_f1': 45.79, 'median_f1': 41.37, 'mean_f1': 39.7, 'min_f1': 16.65, 'std_f1': 6.33, 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.82, 'posix': 2.90234637260437, 'eval_runtime': 181.0962, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 15.84}
[INFO|trainer.py:2925] 2025-07-08 02:23:32,090 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 02:23:32,091 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 02:23:32,091 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.24), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.15), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.77), 'median_precision': np.float64(34.5), 'mean_precision': np.float64(35.48), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.59), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.22), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.14), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.79), 'median_f1': np.float64(41.37), 'mean_f1': np.float64(39.7), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.82, 'posix': 2.90234637260437}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.24), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.15), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.77), 'median_precision': np.float64(34.5), 'mean_precision': np.float64(35.48), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.59), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.22), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.14), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.79), 'median_f1': np.float64(41.37), 'mean_f1': np.float64(39.7), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.82, 'posix': 2.90234637260437}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.24), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.15), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.77), 'median_precision': np.float64(34.5), 'mean_precision': np.float64(35.48), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.59), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.22), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.14), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.79), 'median_f1': np.float64(41.37), 'mean_f1': np.float64(39.7), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.82, 'posix': 2.90234637260437}
all entropy: [0.6818546087307834, 0.28321090755525935, 0.5947107372245103, 0.6772204456534001, 0.6875186123480206, 0.6877293893152671, 0.9162348680617829, 0.681241859470388, 0.6455697149230675, 0.687936122542609, 0.0, 0.6851880420827186, 0.688722651213499, 0.6901020884407325, 0.658421318067677]
avg entropy: 0.6177107577086477
avg cont entropy: 0.7210442350646176
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.90234637260437
delta all entropy: [-0.002287508812640171, -0.04972608681987034, 0.1437588048489845, 0.0018508651709776558, 0.0008835801353759276, 0.0006444690263781583, -0.03653496734577211, 0.001274685666587061, 0.0032165528284000544, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.001058751494901311, -0.000995691722191161, 0.0016288762236412735]
delta avg entropy: 0.004542208184325751
all entropy: [0.6818546087307834, 0.28321090755525935, 0.5947107372245103, 0.6772204456534001, 0.6875186123480206, 0.6877293893152671, 0.9162348680617829, 0.681241859470388, 0.6455697149230675, 0.687936122542609, 0.0, 0.6851880420827186, 0.688722651213499, 0.6901020884407325, 0.658421318067677]
avg entropy: 0.6177107577086477
avg cont entropy: 0.7210442350646176
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.90234637260437
delta all entropy: [-0.002287508812640171, -0.04972608681987034, 0.1437588048489845, 0.0018508651709776558, 0.0008835801353759276, 0.0006444690263781583, -0.03653496734577211, 0.001274685666587061, 0.0032165528284000544, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.001058751494901311, -0.000995691722191161, 0.0016288762236412735]
delta avg entropy: 0.004542208184325751
all entropy: [0.6818546087307834, 0.28321090755525935, 0.5947107372245103, 0.6772204456534001, 0.6875186123480206, 0.6877293893152671, 0.9162348680617829, 0.681241859470388, 0.6455697149230675, 0.687936122542609, 0.0, 0.6851880420827186, 0.688722651213499, 0.6901020884407325, 0.658421318067677]
avg entropy: 0.6177107577086477
avg cont entropy: 0.7210442350646176
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.90234637260437
delta all entropy: [-0.002287508812640171, -0.04972608681987034, 0.1437588048489845, 0.0018508651709776558, 0.0008835801353759276, 0.0006444690263781583, -0.03653496734577211, 0.001274685666587061, 0.0032165528284000544, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.001058751494901311, -0.000995691722191161, 0.0016288762236412735]
delta avg entropy: 0.004542208184325751
{'all entropy': [0.6818546087307834, 0.28321090755525935, 0.5947107372245103, 0.6772204456534001, 0.6875186123480206, 0.6877293893152671, 0.9162348680617829, 0.681241859470388, 0.6455697149230675, 0.687936122542609, 0.0, 0.6851880420827186, 0.688722651213499, 0.6901020884407325, 0.658421318067677], 'avg entropy': 0.6177107577086477, 'avg cont entropy': 0.7210442350646176, 'accuracy': 0.521, 'precision': 0.34715764502622976, 'recall': 0.5206913500326674, 'f1': 0.4165747356787607, 'posix': 2.90234637260437, 'delta all entropy': [-0.002287508812640171, -0.04972608681987034, 0.1437588048489845, 0.0018508651709776558, 0.0008835801353759276, 0.0006444690263781583, -0.03653496734577211, 0.001274685666587061, 0.0032165528284000544, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.001058751494901311, -0.000995691722191161, 0.0016288762236412735], 'delta avg entropy': 0.004542208184325751, 'unsupervised_dev_runtime': 180.7748, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 15.84}
all entropy: [0.6818546087307834, 0.28321090755525935, 0.5947107372245103, 0.6772204456534001, 0.6875186123480206, 0.6877293893152671, 0.9162348680617829, 0.681241859470388, 0.6455697149230675, 0.687936122542609, 0.0, 0.6851880420827186, 0.688722651213499, 0.6901020884407325, 0.658421318067677]
avg entropy: 0.6177107577086477
avg cont entropy: 0.7210442350646176
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.90234637260437
delta all entropy: [-0.002287508812640171, -0.04972608681987034, 0.1437588048489845, 0.0018508651709776558, 0.0008835801353759276, 0.0006444690263781583, -0.03653496734577211, 0.001274685666587061, 0.0032165528284000544, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.001058751494901311, -0.000995691722191161, 0.0016288762236412735]
delta avg entropy: 0.004542208184325751
all entropy: [0.6818546087307834, 0.28321090755525935, 0.5947107372245103, 0.6772204456534001, 0.6875186123480206, 0.6877293893152671, 0.9162348680617829, 0.681241859470388, 0.6455697149230675, 0.687936122542609, 0.0, 0.6851880420827186, 0.688722651213499, 0.6901020884407325, 0.658421318067677]
avg entropy: 0.6177107577086477
avg cont entropy: 0.7210442350646176
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.90234637260437
delta all entropy: [-0.002287508812640171, -0.04972608681987034, 0.1437588048489845, 0.0018508651709776558, 0.0008835801353759276, 0.0006444690263781583, -0.03653496734577211, 0.001274685666587061, 0.0032165528284000544, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.001058751494901311, -0.000995691722191161, 0.0016288762236412735]
delta avg entropy: 0.004542208184325751
all entropy: [0.6818546087307834, 0.28321090755525935, 0.5947107372245103, 0.6772204456534001, 0.6875186123480206, 0.6877293893152671, 0.9162348680617829, 0.681241859470388, 0.6455697149230675, 0.687936122542609, 0.0, 0.6851880420827186, 0.688722651213499, 0.6901020884407325, 0.658421318067677]
avg entropy: 0.6177107577086477
avg cont entropy: 0.7210442350646176
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.90234637260437
delta all entropy: [-0.002287508812640171, -0.04972608681987034, 0.1437588048489845, 0.0018508651709776558, 0.0008835801353759276, 0.0006444690263781583, -0.03653496734577211, 0.001274685666587061, 0.0032165528284000544, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.001058751494901311, -0.000995691722191161, 0.0016288762236412735]
delta avg entropy: 0.004542208184325751
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-08 02:26:33,119 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-650
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-08 02:26:33,120 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-650/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 02:27:01,137 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-650/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 02:27:01,141 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-650/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 02:27:01,141 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-650/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 02:27:01,186 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-650/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-08 02:27:01,191] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step661 is begin to save!
[2025-07-08 02:27:12,000] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-650/global_step661/mp_rank_00_model_states.pt
[2025-07-08 02:28:04,731] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step661 is begin to save!
[2025-07-08 02:28:15,427] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-650/global_step661/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 02:29:15,930 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-550] due to args.save_total_limit
{'loss': 0.0379, 'learning_rate': 7.352444444444446e-06, 'epoch': 16.1}
[INFO|trainer.py:2661] 2025-07-08 02:31:48,037 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-660
[INFO|configuration_utils.py:425] 2025-07-08 02:31:48,039 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-660/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 02:32:15,380 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-660/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 02:32:15,383 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-660/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 02:32:15,384 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-660/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 02:32:15,428 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-660/spiece.model
[2025-07-08 02:32:15,433] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step672 is begin to save!
[2025-07-08 02:32:26,106] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-660/global_step672/mp_rank_00_model_states.pt
[2025-07-08 02:33:19,776] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step672 is begin to save!
[2025-07-08 02:33:31,805] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-660/global_step672/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 02:34:31,648 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-560] due to args.save_total_limit
{'loss': 0.0339, 'learning_rate': 7.131333333333335e-06, 'epoch': 16.34}
[INFO|trainer.py:2661] 2025-07-08 02:36:49,843 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-670
[INFO|configuration_utils.py:425] 2025-07-08 02:36:49,845 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-670/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 02:37:17,670 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-670/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 02:37:17,675 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-670/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 02:37:17,675 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-670/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 02:37:17,720 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-670/spiece.model
[2025-07-08 02:37:17,724] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step682 is begin to save!
[2025-07-08 02:37:28,841] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-670/global_step682/mp_rank_00_model_states.pt
[2025-07-08 02:38:23,572] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step682 is begin to save!
[2025-07-08 02:38:39,685] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-670/global_step682/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 02:39:47,345 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-570] due to args.save_total_limit
{'loss': 0.0331, 'learning_rate': 6.910222222222222e-06, 'epoch': 16.57}
[INFO|trainer.py:2661] 2025-07-08 02:42:19,450 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-680
[INFO|configuration_utils.py:425] 2025-07-08 02:42:19,452 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-680/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 02:42:46,886 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-680/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 02:42:46,887 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 02:42:46,888 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-680/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 02:42:46,939 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-680/spiece.model
[2025-07-08 02:42:46,944] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step692 is begin to save!
[2025-07-08 02:42:57,745] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-680/global_step692/mp_rank_00_model_states.pt
[2025-07-08 02:43:46,572] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step692 is begin to save!
[2025-07-08 02:43:57,299] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-680/global_step692/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 02:44:51,964 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-580] due to args.save_total_limit
{'loss': 0.0319, 'learning_rate': 6.6891111111111115e-06, 'epoch': 16.81}
[INFO|trainer.py:2661] 2025-07-08 02:47:09,847 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-690
[INFO|configuration_utils.py:425] 2025-07-08 02:47:09,849 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-690/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 02:47:37,434 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-690/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 02:47:37,437 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-690/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 02:47:37,438 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-690/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 02:47:37,488 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-690/spiece.model
[2025-07-08 02:47:37,494] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step702 is begin to save!
[2025-07-08 02:47:48,442] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-690/global_step702/mp_rank_00_model_states.pt
[2025-07-08 02:48:40,017] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step702 is begin to save!
[2025-07-08 02:48:50,924] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-690/global_step702/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 02:49:46,459 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-590] due to args.save_total_limit
{'loss': 0.0356, 'learning_rate': 6.468e-06, 'epoch': 17.07}
[INFO|trainer.py:2925] 2025-07-08 02:52:15,832 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 02:52:15,832 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 02:52:15,832 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(49.42), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.32), 'median_f1': np.float64(41.29), 'mean_f1': np.float64(39.69), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.3), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.66, 'posix': 2.9205515384674072}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(49.42), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.32), 'median_f1': np.float64(41.29), 'mean_f1': np.float64(39.69), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.3), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.66, 'posix': 2.9205515384674072}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(49.42), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.32), 'median_f1': np.float64(41.29), 'mean_f1': np.float64(39.69), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.3), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.66, 'posix': 2.9205515384674072}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(49.42), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.32), 'median_f1': np.float64(41.29), 'mean_f1': np.float64(39.69), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.3), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.66, 'posix': 2.9205515384674072}
{'max_accuracy': 52.3, 'median_accuracy': 49.8, 'mean_accuracy': 47.25, 'min_accuracy': 33.3, 'std_accuracy': 6.12, 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.1, 'max_precision': 49.42, 'median_precision': 34.42, 'mean_precision': 35.42, 'min_precision': 11.1, 'std_precision': 8.47, 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.72, 'max_recall': 52.26, 'median_recall': 49.78, 'mean_recall': 47.23, 'min_recall': 33.33, 'std_recall': 6.1, 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.07, 'max_f1': 45.32, 'median_f1': 41.29, 'mean_f1': 39.69, 'min_f1': 16.65, 'std_f1': 6.3, 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.66, 'posix': 2.9205515384674072, 'eval_runtime': 181.6418, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 17.07}
[INFO|trainer.py:2925] 2025-07-08 02:55:17,477 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 02:55:17,478 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 02:55:17,478 >>   Batch size = 100
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(49.42), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.32), 'median_f1': np.float64(41.29), 'mean_f1': np.float64(39.69), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.3), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.66, 'posix': 2.9205515384674072}
{'max_accuracy': np.float64(52.3), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.12), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.1, 'max_precision': np.float64(49.42), 'median_precision': np.float64(34.42), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.47), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.72, 'max_recall': np.float64(52.26), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.1), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.07, 'max_f1': np.float64(45.32), 'median_f1': np.float64(41.29), 'mean_f1': np.float64(39.69), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.3), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.66, 'posix': 2.9205515384674072}
all entropy: [0.6809293403663059, 0.2763947550590927, 0.6113207445803619, 0.6764925138736271, 0.6877293893152671, 0.6877293893152671, 0.9137179557240116, 0.6809293403663059, 0.6461997616444423, 0.6875186123480206, 0.0, 0.6856866086439042, 0.688722651213499, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6180690083862441
avg cont entropy: 0.7216409736057454
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.9205515384674072
delta all entropy: [-0.003212777177117654, -0.05654223931603697, 0.16036881220483612, 0.0011229333912046169, 0.0010943571026224008, 0.0006444690263781583, -0.0390518796835434, 0.0009621665625050291, 0.003846599549774843, 0.001114602252997643, 0.0, 0.004444749173516227, -0.001058751494901311, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.004900458861922208
all entropy: [0.6809293403663059, 0.2763947550590927, 0.6113207445803619, 0.6764925138736271, 0.6877293893152671, 0.6877293893152671, 0.9137179557240116, 0.6809293403663059, 0.6461997616444423, 0.6875186123480206, 0.0, 0.6856866086439042, 0.688722651213499, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6180690083862441
avg cont entropy: 0.7216409736057454
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.9205515384674072
delta all entropy: [-0.003212777177117654, -0.05654223931603697, 0.16036881220483612, 0.0011229333912046169, 0.0010943571026224008, 0.0006444690263781583, -0.0390518796835434, 0.0009621665625050291, 0.003846599549774843, 0.001114602252997643, 0.0, 0.004444749173516227, -0.001058751494901311, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.004900458861922208
{'all entropy': [0.6809293403663059, 0.2763947550590927, 0.6113207445803619, 0.6764925138736271, 0.6877293893152671, 0.6877293893152671, 0.9137179557240116, 0.6809293403663059, 0.6461997616444423, 0.6875186123480206, 0.0, 0.6856866086439042, 0.688722651213499, 0.6897814027084003, 0.6578826606351565], 'avg entropy': 0.6180690083862441, 'avg cont entropy': 0.7216409736057454, 'accuracy': 0.522, 'precision': 0.3478214346858414, 'recall': 0.5216893540246833, 'f1': 0.41737202558760805, 'posix': 2.9205515384674072, 'delta all entropy': [-0.003212777177117654, -0.05654223931603697, 0.16036881220483612, 0.0011229333912046169, 0.0010943571026224008, 0.0006444690263781583, -0.0390518796835434, 0.0009621665625050291, 0.003846599549774843, 0.001114602252997643, 0.0, 0.004444749173516227, -0.001058751494901311, -0.0013163774545233986, 0.0010902187911208117], 'delta avg entropy': 0.004900458861922208, 'unsupervised_dev_runtime': 180.4848, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 17.07}
all entropy: [0.6809293403663059, 0.2763947550590927, 0.6113207445803619, 0.6764925138736271, 0.6877293893152671, 0.6877293893152671, 0.9137179557240116, 0.6809293403663059, 0.6461997616444423, 0.6875186123480206, 0.0, 0.6856866086439042, 0.688722651213499, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6180690083862441
avg cont entropy: 0.7216409736057454
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.9205515384674072
delta all entropy: [-0.003212777177117654, -0.05654223931603697, 0.16036881220483612, 0.0011229333912046169, 0.0010943571026224008, 0.0006444690263781583, -0.0390518796835434, 0.0009621665625050291, 0.003846599549774843, 0.001114602252997643, 0.0, 0.004444749173516227, -0.001058751494901311, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.004900458861922208
all entropy: [0.6809293403663059, 0.2763947550590927, 0.6113207445803619, 0.6764925138736271, 0.6877293893152671, 0.6877293893152671, 0.9137179557240116, 0.6809293403663059, 0.6461997616444423, 0.6875186123480206, 0.0, 0.6856866086439042, 0.688722651213499, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6180690083862441
avg cont entropy: 0.7216409736057454
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.9205515384674072
delta all entropy: [-0.003212777177117654, -0.05654223931603697, 0.16036881220483612, 0.0011229333912046169, 0.0010943571026224008, 0.0006444690263781583, -0.0390518796835434, 0.0009621665625050291, 0.003846599549774843, 0.001114602252997643, 0.0, 0.004444749173516227, -0.001058751494901311, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.004900458861922208
all entropy: [0.6809293403663059, 0.2763947550590927, 0.6113207445803619, 0.6764925138736271, 0.6877293893152671, 0.6877293893152671, 0.9137179557240116, 0.6809293403663059, 0.6461997616444423, 0.6875186123480206, 0.0, 0.6856866086439042, 0.688722651213499, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6180690083862441
avg cont entropy: 0.7216409736057454
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.9205515384674072
delta all entropy: [-0.003212777177117654, -0.05654223931603697, 0.16036881220483612, 0.0011229333912046169, 0.0010943571026224008, 0.0006444690263781583, -0.0390518796835434, 0.0009621665625050291, 0.003846599549774843, 0.001114602252997643, 0.0, 0.004444749173516227, -0.001058751494901311, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.004900458861922208
all entropy: [0.6809293403663059, 0.2763947550590927, 0.6113207445803619, 0.6764925138736271, 0.6877293893152671, 0.6877293893152671, 0.9137179557240116, 0.6809293403663059, 0.6461997616444423, 0.6875186123480206, 0.0, 0.6856866086439042, 0.688722651213499, 0.6897814027084003, 0.6578826606351565]
avg entropy: 0.6180690083862441
avg cont entropy: 0.7216409736057454
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.9205515384674072
delta all entropy: [-0.003212777177117654, -0.05654223931603697, 0.16036881220483612, 0.0011229333912046169, 0.0010943571026224008, 0.0006444690263781583, -0.0390518796835434, 0.0009621665625050291, 0.003846599549774843, 0.001114602252997643, 0.0, 0.004444749173516227, -0.001058751494901311, -0.0013163774545233986, 0.0010902187911208117]
delta avg entropy: 0.004900458861922208
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-08 02:58:18,221 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-700
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-08 02:58:18,222 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-700/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 02:58:45,891 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-700/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 02:58:45,895 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 02:58:45,896 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-700/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 02:58:45,943 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-700/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-08 02:58:45,948] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step712 is begin to save!
[2025-07-08 02:58:56,559] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-700/global_step712/mp_rank_00_model_states.pt
[2025-07-08 02:59:44,254] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step712 is begin to save!
[2025-07-08 02:59:54,837] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-700/global_step712/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 03:00:53,656 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-600] due to args.save_total_limit
{'loss': 0.033, 'learning_rate': 6.24688888888889e-06, 'epoch': 17.31}
[INFO|trainer.py:2661] 2025-07-08 03:03:14,810 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-710
[INFO|configuration_utils.py:425] 2025-07-08 03:03:14,811 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-710/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 03:03:41,994 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-710/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 03:03:41,997 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-710/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 03:03:41,998 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-710/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 03:03:42,045 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-710/spiece.model
[2025-07-08 03:03:42,050] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step722 is begin to save!
[2025-07-08 03:03:52,930] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-710/global_step722/mp_rank_00_model_states.pt
[2025-07-08 03:04:45,757] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step722 is begin to save!
[2025-07-08 03:04:56,571] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-710/global_step722/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 03:05:54,704 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-610] due to args.save_total_limit
{'loss': 0.0334, 'learning_rate': 6.02577777777778e-06, 'epoch': 17.55}
[INFO|trainer.py:2661] 2025-07-08 03:08:15,034 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-720
[INFO|configuration_utils.py:425] 2025-07-08 03:08:15,035 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-720/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 03:08:41,649 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-720/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 03:08:41,653 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-720/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 03:08:41,653 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-720/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 03:08:41,698 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-720/spiece.model
[2025-07-08 03:08:41,703] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step732 is begin to save!
[2025-07-08 03:08:52,411] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-720/global_step732/mp_rank_00_model_states.pt
[2025-07-08 03:09:44,334] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step732 is begin to save!
[2025-07-08 03:09:57,126] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-720/global_step732/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 03:10:56,352 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-620] due to args.save_total_limit
{'loss': 0.0333, 'learning_rate': 5.804666666666666e-06, 'epoch': 17.79}
[INFO|trainer.py:2661] 2025-07-08 03:13:16,497 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-730
[INFO|configuration_utils.py:425] 2025-07-08 03:13:16,499 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-730/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 03:13:44,775 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-730/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 03:13:44,779 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-730/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 03:13:44,779 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-730/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 03:13:44,822 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-730/spiece.model
[2025-07-08 03:13:44,826] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step742 is begin to save!
[2025-07-08 03:13:55,705] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-730/global_step742/mp_rank_00_model_states.pt
[2025-07-08 03:14:48,829] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step742 is begin to save!
[2025-07-08 03:15:05,632] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-730/global_step742/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 03:16:12,630 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-630] due to args.save_total_limit
{'loss': 0.0331, 'learning_rate': 5.561444444444446e-06, 'epoch': 18.05}
[INFO|trainer.py:2661] 2025-07-08 03:18:52,123 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-740
[INFO|configuration_utils.py:425] 2025-07-08 03:18:52,124 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-740/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 03:19:20,157 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-740/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 03:19:20,161 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-740/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 03:19:20,161 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-740/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 03:19:20,213 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-740/spiece.model
[2025-07-08 03:19:20,218] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step753 is begin to save!
[2025-07-08 03:19:30,938] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-740/global_step753/mp_rank_00_model_states.pt
[2025-07-08 03:20:21,897] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step753 is begin to save!
[2025-07-08 03:20:32,568] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-740/global_step753/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 03:21:28,261 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-640] due to args.save_total_limit
{'loss': 0.0321, 'learning_rate': 5.340333333333334e-06, 'epoch': 18.29}
[INFO|trainer.py:2925] 2025-07-08 03:23:48,890 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 03:23:48,890 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 03:23:48,890 >>   Batch size = 100
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.4, 'max_precision': np.float64(50.85), 'median_precision': np.float64(34.29), 'mean_precision': np.float64(35.55), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.71), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.91, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.37, 'max_f1': np.float64(45.63), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.74), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.9, 'posix': 2.9307758808135986}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.4, 'max_precision': np.float64(50.85), 'median_precision': np.float64(34.29), 'mean_precision': np.float64(35.55), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.71), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.91, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.37, 'max_f1': np.float64(45.63), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.74), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.9, 'posix': 2.9307758808135986}
{'max_accuracy': 52.4, 'median_accuracy': 49.8, 'mean_accuracy': 47.25, 'min_accuracy': 33.3, 'std_accuracy': 6.09, 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.4, 'max_precision': 50.85, 'median_precision': 34.29, 'mean_precision': 35.55, 'min_precision': 11.1, 'std_precision': 8.71, 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.91, 'max_recall': 52.37, 'median_recall': 49.78, 'mean_recall': 47.23, 'min_recall': 33.33, 'std_recall': 6.08, 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.37, 'max_f1': 45.63, 'median_f1': 41.13, 'mean_f1': 39.74, 'min_f1': 16.65, 'std_f1': 6.33, 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.9, 'posix': 2.9307758808135986, 'eval_runtime': 180.9569, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 18.29}
[INFO|trainer.py:2925] 2025-07-08 03:26:49,849 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 03:26:49,849 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 03:26:49,849 >>   Batch size = 100
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.4, 'max_precision': np.float64(50.85), 'median_precision': np.float64(34.29), 'mean_precision': np.float64(35.55), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.71), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.91, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.37, 'max_f1': np.float64(45.63), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.74), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.9, 'posix': 2.9307758808135986}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.4, 'max_precision': np.float64(50.85), 'median_precision': np.float64(34.29), 'mean_precision': np.float64(35.55), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.71), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.91, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.37, 'max_f1': np.float64(45.63), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.74), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.9, 'posix': 2.9307758808135986}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.4, 'max_precision': np.float64(50.85), 'median_precision': np.float64(34.29), 'mean_precision': np.float64(35.55), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.71), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.91, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.37, 'max_f1': np.float64(45.63), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.74), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.9, 'posix': 2.9307758808135986}
{'max_accuracy': np.float64(52.4), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.09), 'avg_ensemble_accuracy': 52.1, 'vote_ensemble_accuracy': 52.4, 'max_precision': np.float64(50.85), 'median_precision': np.float64(34.29), 'mean_precision': np.float64(35.55), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.71), 'avg_ensemble_precision': 34.72, 'vote_ensemble_precision': 34.91, 'max_recall': np.float64(52.37), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.08), 'avg_ensemble_recall': 52.07, 'vote_ensemble_recall': 52.37, 'max_f1': np.float64(45.63), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.74), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.33), 'avg_ensemble_f1': 41.66, 'vote_ensemble_f1': 41.9, 'posix': 2.9307758808135986}
all entropy: [0.6793051948496998, 0.2722425107838469, 0.6153284797339046, 0.6768585467349507, 0.6873037899231933, 0.687936122542609, 0.9111682679672901, 0.681241859470388, 0.6474466390346325, 0.687936122542609, 0.0, 0.6854393565969992, 0.688722651213499, 0.6901020884407325, 0.6573397035288462]
avg entropy: 0.6178914222242133
avg cont entropy: 0.7223135319826116
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.9307758808135986
delta all entropy: [-0.004836922693723822, -0.060694483591282766, 0.1643765473583788, 0.0014889662525282565, 0.0006687577105486397, 0.0008512022537201513, -0.041601567440264886, 0.001274685666587061, 0.005093476939965091, 0.0015321124475861092, 0.0, 0.004197497126611216, -0.001058751494901311, -0.000995691722191161, 0.0005472616848104517]
delta avg entropy: 0.004722872699891456
all entropy: [0.6793051948496998, 0.2722425107838469, 0.6153284797339046, 0.6768585467349507, 0.6873037899231933, 0.687936122542609, 0.9111682679672901, 0.681241859470388, 0.6474466390346325, 0.687936122542609, 0.0, 0.6854393565969992, 0.688722651213499, 0.6901020884407325, 0.6573397035288462]
avg entropy: 0.6178914222242133
avg cont entropy: 0.7223135319826116
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.9307758808135986
delta all entropy: [-0.004836922693723822, -0.060694483591282766, 0.1643765473583788, 0.0014889662525282565, 0.0006687577105486397, 0.0008512022537201513, -0.041601567440264886, 0.001274685666587061, 0.005093476939965091, 0.0015321124475861092, 0.0, 0.004197497126611216, -0.001058751494901311, -0.000995691722191161, 0.0005472616848104517]
delta avg entropy: 0.004722872699891456
all entropy: [0.6793051948496998, 0.2722425107838469, 0.6153284797339046, 0.6768585467349507, 0.6873037899231933, 0.687936122542609, 0.9111682679672901, 0.681241859470388, 0.6474466390346325, 0.687936122542609, 0.0, 0.6854393565969992, 0.688722651213499, 0.6901020884407325, 0.6573397035288462]
avg entropy: 0.6178914222242133
avg cont entropy: 0.7223135319826116
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.9307758808135986
delta all entropy: [-0.004836922693723822, -0.060694483591282766, 0.1643765473583788, 0.0014889662525282565, 0.0006687577105486397, 0.0008512022537201513, -0.041601567440264886, 0.001274685666587061, 0.005093476939965091, 0.0015321124475861092, 0.0, 0.004197497126611216, -0.001058751494901311, -0.000995691722191161, 0.0005472616848104517]
delta avg entropy: 0.004722872699891456
{'all entropy': [0.6793051948496998, 0.2722425107838469, 0.6153284797339046, 0.6768585467349507, 0.6873037899231933, 0.687936122542609, 0.9111682679672901, 0.681241859470388, 0.6474466390346325, 0.687936122542609, 0.0, 0.6854393565969992, 0.688722651213499, 0.6901020884407325, 0.6573397035288462], 'avg entropy': 0.6178914222242133, 'avg cont entropy': 0.7223135319826116, 'accuracy': 0.521, 'precision': 0.34715764502622976, 'recall': 0.5206913500326674, 'f1': 0.4165747356787607, 'posix': 2.9307758808135986, 'delta all entropy': [-0.004836922693723822, -0.060694483591282766, 0.1643765473583788, 0.0014889662525282565, 0.0006687577105486397, 0.0008512022537201513, -0.041601567440264886, 0.001274685666587061, 0.005093476939965091, 0.0015321124475861092, 0.0, 0.004197497126611216, -0.001058751494901311, -0.000995691722191161, 0.0005472616848104517], 'delta avg entropy': 0.004722872699891456, 'unsupervised_dev_runtime': 181.1162, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 18.29}
all entropy: [0.6793051948496998, 0.2722425107838469, 0.6153284797339046, 0.6768585467349507, 0.6873037899231933, 0.687936122542609, 0.9111682679672901, 0.681241859470388, 0.6474466390346325, 0.687936122542609, 0.0, 0.6854393565969992, 0.688722651213499, 0.6901020884407325, 0.6573397035288462]
avg entropy: 0.6178914222242133
avg cont entropy: 0.7223135319826116
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.9307758808135986
delta all entropy: [-0.004836922693723822, -0.060694483591282766, 0.1643765473583788, 0.0014889662525282565, 0.0006687577105486397, 0.0008512022537201513, -0.041601567440264886, 0.001274685666587061, 0.005093476939965091, 0.0015321124475861092, 0.0, 0.004197497126611216, -0.001058751494901311, -0.000995691722191161, 0.0005472616848104517]
delta avg entropy: 0.004722872699891456
all entropy: [0.6793051948496998, 0.2722425107838469, 0.6153284797339046, 0.6768585467349507, 0.6873037899231933, 0.687936122542609, 0.9111682679672901, 0.681241859470388, 0.6474466390346325, 0.687936122542609, 0.0, 0.6854393565969992, 0.688722651213499, 0.6901020884407325, 0.6573397035288462]
avg entropy: 0.6178914222242133
avg cont entropy: 0.7223135319826116
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.9307758808135986
delta all entropy: [-0.004836922693723822, -0.060694483591282766, 0.1643765473583788, 0.0014889662525282565, 0.0006687577105486397, 0.0008512022537201513, -0.041601567440264886, 0.001274685666587061, 0.005093476939965091, 0.0015321124475861092, 0.0, 0.004197497126611216, -0.001058751494901311, -0.000995691722191161, 0.0005472616848104517]
delta avg entropy: 0.004722872699891456
all entropy: [0.6793051948496998, 0.2722425107838469, 0.6153284797339046, 0.6768585467349507, 0.6873037899231933, 0.687936122542609, 0.9111682679672901, 0.681241859470388, 0.6474466390346325, 0.687936122542609, 0.0, 0.6854393565969992, 0.688722651213499, 0.6901020884407325, 0.6573397035288462]
avg entropy: 0.6178914222242133
avg cont entropy: 0.7223135319826116
accuracy: 0.521
precision: 0.34715764502622976
recall: 0.5206913500326674
f1: 0.4165747356787607
posix: 2.9307758808135986
delta all entropy: [-0.004836922693723822, -0.060694483591282766, 0.1643765473583788, 0.0014889662525282565, 0.0006687577105486397, 0.0008512022537201513, -0.041601567440264886, 0.001274685666587061, 0.005093476939965091, 0.0015321124475861092, 0.0, 0.004197497126611216, -0.001058751494901311, -0.000995691722191161, 0.0005472616848104517]
delta avg entropy: 0.004722872699891456
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-08 03:29:51,181 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-750
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-08 03:29:51,182 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-750/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 03:30:19,320 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-750/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 03:30:19,324 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 03:30:19,324 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-750/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 03:30:19,374 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-750/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-08 03:30:19,379] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step763 is begin to save!
[2025-07-08 03:30:30,318] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-750/global_step763/mp_rank_00_model_states.pt
[2025-07-08 03:31:20,132] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step763 is begin to save!
[2025-07-08 03:31:31,053] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-750/global_step763/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 03:32:25,732 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-650] due to args.save_total_limit
{'loss': 0.032, 'learning_rate': 5.119222222222223e-06, 'epoch': 18.53}
[INFO|trainer.py:2661] 2025-07-08 03:34:45,761 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-760
[INFO|configuration_utils.py:425] 2025-07-08 03:34:45,762 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-760/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 03:35:13,555 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-760/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 03:35:13,559 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-760/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 03:35:13,560 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-760/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 03:35:13,608 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-760/spiece.model
[2025-07-08 03:35:13,613] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step773 is begin to save!
[2025-07-08 03:35:24,511] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-760/global_step773/mp_rank_00_model_states.pt
[2025-07-08 03:36:12,232] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step773 is begin to save!
[2025-07-08 03:36:23,110] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-760/global_step773/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 03:37:20,629 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-660] due to args.save_total_limit
{'loss': 0.0319, 'learning_rate': 4.898111111111113e-06, 'epoch': 18.77}
[INFO|trainer.py:2661] 2025-07-08 03:39:39,088 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-770
[INFO|configuration_utils.py:425] 2025-07-08 03:39:39,089 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-770/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 03:40:06,336 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-770/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 03:40:06,339 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-770/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 03:40:06,339 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-770/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 03:40:06,388 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-770/spiece.model
[2025-07-08 03:40:06,393] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step783 is begin to save!
[2025-07-08 03:40:17,341] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-770/global_step783/mp_rank_00_model_states.pt
[2025-07-08 03:41:05,572] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step783 is begin to save!
[2025-07-08 03:41:16,410] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-770/global_step783/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 03:42:08,148 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-670] due to args.save_total_limit
{'loss': 0.0358, 'learning_rate': 4.65488888888889e-06, 'epoch': 19.02}
[INFO|trainer.py:2661] 2025-07-08 03:44:38,008 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-780
[INFO|configuration_utils.py:425] 2025-07-08 03:44:38,009 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-780/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 03:45:05,226 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 03:45:05,230 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 03:45:05,230 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-780/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 03:45:05,273 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-780/spiece.model
[2025-07-08 03:45:05,278] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step794 is begin to save!
[2025-07-08 03:45:16,167] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-780/global_step794/mp_rank_00_model_states.pt
[2025-07-08 03:46:07,292] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step794 is begin to save!
[2025-07-08 03:46:19,894] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-780/global_step794/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 03:47:20,753 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-680] due to args.save_total_limit
{'loss': 0.0318, 'learning_rate': 4.433777777777778e-06, 'epoch': 19.26}
[INFO|trainer.py:2661] 2025-07-08 03:49:43,628 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-790
[INFO|configuration_utils.py:425] 2025-07-08 03:49:43,629 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-790/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 03:50:11,428 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-790/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 03:50:11,432 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-790/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 03:50:11,433 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-790/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 03:50:11,476 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-790/spiece.model
[2025-07-08 03:50:11,481] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step804 is begin to save!
[2025-07-08 03:50:22,401] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-790/global_step804/mp_rank_00_model_states.pt
[2025-07-08 03:51:15,529] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step804 is begin to save!
[2025-07-08 03:51:32,425] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-790/global_step804/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 03:52:53,501 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-690] due to args.save_total_limit
{'loss': 0.032, 'learning_rate': 4.212666666666667e-06, 'epoch': 19.5}
[INFO|trainer.py:2925] 2025-07-08 03:55:13,383 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 03:55:13,383 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 03:55:13,383 >>   Batch size = 100
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
{'max_accuracy': 52.5, 'median_accuracy': 49.8, 'mean_accuracy': 47.25, 'min_accuracy': 33.3, 'std_accuracy': 6.18, 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': 49.37, 'median_precision': 34.32, 'mean_precision': 35.42, 'min_precision': 11.1, 'std_precision': 8.45, 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': 52.46, 'median_recall': 49.78, 'mean_recall': 47.23, 'min_recall': 33.33, 'std_recall': 6.16, 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': 45.09, 'median_f1': 41.13, 'mean_f1': 39.68, 'min_f1': 16.65, 'std_f1': 6.29, 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166, 'eval_runtime': 180.8572, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 19.5}
[INFO|trainer.py:2925] 2025-07-08 03:58:14,243 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 03:58:14,244 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 03:58:14,244 >>   Batch size = 100
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
{'all entropy': [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015], 'avg entropy': 0.6169616796905224, 'avg cont entropy': 0.7227948477917907, 'accuracy': 0.522, 'precision': 0.3478214346858414, 'recall': 0.5216893540246833, 'f1': 0.41737202558760805, 'posix': 2.94332218170166, 'delta all entropy': [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025], 'delta avg entropy': 0.0037931301662003833, 'unsupervised_dev_runtime': 180.5259, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 19.5}
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2661] 2025-07-08 04:01:14,970 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-800
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-07-08 04:01:14,971 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-800/config.json
[INFO|modeling_utils.py:1070] 2025-07-08 04:01:42,128 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-800/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-07-08 04:01:42,130 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-07-08 04:01:42,130 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-800/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-07-08 04:01:42,166 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-800/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-07-08 04:01:42,172] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step814 is begin to save!
[2025-07-08 04:01:52,977] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-800/global_step814/mp_rank_00_model_states.pt
[2025-07-08 04:02:42,512] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step814 is begin to save!
[2025-07-08 04:02:53,230] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-800/global_step814/mp_rank_00_model_states.pt
[INFO|trainer.py:2739] 2025-07-08 04:03:48,644 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707/checkpoint-700] due to args.save_total_limit
[INFO|trainer.py:2037] 2025-07-08 04:03:55,267 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 29943.1873, 'train_samples_per_second': 0.802, 'train_steps_per_second': 0.033, 'train_loss': 0.05005712438374758, 'epoch': 19.5}
[INFO|trainer.py:2925] 2025-07-08 04:03:55,270 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 04:03:55,270 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 04:03:55,271 >>   Batch size = 100
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
{'all entropy': [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015], 'avg entropy': 0.6169616796905224, 'avg cont entropy': 0.7227948477917907, 'accuracy': 0.522, 'precision': 0.3478214346858414, 'recall': 0.5216893540246833, 'f1': 0.41737202558760805, 'posix': 2.94332218170166, 'delta all entropy': [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025], 'delta avg entropy': 0.0037931301662003833, 'unsupervised_dev_runtime': 179.4493, 'unsupervised_dev_samples_per_second': 0.006, 'unsupervised_dev_steps_per_second': 0.006, 'epoch': 19.5}
07/08/2025 04:06:54 - INFO - __main__ - dev_unsupervised_all entropy = [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
07/08/2025 04:06:54 - INFO - __main__ - dev_unsupervised_avg entropy = 0.6169616796905224
07/08/2025 04:06:54 - INFO - __main__ - dev_unsupervised_avg cont entropy = 0.7227948477917907
07/08/2025 04:06:54 - INFO - __main__ - dev_unsupervised_accuracy = 0.522
07/08/2025 04:06:54 - INFO - __main__ - dev_unsupervised_precision = 0.3478214346858414
07/08/2025 04:06:54 - INFO - __main__ - dev_unsupervised_recall = 0.5216893540246833
07/08/2025 04:06:54 - INFO - __main__ - dev_unsupervised_f1 = 0.41737202558760805
07/08/2025 04:06:54 - INFO - __main__ - dev_unsupervised_posix = 2.94332218170166
07/08/2025 04:06:54 - INFO - __main__ - dev_unsupervised_delta all entropy = [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
07/08/2025 04:06:54 - INFO - __main__ - dev_unsupervised_delta avg entropy = 0.0037931301662003833
[INFO|trainer.py:2925] 2025-07-08 04:06:54,723 >> ***** Running Evaluation *****
[INFO|trainer.py:2927] 2025-07-08 04:06:54,723 >>   Num examples = 45000
[INFO|trainer.py:2930] 2025-07-08 04:06:54,723 >>   Batch size = 100
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
all entropy: [0.6806127214870903, 0.2571199778001798, 0.6231641195682565, 0.6764925138736271, 0.6875186123480206, 0.6881388137135884, 0.901181954840395, 0.681241859470388, 0.6480634814046637, 0.687936122542609, 0.0, 0.6851880420827186, 0.6883374644776967, 0.6899437584583995, 0.6594857532902015]
avg entropy: 0.6169616796905224
avg cont entropy: 0.7227948477917907
accuracy: 0.522
precision: 0.3478214346858414
recall: 0.5216893540246833
f1: 0.41737202558760805
posix: 2.94332218170166
delta all entropy: [-0.0035293960563332893, -0.07581701657494988, 0.17221218719273068, 0.0011229333912046169, 0.0008835801353759276, 0.0010538934246995257, -0.051587880567160016, 0.001274685666587061, 0.005710319309996326, 0.0015321124475861092, 0.0, 0.003946182612330595, -0.0014439382307035453, -0.0011540217045241663, 0.0026933114461658025]
delta avg entropy: 0.0037931301662003833
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
Best checkpoint at step 750: 
max_accuracy=52.4,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.09,avg_ensemble_accuracy=52.1,vote_ensemble_accuracy=52.4
Best checkpoint selected by avg entropy at step 700:
max_accuracy=52.3,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.12,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.1
Best checkpoint selected by avg cont entropy at step 800:
max_accuracy=52.5,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.18,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.3
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
Best checkpoint at step 750: 
max_accuracy=52.4,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.09,avg_ensemble_accuracy=52.1,vote_ensemble_accuracy=52.4
Best checkpoint selected by avg entropy at step 700:
max_accuracy=52.3,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.12,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.1
Best checkpoint selected by avg cont entropy at step 800:
max_accuracy=52.5,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.18,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.3
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
Best checkpoint at step 750: 
max_accuracy=52.4,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.09,avg_ensemble_accuracy=52.1,vote_ensemble_accuracy=52.4
Best checkpoint selected by avg entropy at step 700:
max_accuracy=52.3,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.12,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.1
Best checkpoint selected by avg cont entropy at step 800:
max_accuracy=52.5,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.18,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.3
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
Best checkpoint at step 750: 
max_accuracy=52.4,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.09,avg_ensemble_accuracy=52.1,vote_ensemble_accuracy=52.4
Best checkpoint selected by avg entropy at step 700:
max_accuracy=52.3,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.12,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.1
Best checkpoint selected by avg cont entropy at step 800:
max_accuracy=52.5,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.18,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.3
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
{'max_accuracy': 52.5, 'median_accuracy': 49.8, 'mean_accuracy': 47.25, 'min_accuracy': 33.3, 'std_accuracy': 6.18, 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': 49.37, 'median_precision': 34.32, 'mean_precision': 35.42, 'min_precision': 11.1, 'std_precision': 8.45, 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': 52.46, 'median_recall': 49.78, 'mean_recall': 47.23, 'min_recall': 33.33, 'std_recall': 6.16, 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': 45.09, 'median_f1': 41.13, 'mean_f1': 39.68, 'min_f1': 16.65, 'std_f1': 6.29, 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166, 'eval_runtime': 181.5346, 'eval_samples_per_second': 0.006, 'eval_steps_per_second': 0.006, 'epoch': 19.5}
07/08/2025 04:09:56 - INFO - __main__ - max_accuracy = 52.5
07/08/2025 04:09:56 - INFO - __main__ - median_accuracy = 49.8
07/08/2025 04:09:56 - INFO - __main__ - mean_accuracy = 47.25
07/08/2025 04:09:56 - INFO - __main__ - min_accuracy = 33.3
07/08/2025 04:09:56 - INFO - __main__ - std_accuracy = 6.18
07/08/2025 04:09:56 - INFO - __main__ - avg_ensemble_accuracy = 52.2
07/08/2025 04:09:56 - INFO - __main__ - vote_ensemble_accuracy = 52.3
07/08/2025 04:09:56 - INFO - __main__ - max_precision = 49.37
07/08/2025 04:09:56 - INFO - __main__ - median_precision = 34.32
07/08/2025 04:09:56 - INFO - __main__ - mean_precision = 35.42
07/08/2025 04:09:56 - INFO - __main__ - min_precision = 11.1
07/08/2025 04:09:56 - INFO - __main__ - std_precision = 8.45
07/08/2025 04:09:56 - INFO - __main__ - avg_ensemble_precision = 34.78
07/08/2025 04:09:56 - INFO - __main__ - vote_ensemble_precision = 34.85
07/08/2025 04:09:56 - INFO - __main__ - max_recall = 52.46
07/08/2025 04:09:56 - INFO - __main__ - median_recall = 49.78
07/08/2025 04:09:56 - INFO - __main__ - mean_recall = 47.23
07/08/2025 04:09:56 - INFO - __main__ - min_recall = 33.33
07/08/2025 04:09:56 - INFO - __main__ - std_recall = 6.16
07/08/2025 04:09:56 - INFO - __main__ - avg_ensemble_recall = 52.17
07/08/2025 04:09:56 - INFO - __main__ - vote_ensemble_recall = 52.27
07/08/2025 04:09:56 - INFO - __main__ - max_f1 = 45.09
07/08/2025 04:09:56 - INFO - __main__ - median_f1 = 41.13
07/08/2025 04:09:56 - INFO - __main__ - mean_f1 = 39.68
07/08/2025 04:09:56 - INFO - __main__ - min_f1 = 16.65
07/08/2025 04:09:56 - INFO - __main__ - std_f1 = 6.29
07/08/2025 04:09:56 - INFO - __main__ - avg_ensemble_f1 = 41.74
07/08/2025 04:09:56 - INFO - __main__ - vote_ensemble_f1 = 41.82
07/08/2025 04:09:56 - INFO - __main__ - posix = 2.94332218170166
Best checkpoint at step 750: 
max_accuracy=52.4,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.09,avg_ensemble_accuracy=52.1,vote_ensemble_accuracy=52.4
Best checkpoint selected by avg entropy at step 700:
max_accuracy=52.3,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.12,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.1
Best checkpoint selected by avg cont entropy at step 800:
max_accuracy=52.5,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.18,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.3
{'max_accuracy': np.float64(52.5), 'median_accuracy': np.float64(49.8), 'mean_accuracy': np.float64(47.25), 'min_accuracy': np.float64(33.3), 'std_accuracy': np.float64(6.18), 'avg_ensemble_accuracy': 52.2, 'vote_ensemble_accuracy': 52.3, 'max_precision': np.float64(49.37), 'median_precision': np.float64(34.32), 'mean_precision': np.float64(35.42), 'min_precision': np.float64(11.1), 'std_precision': np.float64(8.45), 'avg_ensemble_precision': 34.78, 'vote_ensemble_precision': 34.85, 'max_recall': np.float64(52.46), 'median_recall': np.float64(49.78), 'mean_recall': np.float64(47.23), 'min_recall': np.float64(33.33), 'std_recall': np.float64(6.16), 'avg_ensemble_recall': 52.17, 'vote_ensemble_recall': 52.27, 'max_f1': np.float64(45.09), 'median_f1': np.float64(41.13), 'mean_f1': np.float64(39.68), 'min_f1': np.float64(16.65), 'std_f1': np.float64(6.29), 'avg_ensemble_f1': 41.74, 'vote_ensemble_f1': 41.82, 'posix': 2.94332218170166}
Best checkpoint at step 750: 
max_accuracy=52.4,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.09,avg_ensemble_accuracy=52.1,vote_ensemble_accuracy=52.4
Best checkpoint selected by avg entropy at step 700:
max_accuracy=52.3,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.12,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.1
Best checkpoint selected by avg cont entropy at step 800:
max_accuracy=52.5,median_accuracy=49.8,mean_accuracy=47.25,min_accuracy=33.3,std_accuracy=6.18,avg_ensemble_accuracy=52.2,vote_ensemble_accuracy=52.3
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33m/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250707[0m at: [34mhttps://wandb.ai/parsahejabi-academic-team/swarm-distillation/runs/3kds7ets[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250707_194450-3kds7ets/logs[0m
bosco:3458274:3504892 [4] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458274:3504892 [4] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458274:3504892 [4] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458274:3504892 [4] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458274:3504892 [4] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458274:3504892 [4] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458274:3504892 [4] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458274:3504892 [4] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458274:3504892 [4] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458274:3462413 [4] NCCL INFO misc/socket.cc:881 -> 3
bosco:3458275:3504897 [5] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458275:3504897 [5] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458275:3504897 [5] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458275:3504897 [5] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458275:3504897 [5] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458275:3504897 [5] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458275:3504897 [5] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458275:3504897 [5] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458275:3504897 [5] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458274:3462413 [4] NCCL INFO misc/socket.cc:881 -> 3
bosco:3458275:3462411 [5] NCCL INFO misc/socket.cc:881 -> 3
bosco:3458274:3504892 [4] NCCL INFO comm 0x55a93a0253e0 rank 4 nranks 6 cudaDev 4 busId ad000 - Abort COMPLETE
bosco:3458275:3504897 [5] NCCL INFO comm 0x55e7b7919f30 rank 5 nranks 6 cudaDev 5 busId ae000 - Abort COMPLETE
bosco:3458273:3504909 [3] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458273:3504909 [3] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458273:3504909 [3] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458273:3504909 [3] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458273:3504909 [3] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458273:3504909 [3] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458273:3504909 [3] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458273:3504909 [3] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458273:3504909 [3] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458273:3462414 [3] NCCL INFO misc/socket.cc:881 -> 3
bosco:3458271:3504911 [1] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458271:3504911 [1] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458271:3504911 [1] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458271:3504911 [1] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458271:3504911 [1] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458271:3504911 [1] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458271:3504911 [1] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458271:3504911 [1] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458271:3504911 [1] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458271:3462410 [1] NCCL INFO misc/socket.cc:881 -> 3
bosco:3458271:3462410 [1] NCCL INFO misc/socket.cc:881 -> 3
bosco:3458272:3504914 [2] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458272:3504914 [2] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458272:3504914 [2] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458272:3504914 [2] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458272:3504914 [2] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458272:3504914 [2] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458272:3504914 [2] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458272:3504914 [2] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458272:3504914 [2] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458272:3462412 [2] NCCL INFO misc/socket.cc:881 -> 3
bosco:3458271:3504911 [1] NCCL INFO comm 0x55f4dc235fa0 rank 1 nranks 6 cudaDev 1 busId 3a000 - Abort COMPLETE
bosco:3458273:3504909 [3] NCCL INFO comm 0x564814c0b420 rank 3 nranks 6 cudaDev 3 busId 3c000 - Abort COMPLETE
bosco:3458272:3462363 [2] NCCL INFO [Service thread] Connection closed by localRank 4
bosco:3458273:3462371 [3] NCCL INFO [Service thread] Connection closed by localRank 5
bosco:3458272:3504914 [2] NCCL INFO comm 0x55dcb8a3ac10 rank 2 nranks 6 cudaDev 2 busId 3b000 - Abort COMPLETE
bosco:3458272:3462363 [0] NCCL INFO [Service thread] Connection closed by localRank 1
bosco:3458270:3462364 [0] NCCL INFO [Service thread] Connection closed by localRank 1
[2025-07-08 04:10:02,342] [INFO] [launch.py:351:main] Process 3458275 exits successfully.
[2025-07-08 04:10:02,355] [INFO] [launch.py:351:main] Process 3458274 exits successfully.
bosco:3458270:3462364 [0] NCCL INFO [Service thread] Connection closed by localRank 3
[2025-07-08 04:10:03,356] [INFO] [launch.py:351:main] Process 3458272 exits successfully.
[2025-07-08 04:10:03,356] [INFO] [launch.py:351:main] Process 3458271 exits successfully.
[2025-07-08 04:10:03,357] [INFO] [launch.py:351:main] Process 3458273 exits successfully.
[rank0]:[W708 04:10:17.370618698 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
bosco:3458270:3504969 [0] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458270:3504969 [0] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458270:3504969 [0] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458270:3504969 [0] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458270:3504969 [0] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458270:3504969 [0] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458270:3504969 [0] NCCL INFO misc/socket.cc:64 -> 3
bosco:3458270:3504969 [0] NCCL INFO misc/socket.cc:80 -> 3
bosco:3458270:3504969 [0] NCCL INFO misc/socket.cc:829 -> 3
bosco:3458270:3462409 [0] NCCL INFO misc/socket.cc:881 -> 3
bosco:3458270:3504969 [0] NCCL INFO comm 0x55fc2f2890f0 rank 0 nranks 6 cudaDev 0 busId 2d000 - Abort COMPLETE
[2025-07-08 04:10:21,378] [INFO] [launch.py:351:main] Process 3458270 exits successfully.
