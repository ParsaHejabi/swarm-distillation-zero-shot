[2025-06-23 18:17:08,423] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 18:17:10,419] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-23 18:17:10,452] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-06-23 18:17:10,453] [INFO] [runner.py:610:main] cmd = /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNl19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py --deepspeed /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json --dataset_name super_glue --subset_name rte --prompt_set_name super_glue --testset_name validation --model_name_or_path T0pp --per_device_train_batch_size 1 --per_device_eval_batch_size 100 --test_mode ttt_t0 --cache_dir /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface --metric_name accuracy --debug_size -1 --peft_option lora --bottleneck_dim 1 --do_train --logging_steps 10 --num_train_epochs 50 --max_steps 1000 --adam_beta1 0.9 --adam_beta2 0.98 --adam_epsilon 1e-6 --seed 42 --debug_size 10000 --max_dev_size 1000 --learning_rate 2e-5 --evaluation_strategy steps --eval_steps 50 --disable_eval_mode 0 --pseudo_target_mode pairwise --ensemble_subset_size 0.0 --loss_option consistency --jsd 0 --detach_kl_left 1 --detach_kl_right 0 --ensemble_option avg_prob --pseudo_train_loss_weight 1.0 --pseudo_dist smooth --lora_dropout 0.3 --lora_alpha 4 --lora_pos encdec --prob_temperature 1.0 --combine_option uniform --train_random_n_prompts 5 --train_data_source validation --save_strategy steps --save_steps 10 --save_total_limit 10 --warmup_steps 100 --gradient_accumulation_steps 4 --lr_scheduler_type polynomial --output_dir /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623 --overwrite_output_dir --report_to wandb --bf16 --disable_tqdm True
[2025-06-23 18:17:12,306] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 18:17:14,307] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-23 18:17:14,340] [INFO] [launch.py:139:main] 0 NCCL_P2P_DISABLE=1
[2025-06-23 18:17:14,340] [INFO] [launch.py:139:main] 0 NCCL_DEBUG=INFO
[2025-06-23 18:17:14,340] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6]}
[2025-06-23 18:17:14,340] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=7, node_rank=0
[2025-06-23 18:17:14,340] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6]})
[2025-06-23 18:17:14,340] [INFO] [launch.py:164:main] dist_world_size=7
[2025-06-23 18:17:14,340] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6
[2025-06-23 18:17:14,358] [INFO] [launch.py:256:main] process 1948867 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=0', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'super_glue', '--subset_name', 'rte', '--prompt_set_name', 'super_glue', '--testset_name', 'validation', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-06-23 18:17:14,373] [INFO] [launch.py:256:main] process 1948868 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=1', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'super_glue', '--subset_name', 'rte', '--prompt_set_name', 'super_glue', '--testset_name', 'validation', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-06-23 18:17:14,389] [INFO] [launch.py:256:main] process 1948869 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=2', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'super_glue', '--subset_name', 'rte', '--prompt_set_name', 'super_glue', '--testset_name', 'validation', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-06-23 18:17:14,402] [INFO] [launch.py:256:main] process 1948870 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=3', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'super_glue', '--subset_name', 'rte', '--prompt_set_name', 'super_glue', '--testset_name', 'validation', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-06-23 18:17:14,414] [INFO] [launch.py:256:main] process 1948871 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=4', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'super_glue', '--subset_name', 'rte', '--prompt_set_name', 'super_glue', '--testset_name', 'validation', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-06-23 18:17:14,428] [INFO] [launch.py:256:main] process 1948872 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=5', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'super_glue', '--subset_name', 'rte', '--prompt_set_name', 'super_glue', '--testset_name', 'validation', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
[2025-06-23 18:17:14,441] [INFO] [launch.py:256:main] process 1948873 spawned with command: ['/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/bin/python3', '-u', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/examples/pytorch/t0-zero-shot/run_t0.py', '--local_rank=6', '--deepspeed', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json', '--dataset_name', 'super_glue', '--subset_name', 'rte', '--prompt_set_name', 'super_glue', '--testset_name', 'validation', '--model_name_or_path', 'T0pp', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '100', '--test_mode', 'ttt_t0', '--cache_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface', '--metric_name', 'accuracy', '--debug_size', '-1', '--peft_option', 'lora', '--bottleneck_dim', '1', '--do_train', '--logging_steps', '10', '--num_train_epochs', '50', '--max_steps', '1000', '--adam_beta1', '0.9', '--adam_beta2', '0.98', '--adam_epsilon', '1e-6', '--seed', '42', '--debug_size', '10000', '--max_dev_size', '1000', '--learning_rate', '2e-5', '--evaluation_strategy', 'steps', '--eval_steps', '50', '--disable_eval_mode', '0', '--pseudo_target_mode', 'pairwise', '--ensemble_subset_size', '0.0', '--loss_option', 'consistency', '--jsd', '0', '--detach_kl_left', '1', '--detach_kl_right', '0', '--ensemble_option', 'avg_prob', '--pseudo_train_loss_weight', '1.0', '--pseudo_dist', 'smooth', '--lora_dropout', '0.3', '--lora_alpha', '4', '--lora_pos', 'encdec', '--prob_temperature', '1.0', '--combine_option', 'uniform', '--train_random_n_prompts', '5', '--train_data_source', 'validation', '--save_strategy', 'steps', '--save_steps', '10', '--save_total_limit', '10', '--warmup_steps', '100', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'polynomial', '--output_dir', '/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623', '--overwrite_output_dir', '--report_to', 'wandb', '--bf16', '--disable_tqdm', 'True']
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-06-23 18:17:17,880] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 18:17:17,942] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/promptsource/templates.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-06-23 18:17:18,987] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 18:17:19,390] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 18:17:19,492] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 18:17:19,503] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 18:17:19,527] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 18:17:20,781] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-23 18:17:20,784] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-06-23 18:17:21,505] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-23 18:17:21,511] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-06-23 18:17:21,511] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-23 18:17:21,595] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-23 18:17:21,598] [INFO] [comm.py:675:init_distributed] cdb=None
06/23/2025 18:17:21 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
06/23/2025 18:17:22 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: False
[2025-06-23 18:17:22,375] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-23 18:17:22,381] [INFO] [comm.py:675:init_distributed] cdb=None
06/23/2025 18:17:22 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
[2025-06-23 18:17:23,083] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-23 18:17:23,089] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-06-23 18:17:23,159] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-23 18:17:23,165] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-06-23 18:17:23,177] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-23 18:17:23,184] [INFO] [comm.py:675:init_distributed] cdb=None
06/23/2025 18:17:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:604] 2025-06-23 18:17:23,422 >> loading configuration file https://huggingface.co/bigscience/T0pp/resolve/main/config.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/4d1146d0ecdfd40f42d8bb242217b1c3525f9a6527d3776ed01d77fd33a7a441.0f92c21f2009b0840bbd769b797c754010294dc29ff942ac5b8e53ce3393c880
[INFO|configuration_utils.py:641] 2025-06-23 18:17:23,426 >> Model config T5Config {
  "_name_or_path": "bigscience/T0pp",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 10240,
  "d_kv": 64,
  "d_model": 4096,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 24,
  "num_heads": 64,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.14.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

06/23/2025 18:17:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.98,
adam_epsilon=1e-06,
bf16=True,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/deepspeed_configs/ds_config_zero2.json,
disable_tqdm=True,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=50,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/runs/Jun23_18-17-17_bosco,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.POLYNOMIAL,
max_grad_norm=1.0,
max_steps=1000,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=50.0,
output_dir=/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=100,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623,
save_on_each_node=False,
save_steps=10,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=100,
weight_decay=0.0,
xpu_backend=None,
)
06/23/2025 18:17:23 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: False
06/23/2025 18:17:23 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
06/23/2025 18:17:23 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
[INFO|configuration_utils.py:604] 2025-06-23 18:17:23,708 >> loading configuration file https://huggingface.co/bigscience/T0pp/resolve/main/config.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/4d1146d0ecdfd40f42d8bb242217b1c3525f9a6527d3776ed01d77fd33a7a441.0f92c21f2009b0840bbd769b797c754010294dc29ff942ac5b8e53ce3393c880
[INFO|configuration_utils.py:641] 2025-06-23 18:17:23,709 >> Model config T5Config {
  "_name_or_path": "bigscience/T0pp",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 10240,
  "d_kv": 64,
  "d_model": 4096,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 24,
  "num_heads": 64,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.14.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|tokenization_utils_base.py:1742] 2025-06-23 18:17:24,302 >> loading file https://huggingface.co/bigscience/T0pp/resolve/main/spiece.model from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/8114f85106b092be35d1fd155f8e0198e4ab5b91bfde792524b2248496f7009b.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551
[INFO|tokenization_utils_base.py:1742] 2025-06-23 18:17:24,302 >> loading file https://huggingface.co/bigscience/T0pp/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1742] 2025-06-23 18:17:24,302 >> loading file https://huggingface.co/bigscience/T0pp/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1742] 2025-06-23 18:17:24,302 >> loading file https://huggingface.co/bigscience/T0pp/resolve/main/special_tokens_map.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/187314ce7257d07c6c662e057bb8d7b56fa65059b054b505902c338051b5f510.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46
[INFO|tokenization_utils_base.py:1742] 2025-06-23 18:17:24,302 >> loading file https://huggingface.co/bigscience/T0pp/resolve/main/tokenizer_config.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/2e98ea1bd83bead9304da27afdce5f9ffd57c298d8fa5e4bbc351c500a1cb69f.b1a2e3c152960fdc6b3d16520fa9f1591e2818d7dd66946c219e651f224894bf
[INFO|configuration_utils.py:604] 2025-06-23 18:17:24,494 >> loading configuration file https://huggingface.co/bigscience/T0pp/resolve/main/config.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/4d1146d0ecdfd40f42d8bb242217b1c3525f9a6527d3776ed01d77fd33a7a441.0f92c21f2009b0840bbd769b797c754010294dc29ff942ac5b8e53ce3393c880
[INFO|configuration_utils.py:641] 2025-06-23 18:17:24,496 >> Model config T5Config {
  "_name_or_path": "bigscience/T0pp",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 10240,
  "d_kv": 64,
  "d_model": 4096,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 24,
  "num_heads": 64,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.14.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|configuration_utils.py:604] 2025-06-23 18:17:24,783 >> loading configuration file https://huggingface.co/bigscience/T0pp/resolve/main/config.json from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/4d1146d0ecdfd40f42d8bb242217b1c3525f9a6527d3776ed01d77fd33a7a441.0f92c21f2009b0840bbd769b797c754010294dc29ff942ac5b8e53ce3393c880
[INFO|configuration_utils.py:641] 2025-06-23 18:17:24,785 >> Model config T5Config {
  "_name_or_path": "bigscience/T0pp",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 10240,
  "d_kv": 64,
  "d_model": 4096,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "num_decoder_layers": 24,
  "num_heads": 64,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "transformers_version": "4.14.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Overwrite dataset info from restored data version if exists.
06/23/2025 18:17:28 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
06/23/2025 18:17:28 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Found cached dataset super_glue (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646)
06/23/2025 18:17:28 - INFO - datasets.builder - Found cached dataset super_glue (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646)
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
06/23/2025 18:17:28 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
06/23/2025 18:17:29 - INFO - __main__ - Model parameters T5Config {
  "_name_or_path": "bigscience/T0pp",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "bottleneck_dim": 1,
  "combine_option": "uniform",
  "d_ff": 10240,
  "d_kv": 64,
  "d_model": 4096,
  "debug_size": 10000,
  "decoder_start_token_id": 0,
  "detach_kl_left": 1,
  "detach_kl_right": 0,
  "disable_eval_mode": 0,
  "dropout_rate": 0.1,
  "ensemble_option": "avg_prob",
  "ensemble_subset_size": 0.0,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "gradient_checkpointing": false,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "jsd": 0,
  "layer_norm_epsilon": 1e-06,
  "lora_alpha": 4.0,
  "lora_dropout": 0.3,
  "lora_layer_k": 24,
  "lora_pos": "encdec",
  "loss_option": "consistency",
  "max_dev_size": 1000,
  "max_early_stop_patience": 2,
  "metric_name": "accuracy",
  "min_train_steps": 300,
  "model_type": "t5",
  "num_choices": 2,
  "num_decoder_layers": 24,
  "num_heads": 64,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "peft_option": "lora",
  "prob_temperature": 1.0,
  "prune_prompt": null,
  "pseudo_dist": "smooth",
  "pseudo_target_mode": "pairwise",
  "pseudo_train_loss_weight": 1.0,
  "relative_attention_num_buckets": 32,
  "self_train_option": "none",
  "split_answer_groups": 1,
  "test_mode": "ttt_t0",
  "tie_word_embeddings": false,
  "train_data_source": "validation",
  "train_duplicates": 1,
  "train_random_n_prompts": 5,
  "transformers_version": "4.14.0.dev0",
  "use_cache": true,
  "use_deepspeed": false,
  "vocab_size": 32128
}

[INFO|modeling_utils.py:1352] 2025-06-23 18:17:29,497 >> loading weights file https://huggingface.co/bigscience/T0pp/resolve/main/pytorch_model.bin from cache at /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/1f25255af64f1faddb22578b89598cf590af9decc1f32e825724c27d067bf1f2.09d001758c33b10b097a250b8ba56686ad015c7bd75e74a56cd6d8ece9ad2d70
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.0.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.0.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.0.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.0.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.1.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.1.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.1.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.1.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.2.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.2.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.2.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.2.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.3.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.3.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.3.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.3.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.4.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.4.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.4.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.4.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.5.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.5.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.5.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.5.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.6.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.6.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.6.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.6.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.7.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.7.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.7.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.7.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.8.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.8.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.8.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.8.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.9.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.9.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.9.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.9.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.10.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.10.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.10.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.10.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.11.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.11.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.11.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.11.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.12.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.12.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.12.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.12.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.13.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.13.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.13.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.13.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.14.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.14.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.14.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.14.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.15.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.15.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.15.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.15.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.16.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.16.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.16.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.16.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.17.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.17.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.17.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.17.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.18.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.18.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.18.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.18.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.19.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.19.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.19.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.19.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.20.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.20.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.20.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.20.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.21.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.21.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.21.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.21.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.22.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.22.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.22.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.22.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.23.layer.1.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.23.layer.1.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.23.layer.1.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune encoder.block.23.layer.1.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.0.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.0.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.0.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.0.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.1.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.1.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.1.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.1.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.2.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.2.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.2.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.2.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.3.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.3.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.3.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.3.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.4.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.4.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.4.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.4.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.5.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.5.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.5.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.5.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.6.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.6.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.6.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.6.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.7.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.7.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.7.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.7.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.8.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.8.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.8.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.8.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.9.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.9.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.9.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.9.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.10.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.10.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.10.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.10.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.11.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.11.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.11.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.11.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.12.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.12.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.12.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.12.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.13.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.13.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.13.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.13.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.14.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.14.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.14.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.14.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.15.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.15.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.15.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.15.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.16.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.16.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.16.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.16.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.17.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.17.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.17.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.17.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.18.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.18.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.18.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.18.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.19.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.19.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.19.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.19.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.20.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.20.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.20.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.20.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.21.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.21.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.21.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.21.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.22.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.22.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.22.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.22.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.23.layer.2.DenseReluDense.wi_0.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.23.layer.2.DenseReluDense.wi_0.ef_lora_B
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.23.layer.2.DenseReluDense.wo.ef_lora_A
06/23/2025 18:19:14 - INFO - __main__ - tune decoder.block.23.layer.2.DenseReluDense.wo.ef_lora_B
06/23/2025 18:19:44 - INFO - __main__ - there are 10 prompts in total
06/23/2025 18:19:44 - INFO - __main__ - using 5 prompts  during training
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT training set: validation!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT training set: validation!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT training set: validation!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT training set: validation!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT training set: validation!
Overwrite dataset info from restored data version if exists.
06/23/2025 18:19:46 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
06/23/2025 18:19:46 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
Found cached dataset super_glue (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646)
06/23/2025 18:19:46 - INFO - datasets.builder - Found cached dataset super_glue (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646)
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
06/23/2025 18:19:46 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
06/23/2025 18:19:46 - INFO - __main__ - train data number prompts: 10
Building TTT training set: validation!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT training set: validation!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT evaluation test set!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT evaluation test set!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT evaluation test set!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT evaluation test set!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT evaluation test set!
Overwrite dataset info from restored data version if exists.
06/23/2025 18:19:54 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
06/23/2025 18:19:54 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
Found cached dataset super_glue (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646)
06/23/2025 18:19:54 - INFO - datasets.builder - Found cached dataset super_glue (/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646)
Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
06/23/2025 18:19:54 - INFO - datasets.info - Loading Dataset info from /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/pretrain_models/huggingface/super_glue/rte/0.0.0/3de24cf8022e94f4ee4b9d55a6f539891524d646
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT evaluation test set!
super_glue/rte has 10 original task prompts, number choices = 2, total test examples = 277
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
Building TTT evaluation test set!
06/23/2025 18:20:04 - INFO - __main__ - prompt groups [[0], [1, 2, 3, 4, 5, 6, 7, 8, 9]]
[INFO|trainer.py:430] 2025-06-23 18:20:09,279 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:450] 2025-06-23 18:20:09,281 >> Using amp half precision backend
[INFO|trainer.py:634] 2025-06-23 18:20:09,282 >> build train sampler!
[INFO|deepspeed.py:336] 2025-06-23 18:20:09,284 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[2025-06-23 18:20:09,378] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.1, git-hash=unknown, git-branch=unknown
[2025-06-23 18:20:09,379] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 7
[2025-06-23 18:20:41,413] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=7
	 self.mp_world_size=1
	 self.seq_dp_world_size=7
	 self.sequence_parallel_size=1
***********************************************
bosco:1948867:1948867 [0] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:1948867:1948867 [0] NCCL INFO cudaDriverVersion 12080
bosco:1948867:1948867 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:1948867:1948867 [0] NCCL INFO Comm config Blocking set to 1
bosco:1948872:1948872 [5] NCCL INFO cudaDriverVersion 12080
bosco:1948872:1948872 [5] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:1948872:1948872 [5] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:1948873:1948873 [6] NCCL INFO cudaDriverVersion 12080
bosco:1948873:1948873 [6] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:1948873:1948873 [6] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:1948872:1948872 [5] NCCL INFO Comm config Blocking set to 1
bosco:1948873:1948873 [6] NCCL INFO Comm config Blocking set to 1
bosco:1948870:1948870 [3] NCCL INFO cudaDriverVersion 12080
bosco:1948870:1948870 [3] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:1948870:1948870 [3] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:1948868:1948868 [1] NCCL INFO cudaDriverVersion 12080
bosco:1948870:1948870 [3] NCCL INFO Comm config Blocking set to 1
bosco:1948868:1948868 [1] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:1948868:1948868 [1] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:1948868:1948868 [1] NCCL INFO Comm config Blocking set to 1
bosco:1948869:1948869 [2] NCCL INFO cudaDriverVersion 12080
bosco:1948869:1948869 [2] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:1948869:1948869 [2] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:1948869:1948869 [2] NCCL INFO Comm config Blocking set to 1
bosco:1948867:1952599 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:1948867:1952599 [0] NCCL INFO NET/IB : No device found.
bosco:1948867:1952599 [0] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:1948867:1952599 [0] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:1948867:1952599 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:1948867:1952599 [0] NCCL INFO Using network Socket
bosco:1948867:1952599 [0] NCCL INFO ncclCommInitRankConfig comm 0x55eabed0c4a0 rank 0 nranks 7 cudaDev 0 nvmlDev 0 busId 2d000 commId 0xbf39c6ae2204b88e - Init START
bosco:1948868:1952603 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:1948868:1952603 [1] NCCL INFO NET/IB : No device found.
bosco:1948868:1952603 [1] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:1948868:1952603 [1] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:1948868:1952603 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:1948868:1952603 [1] NCCL INFO Using network Socket
bosco:1948873:1952600 [6] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:1948872:1952601 [5] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:1948873:1952600 [6] NCCL INFO NET/IB : No device found.
bosco:1948873:1952600 [6] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:1948873:1952600 [6] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:1948873:1952600 [6] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:1948873:1952600 [6] NCCL INFO Using network Socket
bosco:1948872:1952601 [5] NCCL INFO NET/IB : No device found.
bosco:1948872:1952601 [5] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:1948872:1952601 [5] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:1948872:1952601 [5] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:1948872:1952601 [5] NCCL INFO Using network Socket
bosco:1948870:1952602 [3] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:1948870:1952602 [3] NCCL INFO NET/IB : No device found.
bosco:1948870:1952602 [3] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:1948870:1952602 [3] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:1948870:1952602 [3] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:1948870:1952602 [3] NCCL INFO Using network Socket
bosco:1948868:1952603 [1] NCCL INFO ncclCommInitRankConfig comm 0x5637f3c3c2b0 rank 1 nranks 7 cudaDev 1 nvmlDev 1 busId 3a000 commId 0xbf39c6ae2204b88e - Init START
bosco:1948873:1952600 [6] NCCL INFO ncclCommInitRankConfig comm 0x55ef04303a40 rank 6 nranks 7 cudaDev 6 nvmlDev 6 busId bd000 commId 0xbf39c6ae2204b88e - Init START
bosco:1948867:1952599 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:1948872:1952601 [5] NCCL INFO ncclCommInitRankConfig comm 0x5593f8d2b2c0 rank 5 nranks 7 cudaDev 5 nvmlDev 5 busId ae000 commId 0xbf39c6ae2204b88e - Init START
bosco:1948873:1952600 [6] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:1948870:1952602 [3] NCCL INFO ncclCommInitRankConfig comm 0x55c916e88150 rank 3 nranks 7 cudaDev 3 nvmlDev 3 busId 3c000 commId 0xbf39c6ae2204b88e - Init START
bosco:1948869:1952608 [2] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:1948869:1952608 [2] NCCL INFO NET/IB : No device found.
bosco:1948869:1952608 [2] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:1948869:1952608 [2] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:1948869:1952608 [2] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:1948869:1952608 [2] NCCL INFO Using network Socket
bosco:1948869:1952608 [2] NCCL INFO ncclCommInitRankConfig comm 0x55de4e877650 rank 2 nranks 7 cudaDev 2 nvmlDev 2 busId 3b000 commId 0xbf39c6ae2204b88e - Init START
bosco:1948868:1952603 [1] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:1948869:1952608 [2] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:1948871:1948871 [4] NCCL INFO cudaDriverVersion 12080
bosco:1948871:1948871 [4] NCCL INFO Bootstrap: Using enp3s0f0:10.32.37.23<0>
bosco:1948871:1948871 [4] NCCL INFO NCCL version 2.26.2+cuda12.2
bosco:1948871:1948871 [4] NCCL INFO Comm config Blocking set to 1
bosco:1948871:1952633 [4] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
bosco:1948871:1952633 [4] NCCL INFO NET/IB : No device found.
bosco:1948871:1952633 [4] NCCL INFO NET/IB : Using [RO]; OOB enp3s0f0:10.32.37.23<0>
bosco:1948871:1952633 [4] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.32.37.23<0>
bosco:1948871:1952633 [4] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
bosco:1948871:1952633 [4] NCCL INFO Using network Socket
bosco:1948871:1952633 [4] NCCL INFO ncclCommInitRankConfig comm 0x55d9ee551700 rank 4 nranks 7 cudaDev 4 nvmlDev 4 busId ad000 commId 0xbf39c6ae2204b88e - Init START
bosco:1948870:1952602 [3] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:1948871:1952633 [4] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:1948872:1952601 [5] NCCL INFO RAS client listening socket at 127.0.0.1<28028>
bosco:1948870:1952602 [3] NCCL INFO Bootstrap timings total 1.278835 (create 0.000025, send 0.000134, recv 1.277890, ring 0.000499, delay 0.000000)
bosco:1948869:1952608 [2] NCCL INFO Bootstrap timings total 1.236145 (create 0.000019, send 0.000076, recv 0.000328, ring 1.235515, delay 0.000000)
bosco:1948871:1952633 [4] NCCL INFO Bootstrap timings total 0.001381 (create 0.000030, send 0.000141, recv 0.000555, ring 0.000366, delay 0.000000)
bosco:1948867:1952599 [0] NCCL INFO Bootstrap timings total 1.335408 (create 0.000021, send 0.000085, recv 0.042818, ring 1.282530, delay 0.000000)
bosco:1948868:1952603 [1] NCCL INFO Bootstrap timings total 1.293180 (create 0.000029, send 0.000141, recv 0.057223, ring 1.235519, delay 0.000000)
bosco:1948873:1952600 [6] NCCL INFO Bootstrap timings total 1.283247 (create 0.000032, send 0.000132, recv 0.000301, ring 1.280814, delay 0.000000)
bosco:1948872:1952601 [5] NCCL INFO Bootstrap timings total 1.281454 (create 0.000030, send 0.000143, recv 0.000127, ring 0.000170, delay 0.000000)
bosco:1948870:1952602 [3] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:1948870:1952602 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:1948870:1952602 [3] NCCL INFO NVLS multicast support is not available on dev 3
bosco:1948869:1952608 [2] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:1948869:1952608 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:1948869:1952608 [2] NCCL INFO NVLS multicast support is not available on dev 2
bosco:1948868:1952603 [1] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:1948868:1952603 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:1948868:1952603 [1] NCCL INFO NVLS multicast support is not available on dev 1
bosco:1948867:1952599 [0] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:1948867:1952599 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:1948867:1952599 [0] NCCL INFO NVLS multicast support is not available on dev 0
bosco:1948872:1952601 [5] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:1948872:1952601 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffffff00,00000000,0000ffff,ffffffff,ff000000,00000000
bosco:1948872:1952601 [5] NCCL INFO NVLS multicast support is not available on dev 5
bosco:1948871:1952633 [4] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:1948871:1952633 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffffff00,00000000,0000ffff,ffffffff,ff000000,00000000
bosco:1948871:1952633 [4] NCCL INFO NVLS multicast support is not available on dev 4
bosco:1948873:1952600 [6] NCCL INFO NCCL_P2P_DISABLE set by environment to 1
bosco:1948873:1952600 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,ffffff00,00000000,0000ffff,ffffffff,ff000000,00000000
bosco:1948873:1952600 [6] NCCL INFO NVLS multicast support is not available on dev 6
bosco:1948869:1952608 [2] NCCL INFO comm 0x55de4e877650 rank 2 nRanks 7 nNodes 1 localRanks 7 localRank 2 MNNVL 0
bosco:1948870:1952602 [3] NCCL INFO comm 0x55c916e88150 rank 3 nRanks 7 nNodes 1 localRanks 7 localRank 3 MNNVL 0
bosco:1948872:1952601 [5] NCCL INFO comm 0x5593f8d2b2c0 rank 5 nRanks 7 nNodes 1 localRanks 7 localRank 5 MNNVL 0
bosco:1948871:1952633 [4] NCCL INFO comm 0x55d9ee551700 rank 4 nRanks 7 nNodes 1 localRanks 7 localRank 4 MNNVL 0
bosco:1948867:1952599 [0] NCCL INFO comm 0x55eabed0c4a0 rank 0 nRanks 7 nNodes 1 localRanks 7 localRank 0 MNNVL 0
bosco:1948873:1952600 [6] NCCL INFO comm 0x55ef04303a40 rank 6 nRanks 7 nNodes 1 localRanks 7 localRank 6 MNNVL 0
bosco:1948868:1952603 [1] NCCL INFO comm 0x5637f3c3c2b0 rank 1 nRanks 7 nNodes 1 localRanks 7 localRank 1 MNNVL 0
bosco:1948870:1952602 [3] NCCL INFO Trees [0] -1/-1/-1->3->6 [1] -1/-1/-1->3->6 [2] -1/-1/-1->3->6 [3] -1/-1/-1->3->6
bosco:1948870:1952602 [3] NCCL INFO P2P Chunksize set to 131072
bosco:1948872:1952601 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
bosco:1948867:1952599 [0] NCCL INFO Channel 00/04 : 0 1 2 4 5 6 3
bosco:1948869:1952608 [2] NCCL INFO Trees [0] 4/-1/-1->2->1 [1] 4/-1/-1->2->1 [2] 4/-1/-1->2->1 [3] 4/-1/-1->2->1
bosco:1948872:1952601 [5] NCCL INFO P2P Chunksize set to 131072
bosco:1948867:1952599 [0] NCCL INFO Channel 01/04 : 0 1 2 4 5 6 3
bosco:1948869:1952608 [2] NCCL INFO P2P Chunksize set to 131072
bosco:1948871:1952633 [4] NCCL INFO Trees [0] 5/-1/-1->4->2 [1] 5/-1/-1->4->2 [2] 5/-1/-1->4->2 [3] 5/-1/-1->4->2
bosco:1948867:1952599 [0] NCCL INFO Channel 02/04 : 0 1 2 4 5 6 3
bosco:1948867:1952599 [0] NCCL INFO Channel 03/04 : 0 1 2 4 5 6 3
bosco:1948871:1952633 [4] NCCL INFO P2P Chunksize set to 131072
bosco:1948867:1952599 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1
bosco:1948867:1952599 [0] NCCL INFO P2P Chunksize set to 131072
bosco:1948867:1952599 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
bosco:1948873:1952600 [6] NCCL INFO Trees [0] 3/-1/-1->6->5 [1] 3/-1/-1->6->5 [2] 3/-1/-1->6->5 [3] 3/-1/-1->6->5
bosco:1948873:1952600 [6] NCCL INFO P2P Chunksize set to 131072
bosco:1948868:1952603 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
bosco:1948868:1952603 [1] NCCL INFO P2P Chunksize set to 131072
bosco:1948872:1952643 [5] NCCL INFO [Proxy Service] Device 5 CPU core 57
bosco:1948873:1952645 [6] NCCL INFO [Proxy Service] Device 6 CPU core 57
bosco:1948869:1952641 [2] NCCL INFO [Proxy Service] Device 2 CPU core 150
bosco:1948867:1952642 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
bosco:1948870:1952640 [3] NCCL INFO [Proxy Service] Device 3 CPU core 138
bosco:1948869:1952648 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 39
bosco:1948870:1952646 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 29
bosco:1948867:1952647 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
bosco:1948873:1952651 [6] NCCL INFO [Proxy Service UDS] Device 6 CPU core 176
bosco:1948871:1952644 [4] NCCL INFO [Proxy Service] Device 4 CPU core 63
bosco:1948872:1952650 [5] NCCL INFO [Proxy Service UDS] Device 5 CPU core 64
bosco:1948871:1952649 [4] NCCL INFO [Proxy Service UDS] Device 4 CPU core 64
bosco:1948868:1952652 [1] NCCL INFO [Proxy Service] Device 1 CPU core 116
bosco:1948868:1952653 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 5
bosco:1948868:1952603 [1] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948868:1952603 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948872:1952601 [5] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948872:1952601 [5] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948870:1952602 [3] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948870:1952602 [3] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948867:1952599 [0] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948867:1952599 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948869:1952608 [2] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948869:1952608 [2] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948873:1952600 [6] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948873:1952600 [6] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948871:1952633 [4] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948871:1952633 [4] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948867:1952599 [0] NCCL INFO CC Off, workFifoBytes 1048576
bosco:1948869:1952608 [2] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:1948869:1952608 [2] NCCL INFO ncclCommInitRankConfig comm 0x55de4e877650 rank 2 nranks 7 cudaDev 2 nvmlDev 2 busId 3b000 commId 0xbf39c6ae2204b88e - Init COMPLETE
bosco:1948869:1952608 [2] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 7 total 1.50 (kernels 0.15, alloc 0.01, bootstrap 1.24, allgathers 0.03, topo 0.03, graphs 0.03, connections 0.01, rest 0.00)
bosco:1948867:1952599 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:1948867:1952599 [0] NCCL INFO ncclCommInitRankConfig comm 0x55eabed0c4a0 rank 0 nranks 7 cudaDev 0 nvmlDev 0 busId 2d000 commId 0xbf39c6ae2204b88e - Init COMPLETE
bosco:1948867:1952599 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 7 total 1.59 (kernels 0.14, alloc 0.01, bootstrap 1.34, allgathers 0.03, topo 0.03, graphs 0.03, connections 0.01, rest 0.00)
bosco:1948871:1952633 [4] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:1948871:1952633 [4] NCCL INFO ncclCommInitRankConfig comm 0x55d9ee551700 rank 4 nranks 7 cudaDev 4 nvmlDev 4 busId ad000 commId 0xbf39c6ae2204b88e - Init COMPLETE
bosco:1948871:1952633 [4] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 7 total 0.27 (kernels 0.15, alloc 0.01, bootstrap 0.00, allgathers 0.00, topo 0.03, graphs 0.06, connections 0.01, rest 0.00)
bosco:1948873:1952600 [6] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:1948872:1952601 [5] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:1948872:1952601 [5] NCCL INFO ncclCommInitRankConfig comm 0x5593f8d2b2c0 rank 5 nranks 7 cudaDev 5 nvmlDev 5 busId ae000 commId 0xbf39c6ae2204b88e - Init COMPLETE
bosco:1948873:1952600 [6] NCCL INFO ncclCommInitRankConfig comm 0x55ef04303a40 rank 6 nranks 7 cudaDev 6 nvmlDev 6 busId bd000 commId 0xbf39c6ae2204b88e - Init COMPLETE
bosco:1948872:1952601 [5] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 7 total 1.56 (kernels 0.14, alloc 0.03, bootstrap 1.28, allgathers 0.01, topo 0.03, graphs 0.05, connections 0.01, rest 0.00)
bosco:1948873:1952600 [6] NCCL INFO Init timings - ncclCommInitRankConfig: rank 6 nranks 7 total 1.56 (kernels 0.14, alloc 0.03, bootstrap 1.28, allgathers 0.01, topo 0.03, graphs 0.05, connections 0.01, rest 0.00)
bosco:1948870:1952602 [3] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:1948870:1952602 [3] NCCL INFO ncclCommInitRankConfig comm 0x55c916e88150 rank 3 nranks 7 cudaDev 3 nvmlDev 3 busId 3c000 commId 0xbf39c6ae2204b88e - Init COMPLETE
bosco:1948870:1952602 [3] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 7 total 1.56 (kernels 0.15, alloc 0.03, bootstrap 1.28, allgathers 0.03, topo 0.03, graphs 0.03, connections 0.01, rest 0.00)
bosco:1948868:1952603 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
bosco:1948868:1952603 [1] NCCL INFO ncclCommInitRankConfig comm 0x5637f3c3c2b0 rank 1 nranks 7 cudaDev 1 nvmlDev 1 busId 3a000 commId 0xbf39c6ae2204b88e - Init COMPLETE
bosco:1948868:1952603 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 7 total 1.56 (kernels 0.14, alloc 0.02, bootstrap 1.29, allgathers 0.00, topo 0.03, graphs 0.06, connections 0.01, rest 0.00)
bosco:1948867:1952654 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
bosco:1948867:1952654 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
bosco:1948871:1952659 [4] NCCL INFO Channel 00 : 4[4] -> 5[5] via SHM/direct/direct
bosco:1948867:1952654 [0] NCCL INFO Channel 02 : 0[0] -> 1[1] via SHM/direct/direct
bosco:1948867:1952654 [0] NCCL INFO Channel 03 : 0[0] -> 1[1] via SHM/direct/direct
bosco:1948871:1952659 [4] NCCL INFO Channel 01 : 4[4] -> 5[5] via SHM/direct/direct
bosco:1948871:1952659 [4] NCCL INFO Channel 02 : 4[4] -> 5[5] via SHM/direct/direct
bosco:1948871:1952659 [4] NCCL INFO Channel 03 : 4[4] -> 5[5] via SHM/direct/direct
bosco:1948872:1952657 [5] NCCL INFO Channel 00 : 5[5] -> 6[6] via SHM/direct/direct
bosco:1948872:1952657 [5] NCCL INFO Channel 01 : 5[5] -> 6[6] via SHM/direct/direct
bosco:1948870:1952658 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
bosco:1948870:1952658 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
bosco:1948872:1952657 [5] NCCL INFO Channel 02 : 5[5] -> 6[6] via SHM/direct/direct
bosco:1948868:1952660 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
bosco:1948870:1952658 [3] NCCL INFO Channel 02 : 3[3] -> 0[0] via SHM/direct/direct
bosco:1948868:1952660 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
bosco:1948872:1952657 [5] NCCL INFO Channel 03 : 5[5] -> 6[6] via SHM/direct/direct
bosco:1948868:1952660 [1] NCCL INFO Channel 02 : 1[1] -> 2[2] via SHM/direct/direct
bosco:1948870:1952658 [3] NCCL INFO Channel 03 : 3[3] -> 0[0] via SHM/direct/direct
bosco:1948873:1952656 [6] NCCL INFO Channel 00 : 6[6] -> 3[3] via SHM/direct/direct
bosco:1948868:1952660 [1] NCCL INFO Channel 03 : 1[1] -> 2[2] via SHM/direct/direct
bosco:1948869:1952655 [2] NCCL INFO Channel 00 : 2[2] -> 4[4] via SHM/direct/direct
bosco:1948873:1952656 [6] NCCL INFO Channel 01 : 6[6] -> 3[3] via SHM/direct/direct
bosco:1948869:1952655 [2] NCCL INFO Channel 01 : 2[2] -> 4[4] via SHM/direct/direct
bosco:1948873:1952656 [6] NCCL INFO Channel 02 : 6[6] -> 3[3] via SHM/direct/direct
bosco:1948869:1952655 [2] NCCL INFO Channel 02 : 2[2] -> 4[4] via SHM/direct/direct
bosco:1948873:1952656 [6] NCCL INFO Channel 03 : 6[6] -> 3[3] via SHM/direct/direct
bosco:1948869:1952655 [2] NCCL INFO Channel 03 : 2[2] -> 4[4] via SHM/direct/direct
bosco:1948872:1952657 [5] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948873:1952656 [6] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948871:1952659 [4] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948870:1952658 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948867:1952654 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948869:1952655 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948868:1952660 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
[2025-06-23 18:20:43,275] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-23 18:20:43,276] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-06-23 18:20:43,276] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-23 18:20:43,290] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-23 18:20:43,290] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2025-06-23 18:20:43,290] [WARNING] [engine.py:1359:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2025-06-23 18:20:43,290] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-06-23 18:20:43,291] [INFO] [stage_1_and_2.py:151:__init__] Reduce bucket size 200000000
[2025-06-23 18:20:43,291] [INFO] [stage_1_and_2.py:152:__init__] Allgather bucket size 200000000
[2025-06-23 18:20:43,291] [INFO] [stage_1_and_2.py:153:__init__] CPU Offload: True
[2025-06-23 18:20:43,291] [INFO] [stage_1_and_2.py:154:__init__] Round robin gradient partitioning: False
run dev evaluation first to collect initial predictions
run dev evaluation first to collect initial predictions
run dev evaluation first to collect initial predictions
run dev evaluation first to collect initial predictions
run dev evaluation first to collect initial predictions
run dev evaluation first to collect initial predictions
[2025-06-23 18:20:45,549] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-06-23 18:20:45,550] [INFO] [utils.py:782:see_memory_usage] MA 20.75 GB         Max_MA 20.75 GB         CA 20.77 GB         Max_CA 21 GB 
[2025-06-23 18:20:45,551] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 170.77 GB, percent = 8.5%
[2025-06-23 18:20:45,791] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-06-23 18:20:45,792] [INFO] [utils.py:782:see_memory_usage] MA 20.75 GB         Max_MA 20.75 GB         CA 20.77 GB         Max_CA 21 GB 
[2025-06-23 18:20:45,792] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 171.72 GB, percent = 8.5%
[2025-06-23 18:20:45,792] [INFO] [stage_1_and_2.py:573:__init__] optimizer state initialized
[2025-06-23 18:20:45,985] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-06-23 18:20:45,986] [INFO] [utils.py:782:see_memory_usage] MA 20.75 GB         Max_MA 20.75 GB         CA 20.77 GB         Max_CA 21 GB 
[2025-06-23 18:20:45,986] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 171.81 GB, percent = 8.5%
[2025-06-23 18:20:45,988] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-06-23 18:20:45,988] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-23 18:20:45,988] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f7da1bd5880>
[2025-06-23 18:20:45,988] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.98)]
[2025-06-23 18:20:45,992] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-06-23 18:20:45,992] [INFO] [config.py:921:print] DeepSpeedEngine configuration:
[2025-06-23 18:20:45,993] [INFO] [config.py:925:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-23 18:20:45,993] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-06-23 18:20:45,993] [INFO] [config.py:925:print]   amp_enabled .................. False
[2025-06-23 18:20:45,993] [INFO] [config.py:925:print]   amp_params ................... False
[2025-06-23 18:20:45,993] [INFO] [config.py:925:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=True immediate_grad_update=False check_grad_overflow=False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7da1bd5970>
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   communication_data_type ...... None
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   dataloader_drop_last ......... False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   disable_allgather ............ False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   dump_state ................... False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   elasticity_enabled ........... False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   global_rank .................. 0
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   grad_accum_dtype ............. None
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 4
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   gradient_clipping ............ 1.0
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0
[2025-06-23 18:20:45,994] [INFO] [config.py:925:print]   graph_harvesting ............. False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   load_universal_checkpoint .... False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   memory_breakdown ............. False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   mics_shard_size .............. -1
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   optimizer_name ............... None
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   optimizer_params ............. None
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   pld_enabled .................. False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   pld_params ................... False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   prescale_gradients ........... False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   scheduler_name ............... None
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   scheduler_params ............. None
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   sparse_attention ............. None
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   steps_per_print .............. 2000
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   train_batch_size ............. 28
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  1
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   use_node_local_storage ....... False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   weight_quantization_config ... None
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   world_size ................... 7
[2025-06-23 18:20:45,995] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  True
[2025-06-23 18:20:45,996] [INFO] [config.py:925:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-06-23 18:20:45,996] [INFO] [config.py:925:print]   zero_enabled ................. True
[2025-06-23 18:20:45,996] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. False
[2025-06-23 18:20:45,996] [INFO] [config.py:925:print]   zero_optimization_stage ...... 2
[2025-06-23 18:20:45,996] [INFO] [config.py:911:print_user_config]   json = {
    "bfloat16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true
    }, 
    "zero_allow_untested_optimizer": true, 
    "zero_force_ds_cpu_optimizer": false, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 2.000000e+03, 
    "train_batch_size": 28, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false
}
run dev evaluation first to collect initial predictions
[INFO|trainer.py:2907] 2025-06-23 18:20:45,997 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 18:20:45,997 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 18:20:45,997 >>   Batch size = 100
bosco:1948867:1948867 [0] NCCL INFO Comm config Blocking set to 1
bosco:1948867:1952696 [0] NCCL INFO Using network Socket
bosco:1948867:1952696 [0] NCCL INFO ncclCommInitRankConfig comm 0x55eaef19bf20 rank 0 nranks 7 cudaDev 0 nvmlDev 0 busId 2d000 commId 0x3708641410309354 - Init START
bosco:1948872:1948872 [5] NCCL INFO Comm config Blocking set to 1
bosco:1948871:1948871 [4] NCCL INFO Comm config Blocking set to 1
bosco:1948868:1948868 [1] NCCL INFO Comm config Blocking set to 1
bosco:1948869:1948869 [2] NCCL INFO Comm config Blocking set to 1
bosco:1948870:1948870 [3] NCCL INFO Comm config Blocking set to 1
bosco:1948871:1952698 [4] NCCL INFO Using network Socket
bosco:1948872:1952697 [5] NCCL INFO Using network Socket
bosco:1948868:1952699 [1] NCCL INFO Using network Socket
bosco:1948871:1952698 [4] NCCL INFO ncclCommInitRankConfig comm 0x55da1e4d1500 rank 4 nranks 7 cudaDev 4 nvmlDev 4 busId ad000 commId 0x3708641410309354 - Init START
bosco:1948869:1952700 [2] NCCL INFO Using network Socket
bosco:1948870:1952701 [3] NCCL INFO Using network Socket
bosco:1948872:1952697 [5] NCCL INFO ncclCommInitRankConfig comm 0x559427c97290 rank 5 nranks 7 cudaDev 5 nvmlDev 5 busId ae000 commId 0x3708641410309354 - Init START
bosco:1948868:1952699 [1] NCCL INFO ncclCommInitRankConfig comm 0x563823b6a150 rank 1 nranks 7 cudaDev 1 nvmlDev 1 busId 3a000 commId 0x3708641410309354 - Init START
bosco:1948870:1952701 [3] NCCL INFO ncclCommInitRankConfig comm 0x55c946db9410 rank 3 nranks 7 cudaDev 3 nvmlDev 3 busId 3c000 commId 0x3708641410309354 - Init START
bosco:1948869:1952700 [2] NCCL INFO ncclCommInitRankConfig comm 0x55de7e7f4b90 rank 2 nranks 7 cudaDev 2 nvmlDev 2 busId 3b000 commId 0x3708641410309354 - Init START
bosco:1948873:1948873 [6] NCCL INFO Comm config Blocking set to 1
bosco:1948873:1952702 [6] NCCL INFO Using network Socket
bosco:1948873:1952702 [6] NCCL INFO ncclCommInitRankConfig comm 0x55ef34271710 rank 6 nranks 7 cudaDev 6 nvmlDev 6 busId bd000 commId 0x3708641410309354 - Init START
bosco:1948868:1952699 [1] NCCL INFO Bootstrap timings total 0.280487 (create 0.000037, send 0.000162, recv 0.000629, ring 0.279488, delay 0.000000)
bosco:1948872:1952697 [5] NCCL INFO Bootstrap timings total 0.280607 (create 0.000037, send 0.000169, recv 0.280092, ring 0.000158, delay 0.000000)
bosco:1948871:1952698 [4] NCCL INFO Bootstrap timings total 0.281721 (create 0.000024, send 0.000120, recv 0.001536, ring 0.279543, delay 0.000000)
bosco:1948867:1952696 [0] NCCL INFO Bootstrap timings total 0.283223 (create 0.000040, send 0.000181, recv 0.003080, ring 0.002137, delay 0.000000)
bosco:1948873:1952702 [6] NCCL INFO Bootstrap timings total 0.002906 (create 0.000035, send 0.000152, recv 0.000401, ring 0.000141, delay 0.000000)
bosco:1948870:1952701 [3] NCCL INFO Bootstrap timings total 0.280295 (create 0.000036, send 0.000164, recv 0.000318, ring 0.279411, delay 0.000000)
bosco:1948869:1952700 [2] NCCL INFO Bootstrap timings total 0.280283 (create 0.000036, send 0.000161, recv 0.000471, ring 0.279379, delay 0.000000)
bosco:1948871:1952698 [4] NCCL INFO Setting affinity for GPU 4 to ffffffff,ffffff00,00000000,0000ffff,ffffffff,ff000000,00000000
bosco:1948871:1952698 [4] NCCL INFO NVLS multicast support is not available on dev 4
bosco:1948869:1952700 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:1948869:1952700 [2] NCCL INFO NVLS multicast support is not available on dev 2
bosco:1948870:1952701 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:1948870:1952701 [3] NCCL INFO NVLS multicast support is not available on dev 3
bosco:1948872:1952697 [5] NCCL INFO Setting affinity for GPU 5 to ffffffff,ffffff00,00000000,0000ffff,ffffffff,ff000000,00000000
bosco:1948872:1952697 [5] NCCL INFO NVLS multicast support is not available on dev 5
bosco:1948868:1952699 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:1948868:1952699 [1] NCCL INFO NVLS multicast support is not available on dev 1
bosco:1948867:1952696 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffff0000,00000000,00ffffff,ffffffff
bosco:1948867:1952696 [0] NCCL INFO NVLS multicast support is not available on dev 0
bosco:1948873:1952702 [6] NCCL INFO Setting affinity for GPU 6 to ffffffff,ffffff00,00000000,0000ffff,ffffffff,ff000000,00000000
bosco:1948873:1952702 [6] NCCL INFO NVLS multicast support is not available on dev 6
bosco:1948871:1952698 [4] NCCL INFO comm 0x55da1e4d1500 rank 4 nRanks 7 nNodes 1 localRanks 7 localRank 4 MNNVL 0
bosco:1948870:1952701 [3] NCCL INFO comm 0x55c946db9410 rank 3 nRanks 7 nNodes 1 localRanks 7 localRank 3 MNNVL 0
bosco:1948872:1952697 [5] NCCL INFO comm 0x559427c97290 rank 5 nRanks 7 nNodes 1 localRanks 7 localRank 5 MNNVL 0
bosco:1948873:1952702 [6] NCCL INFO comm 0x55ef34271710 rank 6 nRanks 7 nNodes 1 localRanks 7 localRank 6 MNNVL 0
bosco:1948869:1952700 [2] NCCL INFO comm 0x55de7e7f4b90 rank 2 nRanks 7 nNodes 1 localRanks 7 localRank 2 MNNVL 0
bosco:1948871:1952698 [4] NCCL INFO Trees [0] 5/-1/-1->4->2 [1] 5/-1/-1->4->2 [2] 5/-1/-1->4->2 [3] 5/-1/-1->4->2
bosco:1948867:1952696 [0] NCCL INFO comm 0x55eaef19bf20 rank 0 nRanks 7 nNodes 1 localRanks 7 localRank 0 MNNVL 0
bosco:1948871:1952698 [4] NCCL INFO P2P Chunksize set to 131072
bosco:1948869:1952700 [2] NCCL INFO Trees [0] 4/-1/-1->2->1 [1] 4/-1/-1->2->1 [2] 4/-1/-1->2->1 [3] 4/-1/-1->2->1
bosco:1948870:1952701 [3] NCCL INFO Trees [0] -1/-1/-1->3->6 [1] -1/-1/-1->3->6 [2] -1/-1/-1->3->6 [3] -1/-1/-1->3->6
bosco:1948868:1952699 [1] NCCL INFO comm 0x563823b6a150 rank 1 nRanks 7 nNodes 1 localRanks 7 localRank 1 MNNVL 0
bosco:1948869:1952700 [2] NCCL INFO P2P Chunksize set to 131072
bosco:1948870:1952701 [3] NCCL INFO P2P Chunksize set to 131072
bosco:1948867:1952696 [0] NCCL INFO Channel 00/04 : 0 1 2 4 5 6 3
bosco:1948872:1952697 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
bosco:1948872:1952697 [5] NCCL INFO P2P Chunksize set to 131072
bosco:1948867:1952696 [0] NCCL INFO Channel 01/04 : 0 1 2 4 5 6 3
bosco:1948867:1952696 [0] NCCL INFO Channel 02/04 : 0 1 2 4 5 6 3
bosco:1948867:1952696 [0] NCCL INFO Channel 03/04 : 0 1 2 4 5 6 3
bosco:1948867:1952696 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1
bosco:1948867:1952696 [0] NCCL INFO P2P Chunksize set to 131072
bosco:1948868:1952699 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
bosco:1948868:1952699 [1] NCCL INFO P2P Chunksize set to 131072
bosco:1948867:1952696 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
bosco:1948873:1952702 [6] NCCL INFO Trees [0] 3/-1/-1->6->5 [1] 3/-1/-1->6->5 [2] 3/-1/-1->6->5 [3] 3/-1/-1->6->5
bosco:1948873:1952702 [6] NCCL INFO P2P Chunksize set to 131072
bosco:1948867:1952713 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 147
bosco:1948871:1952703 [4] NCCL INFO [Proxy Service] Device 4 CPU core 72
bosco:1948867:1952705 [0] NCCL INFO [Proxy Service] Device 0 CPU core 34
bosco:1948871:1952708 [4] NCCL INFO [Proxy Service UDS] Device 4 CPU core 73
bosco:1948873:1952716 [6] NCCL INFO [Proxy Service UDS] Device 6 CPU core 60
bosco:1948872:1952714 [5] NCCL INFO [Proxy Service UDS] Device 5 CPU core 59
bosco:1948873:1952709 [6] NCCL INFO [Proxy Service] Device 6 CPU core 58
bosco:1948869:1952704 [2] NCCL INFO [Proxy Service] Device 2 CPU core 3
bosco:1948872:1952707 [5] NCCL INFO [Proxy Service] Device 5 CPU core 170
bosco:1948869:1952711 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 119
bosco:1948868:1952710 [1] NCCL INFO [Proxy Service] Device 1 CPU core 7
bosco:1948868:1952715 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 121
bosco:1948870:1952712 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 120
bosco:1948870:1952706 [3] NCCL INFO [Proxy Service] Device 3 CPU core 5
bosco:1948872:1952697 [5] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948872:1952697 [5] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948871:1952698 [4] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948871:1952698 [4] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948870:1952701 [3] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948870:1952701 [3] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948873:1952702 [6] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948873:1952702 [6] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948869:1952700 [2] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948869:1952700 [2] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948868:1952699 [1] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948868:1952699 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948867:1952696 [0] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
bosco:1948867:1952696 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
bosco:1948867:1952696 [0] NCCL INFO CC Off, workFifoBytes 1048576
bosco:1948871:1952698 [4] NCCL INFO ncclCommInitRankConfig comm 0x55da1e4d1500 rank 4 nranks 7 cudaDev 4 nvmlDev 4 busId ad000 commId 0x3708641410309354 - Init COMPLETE
bosco:1948871:1952698 [4] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 7 total 0.42 (kernels 0.00, alloc 0.00, bootstrap 0.28, allgathers 0.06, topo 0.03, graphs 0.03, connections 0.01, rest 0.00)
bosco:1948869:1952700 [2] NCCL INFO ncclCommInitRankConfig comm 0x55de7e7f4b90 rank 2 nranks 7 cudaDev 2 nvmlDev 2 busId 3b000 commId 0x3708641410309354 - Init COMPLETE
bosco:1948869:1952700 [2] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 7 total 0.42 (kernels 0.00, alloc 0.00, bootstrap 0.28, allgathers 0.06, topo 0.03, graphs 0.03, connections 0.01, rest 0.00)
bosco:1948873:1952702 [6] NCCL INFO ncclCommInitRankConfig comm 0x55ef34271710 rank 6 nranks 7 cudaDev 6 nvmlDev 6 busId bd000 commId 0x3708641410309354 - Init COMPLETE
bosco:1948867:1952696 [0] NCCL INFO ncclCommInitRankConfig comm 0x55eaef19bf20 rank 0 nranks 7 cudaDev 0 nvmlDev 0 busId 2d000 commId 0x3708641410309354 - Init COMPLETE
bosco:1948873:1952702 [6] NCCL INFO Init timings - ncclCommInitRankConfig: rank 6 nranks 7 total 0.15 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.03, graphs 0.09, connections 0.01, rest 0.00)
bosco:1948867:1952696 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 7 total 0.43 (kernels 0.00, alloc 0.00, bootstrap 0.28, allgathers 0.03, topo 0.03, graphs 0.06, connections 0.01, rest 0.00)
bosco:1948870:1952701 [3] NCCL INFO ncclCommInitRankConfig comm 0x55c946db9410 rank 3 nranks 7 cudaDev 3 nvmlDev 3 busId 3c000 commId 0x3708641410309354 - Init COMPLETE
bosco:1948870:1952701 [3] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 7 total 0.42 (kernels 0.00, alloc 0.00, bootstrap 0.28, allgathers 0.04, topo 0.03, graphs 0.06, connections 0.01, rest 0.00)
bosco:1948872:1952697 [5] NCCL INFO ncclCommInitRankConfig comm 0x559427c97290 rank 5 nranks 7 cudaDev 5 nvmlDev 5 busId ae000 commId 0x3708641410309354 - Init COMPLETE
bosco:1948868:1952699 [1] NCCL INFO ncclCommInitRankConfig comm 0x563823b6a150 rank 1 nranks 7 cudaDev 1 nvmlDev 1 busId 3a000 commId 0x3708641410309354 - Init COMPLETE
bosco:1948872:1952697 [5] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 7 total 0.42 (kernels 0.00, alloc 0.00, bootstrap 0.28, allgathers 0.00, topo 0.03, graphs 0.09, connections 0.01, rest 0.00)
bosco:1948868:1952699 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 7 total 0.42 (kernels 0.00, alloc 0.00, bootstrap 0.28, allgathers 0.03, topo 0.03, graphs 0.06, connections 0.01, rest 0.00)
bosco:1948871:1952718 [4] NCCL INFO Channel 00 : 4[4] -> 5[5] via SHM/direct/direct
bosco:1948867:1952720 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
bosco:1948867:1952720 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
bosco:1948871:1952718 [4] NCCL INFO Channel 01 : 4[4] -> 5[5] via SHM/direct/direct
bosco:1948867:1952720 [0] NCCL INFO Channel 02 : 0[0] -> 1[1] via SHM/direct/direct
bosco:1948867:1952720 [0] NCCL INFO Channel 03 : 0[0] -> 1[1] via SHM/direct/direct
bosco:1948871:1952718 [4] NCCL INFO Channel 02 : 4[4] -> 5[5] via SHM/direct/direct
bosco:1948871:1952718 [4] NCCL INFO Channel 03 : 4[4] -> 5[5] via SHM/direct/direct
bosco:1948872:1952721 [5] NCCL INFO Channel 00 : 5[5] -> 6[6] via SHM/direct/direct
bosco:1948870:1952719 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
bosco:1948868:1952722 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
bosco:1948872:1952721 [5] NCCL INFO Channel 01 : 5[5] -> 6[6] via SHM/direct/direct
bosco:1948872:1952721 [5] NCCL INFO Channel 02 : 5[5] -> 6[6] via SHM/direct/direct
bosco:1948868:1952722 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
bosco:1948870:1952719 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
bosco:1948870:1952719 [3] NCCL INFO Channel 02 : 3[3] -> 0[0] via SHM/direct/direct
bosco:1948868:1952722 [1] NCCL INFO Channel 02 : 1[1] -> 2[2] via SHM/direct/direct
bosco:1948872:1952721 [5] NCCL INFO Channel 03 : 5[5] -> 6[6] via SHM/direct/direct
bosco:1948868:1952722 [1] NCCL INFO Channel 03 : 1[1] -> 2[2] via SHM/direct/direct
bosco:1948873:1952717 [6] NCCL INFO Channel 00 : 6[6] -> 3[3] via SHM/direct/direct
bosco:1948870:1952719 [3] NCCL INFO Channel 03 : 3[3] -> 0[0] via SHM/direct/direct
bosco:1948873:1952717 [6] NCCL INFO Channel 01 : 6[6] -> 3[3] via SHM/direct/direct
bosco:1948869:1952723 [2] NCCL INFO Channel 00 : 2[2] -> 4[4] via SHM/direct/direct
bosco:1948873:1952717 [6] NCCL INFO Channel 02 : 6[6] -> 3[3] via SHM/direct/direct
bosco:1948869:1952723 [2] NCCL INFO Channel 01 : 2[2] -> 4[4] via SHM/direct/direct
bosco:1948873:1952717 [6] NCCL INFO Channel 03 : 6[6] -> 3[3] via SHM/direct/direct
bosco:1948869:1952723 [2] NCCL INFO Channel 02 : 2[2] -> 4[4] via SHM/direct/direct
bosco:1948869:1952723 [2] NCCL INFO Channel 03 : 2[2] -> 4[4] via SHM/direct/direct
bosco:1948870:1952719 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948869:1952723 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948868:1952722 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948867:1952720 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948873:1952717 [6] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948872:1952721 [5] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
bosco:1948871:1952718 [4] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
finish collecting initial predictions before optimization
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.69314066410481 0.69298426068761 0.6841994535753173 0.6798927230435231 0.6429795215301972 0.6560306638491367
avg entropy: 0.6729323563153212
avg cont entropy: 0.6836025318868095
finish collecting initial predictions before optimization
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.69314066410481 0.69298426068761 0.6841994535753173 0.6798927230435231 0.6429795215301972 0.6560306638491367
avg entropy: 0.6729323563153212
avg cont entropy: 0.6836025318868095
finish collecting initial predictions before optimization
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.69314066410481 0.69298426068761 0.6841994535753173 0.6798927230435231 0.6429795215301972 0.6560306638491367
avg entropy: 0.6729323563153212
avg cont entropy: 0.6836025318868095
finish collecting initial predictions before optimization
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.69314066410481 0.69298426068761 0.6841994535753173 0.6798927230435231 0.6429795215301972 0.6560306638491367
avg entropy: 0.6729323563153212
avg cont entropy: 0.6836025318868095
finish collecting initial predictions before optimization
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.69314066410481 0.69298426068761 0.6841994535753173 0.6798927230435231 0.6429795215301972 0.6560306638491367
avg entropy: 0.6729323563153212
avg cont entropy: 0.6836025318868095
finish collecting initial predictions before optimization
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.69314066410481 0.69298426068761 0.6841994535753173 0.6798927230435231 0.6429795215301972 0.6560306638491367
avg entropy: 0.6729323563153212
avg cont entropy: 0.6836025318868095
finish collecting initial predictions before optimization
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.69314066410481 0.69298426068761 0.6841994535753173 0.6798927230435231 0.6429795215301972 0.6560306638491367
avg entropy: 0.6729323563153212
avg cont entropy: 0.6836025318868095
[INFO|trainer.py:1740] 2025-06-23 18:21:13,019 >> ***** Running training *****
[INFO|trainer.py:1741] 2025-06-23 18:21:13,019 >>   Num examples = 277
[INFO|trainer.py:1742] 2025-06-23 18:21:13,019 >>   Num Epochs = 100
[INFO|trainer.py:1743] 2025-06-23 18:21:13,019 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1744] 2025-06-23 18:21:13,019 >>   Total train batch size (w. parallel, distributed & accumulation) = 28
[INFO|trainer.py:1745] 2025-06-23 18:21:13,019 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1746] 2025-06-23 18:21:13,019 >>   Total optimization steps = 1000
[INFO|integrations.py:502] 2025-06-23 18:21:13,021 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: parsahejabi (parsahejabi-academic-team) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/exps_ttt/wandb/run-20250623_182113-d7bynzwx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623
wandb: ⭐️ View project at https://wandb.ai/parsahejabi-academic-team/swarm-distillation
wandb: 🚀 View run at https://wandb.ai/parsahejabi-academic-team/swarm-distillation/runs/d7bynzwx
{'loss': 0.0131, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.0}
[INFO|trainer.py:2655] 2025-06-23 18:22:12,090 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-10
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-06-23 18:22:12,091 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-10/config.json
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|modeling_utils.py:1070] 2025-06-23 18:22:39,907 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-10/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 18:22:39,910 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 18:22:39,910 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-10/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 18:22:39,937 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-10/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 18:22:39,942] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step10 is begin to save!
[2025-06-23 18:22:50,637] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-10/global_step10/mp_rank_00_model_states.pt
[2025-06-23 18:23:41,905] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step10 is begin to save!
[2025-06-23 18:23:52,519] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-10/global_step10/mp_rank_00_model_states.pt
{'loss': 0.0136, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.0}
[INFO|trainer.py:2655] 2025-06-23 18:25:44,870 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-20
[INFO|configuration_utils.py:425] 2025-06-23 18:25:44,871 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-20/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 18:26:11,352 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-20/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 18:26:11,355 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 18:26:11,356 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-20/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 18:26:11,383 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-20/spiece.model
[2025-06-23 18:26:11,387] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step20 is begin to save!
[2025-06-23 18:26:21,999] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-20/global_step20/mp_rank_00_model_states.pt
[2025-06-23 18:27:15,640] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step20 is begin to save!
[2025-06-23 18:27:26,335] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-20/global_step20/mp_rank_00_model_states.pt
{'loss': 0.0135, 'learning_rate': 6e-06, 'epoch': 3.0}
[INFO|trainer.py:2655] 2025-06-23 18:29:24,914 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-30
[INFO|configuration_utils.py:425] 2025-06-23 18:29:24,916 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-30/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 18:29:51,498 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-30/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 18:29:51,501 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 18:29:51,501 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-30/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 18:29:51,529 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-30/spiece.model
[2025-06-23 18:29:51,534] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step30 is begin to save!
[2025-06-23 18:30:02,295] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-30/global_step30/mp_rank_00_model_states.pt
[2025-06-23 18:30:52,437] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step30 is begin to save!
[2025-06-23 18:31:03,126] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-30/global_step30/mp_rank_00_model_states.pt
{'loss': 0.0145, 'learning_rate': 8.000000000000001e-06, 'epoch': 4.0}
[INFO|trainer.py:2655] 2025-06-23 18:32:56,821 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-40
[INFO|configuration_utils.py:425] 2025-06-23 18:32:56,822 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-40/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 18:33:23,489 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-40/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 18:33:23,492 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 18:33:23,493 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-40/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 18:33:23,521 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-40/spiece.model
[2025-06-23 18:33:23,525] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step40 is begin to save!
[2025-06-23 18:33:34,116] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-40/global_step40/mp_rank_00_model_states.pt
[2025-06-23 18:34:27,499] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step40 is begin to save!
[2025-06-23 18:34:38,127] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-40/global_step40/mp_rank_00_model_states.pt
{'loss': 0.0138, 'learning_rate': 1e-05, 'epoch': 5.0}
[INFO|trainer.py:2907] 2025-06-23 18:36:34,214 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 18:36:34,214 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 18:36:34,214 >>   Batch size = 100
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.39), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.39), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.39), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.39), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.39), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.39), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.39), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
[INFO|trainer.py:2907] 2025-06-23 18:37:02,694 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 18:37:02,694 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 18:37:02,694 >>   Batch size = 100
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6810494166250649 0.6930885314445381 0.69298426068761 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6728285236427223
avg cont entropy: 0.6835962628648032
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 -0.00110327425553991 -5.213266027193075e-05 0.0 0.0 0.0011566935815418011 0.0 0.0
delta avg entropy: -0.00010383267259892471
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6810494166250649 0.6930885314445381 0.69298426068761 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6728285236427223
avg cont entropy: 0.6835962628648032
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 -0.00110327425553991 -5.213266027193075e-05 0.0 0.0 0.0011566935815418011 0.0 0.0
delta avg entropy: -0.00010383267259892471
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6810494166250649 0.6930885314445381 0.69298426068761 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6728285236427223
avg cont entropy: 0.6835962628648032
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 -0.00110327425553991 -5.213266027193075e-05 0.0 0.0 0.0011566935815418011 0.0 0.0
delta avg entropy: -0.00010383267259892471
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6810494166250649 0.6930885314445381 0.69298426068761 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.6560306638491367all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6810494166250649 0.6930885314445381 0.69298426068761 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.6560306638491367

avg entropy: 0.6728285236427223
avg cont entropy: 0.6835962628648032avg entropy: 0.6728285236427223

avg cont entropy: 0.6835962628648032
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 -0.00110327425553991 -5.213266027193075e-05 0.0 0.0 0.0011566935815418011 0.0 0.0
delta avg entropy: -0.00010383267259892471
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 -0.00110327425553991 -5.213266027193075e-05 0.0 0.0 0.0011566935815418011 0.0 0.0
delta avg entropy: -0.00010383267259892471
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6810494166250649 0.6930885314445381 0.69298426068761 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6728285236427223
avg cont entropy: 0.6835962628648032
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 -0.00110327425553991 -5.213266027193075e-05 0.0 0.0 0.0011566935815418011 0.0 0.0
delta avg entropy: -0.00010383267259892471
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6810494166250649 0.6930885314445381 0.69298426068761 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6728285236427223
avg cont entropy: 0.6835962628648032
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 -0.00110327425553991 -5.213266027193075e-05 0.0 0.0 0.0011566935815418011 0.0 0.0
delta avg entropy: -0.00010383267259892471
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 18:37:29,925 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-50
[INFO|configuration_utils.py:425] 2025-06-23 18:37:29,926 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-50/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 18:37:57,452 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-50/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 18:37:57,455 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-50/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 18:37:57,455 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-50/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 18:37:57,485 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-50/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 18:37:57,490] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is begin to save!
[2025-06-23 18:38:08,451] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-50/global_step50/mp_rank_00_model_states.pt
[2025-06-23 18:38:58,348] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is begin to save!
[2025-06-23 18:39:09,381] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-50/global_step50/mp_rank_00_model_states.pt
{'loss': 0.0134, 'learning_rate': 1.2e-05, 'epoch': 6.0}
[INFO|trainer.py:2655] 2025-06-23 18:41:02,782 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-60
[INFO|configuration_utils.py:425] 2025-06-23 18:41:02,783 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-60/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 18:41:29,081 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-60/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 18:41:29,084 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-60/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 18:41:29,085 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-60/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 18:41:29,114 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-60/spiece.model
[2025-06-23 18:41:29,119] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step60 is begin to save!
[2025-06-23 18:41:39,878] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-60/global_step60/mp_rank_00_model_states.pt
[2025-06-23 18:42:32,424] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step60 is begin to save!
[2025-06-23 18:42:43,219] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-60/global_step60/mp_rank_00_model_states.pt
{'loss': 0.0131, 'learning_rate': 1.4e-05, 'epoch': 7.0}
[INFO|trainer.py:2655] 2025-06-23 18:44:37,519 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-70
[INFO|configuration_utils.py:425] 2025-06-23 18:44:37,521 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-70/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 18:45:03,603 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-70/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 18:45:03,606 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 18:45:03,607 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-70/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 18:45:03,637 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-70/spiece.model
[2025-06-23 18:45:03,642] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step70 is begin to save!
[2025-06-23 18:45:14,298] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-70/global_step70/mp_rank_00_model_states.pt
[2025-06-23 18:46:06,531] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step70 is begin to save!
[2025-06-23 18:46:17,200] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-70/global_step70/mp_rank_00_model_states.pt
{'loss': 0.0128, 'learning_rate': 1.6000000000000003e-05, 'epoch': 8.0}
[INFO|trainer.py:2655] 2025-06-23 18:48:10,622 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-80
[INFO|configuration_utils.py:425] 2025-06-23 18:48:10,623 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-80/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 18:48:38,835 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 18:48:38,838 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 18:48:38,838 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-80/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 18:48:38,869 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-80/spiece.model
[2025-06-23 18:48:38,874] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step80 is begin to save!
[2025-06-23 18:48:51,776] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-80/global_step80/mp_rank_00_model_states.pt
[2025-06-23 18:49:58,390] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step80 is begin to save!
[2025-06-23 18:50:09,025] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-80/global_step80/mp_rank_00_model_states.pt
{'loss': 0.014, 'learning_rate': 1.8e-05, 'epoch': 9.0}
[INFO|trainer.py:2655] 2025-06-23 18:52:00,622 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-90
[INFO|configuration_utils.py:425] 2025-06-23 18:52:00,623 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-90/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 18:52:27,653 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-90/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 18:52:27,656 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-90/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 18:52:27,656 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-90/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 18:52:27,684 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-90/spiece.model
[2025-06-23 18:52:27,689] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step90 is begin to save!
[2025-06-23 18:52:38,094] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-90/global_step90/mp_rank_00_model_states.pt
[2025-06-23 18:53:29,880] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step90 is begin to save!
[2025-06-23 18:53:40,264] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-90/global_step90/mp_rank_00_model_states.pt
{'loss': 0.0134, 'learning_rate': 2e-05, 'epoch': 10.0}
[INFO|trainer.py:2907] 2025-06-23 18:55:33,676 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 18:55:33,676 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 18:55:33,677 >>   Batch size = 100
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.51), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.51), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.51), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.51), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.51), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.51), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.51), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
[INFO|trainer.py:2907] 2025-06-23 18:56:02,017 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 18:56:02,018 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 18:56:02,019 >>   Batch size = 100
all entropy: 0.6519079859154411 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6928278409578548 0.6930885314445381 0.6851431626922719 0.678682484276075 0.6429795215301972 0.6560306638491367
avg entropy: 0.6725655618947626
avg cont entropy: 0.6835774532311052
delta all entropy: -0.0020895879095259584 0.0 0.0 -0.00110327425553991 -0.0003128231469552434 0.00010427075692809318 0.0009437091169546274 -0.0012102387674480797 0.0 0.0
delta avg entropy: -0.0003667944205586471
all entropy: 0.6519079859154411 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6928278409578548 0.6930885314445381 0.6851431626922719 0.678682484276075 0.6429795215301972 0.6560306638491367
avg entropy: 0.6725655618947626
avg cont entropy: 0.6835774532311052
delta all entropy: -0.0020895879095259584 0.0 0.0 -0.00110327425553991 -0.0003128231469552434 0.00010427075692809318 0.0009437091169546274 -0.0012102387674480797 0.0 0.0
delta avg entropy: -0.0003667944205586471
all entropy: 0.6519079859154411 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6928278409578548 0.6930885314445381 0.6851431626922719 0.678682484276075 0.6429795215301972 0.6560306638491367
all entropy: 0.6519079859154411 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6928278409578548 0.6930885314445381 0.6851431626922719 0.678682484276075 0.6429795215301972 0.6560306638491367all entropy: 0.6519079859154411 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6928278409578548 0.6930885314445381 0.6851431626922719 0.678682484276075 0.6429795215301972 0.6560306638491367avg entropy: 0.6725655618947626

avg cont entropy: 0.6835774532311052

avg entropy: 0.6725655618947626
avg cont entropy: 0.6835774532311052
delta all entropy: -0.0020895879095259584 0.0 0.0 -0.00110327425553991 -0.0003128231469552434 0.00010427075692809318 0.0009437091169546274 -0.0012102387674480797 0.0 0.0
delta avg entropy: -0.0003667944205586471
avg entropy: 0.6725655618947626
delta all entropy: -0.0020895879095259584 0.0 0.0 -0.00110327425553991 -0.0003128231469552434 0.00010427075692809318 0.0009437091169546274 -0.0012102387674480797 0.0 0.0avg cont entropy: 0.6835774532311052

delta avg entropy: -0.0003667944205586471
delta all entropy: -0.0020895879095259584 0.0 0.0 -0.00110327425553991 -0.0003128231469552434 0.00010427075692809318 0.0009437091169546274 -0.0012102387674480797 0.0 0.0
delta avg entropy: -0.0003667944205586471
all entropy: 0.6519079859154411 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6928278409578548 0.6930885314445381 0.6851431626922719 0.678682484276075 0.6429795215301972 0.6560306638491367
avg entropy: 0.6725655618947626
avg cont entropy: 0.6835774532311052
delta all entropy: -0.0020895879095259584 0.0 0.0 -0.00110327425553991 -0.0003128231469552434 0.00010427075692809318 0.0009437091169546274 -0.0012102387674480797 0.0 0.0
delta avg entropy: -0.0003667944205586471
all entropy: 0.6519079859154411 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6928278409578548 0.6930885314445381 0.6851431626922719 0.678682484276075 0.6429795215301972 0.6560306638491367
avg entropy: 0.6725655618947626
avg cont entropy: 0.6835774532311052
delta all entropy: -0.0020895879095259584 0.0 0.0 -0.00110327425553991 -0.0003128231469552434 0.00010427075692809318 0.0009437091169546274 -0.0012102387674480797 0.0 0.0
delta avg entropy: -0.0003667944205586471
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 18:56:28,911 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-100
[INFO|configuration_utils.py:425] 2025-06-23 18:56:28,912 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-100/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 18:56:55,664 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-100/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 18:56:55,667 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 18:56:55,668 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-100/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 18:56:55,697 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-100/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 18:56:55,702] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is begin to save!
[2025-06-23 18:57:06,316] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-100/global_step100/mp_rank_00_model_states.pt
[2025-06-23 18:58:01,233] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is begin to save!
[2025-06-23 18:58:12,862] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-100/global_step100/mp_rank_00_model_states.pt
{'loss': 0.013, 'learning_rate': 1.9778888888888892e-05, 'epoch': 11.0}
[INFO|trainer.py:2655] 2025-06-23 19:00:07,943 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-110
[INFO|configuration_utils.py:425] 2025-06-23 19:00:07,944 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-110/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:00:33,817 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-110/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:00:33,820 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-110/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:00:33,820 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-110/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:00:33,849 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-110/spiece.model
[2025-06-23 19:00:33,854] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step110 is begin to save!
[2025-06-23 19:00:48,775] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-110/global_step110/mp_rank_00_model_states.pt
[2025-06-23 19:01:41,545] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step110 is begin to save!
[2025-06-23 19:01:53,437] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-110/global_step110/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:02:52,370 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-10] due to args.save_total_limit
{'loss': 0.0131, 'learning_rate': 1.955777777777778e-05, 'epoch': 12.0}
[INFO|trainer.py:2655] 2025-06-23 19:03:54,347 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-120
[INFO|configuration_utils.py:425] 2025-06-23 19:03:54,349 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-120/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:04:21,164 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:04:21,167 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:04:21,168 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-120/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:04:21,210 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-120/spiece.model
[2025-06-23 19:04:21,216] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step120 is begin to save!
[2025-06-23 19:04:32,442] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-120/global_step120/mp_rank_00_model_states.pt
[2025-06-23 19:05:21,939] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step120 is begin to save!
[2025-06-23 19:05:33,289] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-120/global_step120/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:06:32,871 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-20] due to args.save_total_limit
{'loss': 0.0148, 'learning_rate': 1.9336666666666667e-05, 'epoch': 13.0}
[INFO|trainer.py:2655] 2025-06-23 19:07:35,800 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-130
[INFO|configuration_utils.py:425] 2025-06-23 19:07:35,802 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-130/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:08:02,417 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-130/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:08:02,420 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-130/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:08:02,420 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-130/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:08:02,450 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-130/spiece.model
[2025-06-23 19:08:02,455] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step130 is begin to save!
[2025-06-23 19:08:13,515] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-130/global_step130/mp_rank_00_model_states.pt
[2025-06-23 19:09:07,304] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step130 is begin to save!
[2025-06-23 19:09:18,365] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-130/global_step130/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:10:18,988 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-30] due to args.save_total_limit
{'loss': 0.0134, 'learning_rate': 1.9115555555555558e-05, 'epoch': 14.0}
[INFO|trainer.py:2655] 2025-06-23 19:11:22,579 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-140
[INFO|configuration_utils.py:425] 2025-06-23 19:11:22,581 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-140/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:11:50,621 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-140/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:11:50,625 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:11:50,625 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-140/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:11:50,655 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-140/spiece.model
[2025-06-23 19:11:50,660] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step140 is begin to save!
[2025-06-23 19:12:01,786] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-140/global_step140/mp_rank_00_model_states.pt
[2025-06-23 19:12:53,577] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step140 is begin to save!
[2025-06-23 19:13:04,767] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-140/global_step140/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:14:04,338 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-40] due to args.save_total_limit
{'loss': 0.0139, 'learning_rate': 1.8894444444444446e-05, 'epoch': 15.0}
[INFO|trainer.py:2907] 2025-06-23 19:15:07,418 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 19:15:07,418 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 19:15:07,419 >>   Batch size = 100
{'max_accuracy': np.float64(89.89), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.35), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(89.89), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.35), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(89.89), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.35), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
[INFO|trainer.py:2907] 2025-06-23 19:15:34,806 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 19:15:34,806 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 19:15:34,807 >>   Batch size = 100
{'max_accuracy': np.float64(89.89), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.35), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(89.89), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.35), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(89.89), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.35), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(89.89), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.2), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.35), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
all entropy: 0.6497616500313415 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6930885314445381 0.6930885314445381 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.653997573824967
avg entropy: 0.6723160106758075
avg cont entropy: 0.6836223666989683
delta all entropy: -0.0042359237936255845 0.0 0.0 -0.00110327425553991 -5.213266027193075e-05 0.00010427075692809318 0.0 0.0011566935815418011 0.0 -0.002033090024169626
delta avg entropy: -0.0006163456395137157
all entropy: 0.6497616500313415 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6930885314445381 0.6930885314445381 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.653997573824967
avg entropy: 0.6723160106758075
avg cont entropy: 0.6836223666989683
delta all entropy: -0.0042359237936255845 0.0 0.0 -0.00110327425553991 -5.213266027193075e-05 0.00010427075692809318 0.0 0.0011566935815418011 0.0 -0.002033090024169626
delta avg entropy: -0.0006163456395137157
all entropy: 0.6497616500313415 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6930885314445381 0.6930885314445381 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.653997573824967
avg entropy: 0.6723160106758075
avg cont entropy: 0.6836223666989683
delta all entropy: -0.0042359237936255845 0.0 0.0 -0.00110327425553991 -5.213266027193075e-05 0.00010427075692809318 0.0 0.0011566935815418011 0.0 -0.002033090024169626
delta avg entropy: -0.0006163456395137157
all entropy: 0.6497616500313415 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6930885314445381 0.6930885314445381 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.653997573824967
avg entropy: 0.6723160106758075
avg cont entropy: 0.6836223666989683
delta all entropy: -0.0042359237936255845 0.0 0.0 -0.00110327425553991 -5.213266027193075e-05 0.00010427075692809318 0.0 0.0011566935815418011 0.0 -0.002033090024169626
delta avg entropy: -0.0006163456395137157
all entropy: 0.6497616500313415 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6930885314445381 0.6930885314445381 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.653997573824967
all entropy: 0.6497616500313415 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6930885314445381 0.6930885314445381 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.653997573824967avg entropy: 0.6723160106758075

avg cont entropy: 0.6836223666989683
avg entropy: 0.6723160106758075delta all entropy: -0.0042359237936255845 0.0 0.0 -0.00110327425553991 -5.213266027193075e-05 0.00010427075692809318 0.0 0.0011566935815418011 0.0 -0.002033090024169626

avg cont entropy: 0.6836223666989683delta avg entropy: -0.0006163456395137157

delta all entropy: -0.0042359237936255845 0.0 0.0 -0.00110327425553991 -5.213266027193075e-05 0.00010427075692809318 0.0 0.0011566935815418011 0.0 -0.002033090024169626
delta avg entropy: -0.0006163456395137157
all entropy: 0.6497616500313415 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.6930885314445381 0.6930885314445381 0.6841994535753173 0.6810494166250649 0.6429795215301972 0.653997573824967
avg entropy: 0.6723160106758075
avg cont entropy: 0.6836223666989683
delta all entropy: -0.0042359237936255845 0.0 0.0 -0.00110327425553991 -5.213266027193075e-05 0.00010427075692809318 0.0 0.0011566935815418011 0.0 -0.002033090024169626
delta avg entropy: -0.0006163456395137157
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 19:16:02,921 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-150
[INFO|configuration_utils.py:425] 2025-06-23 19:16:02,922 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-150/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:16:29,672 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-150/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:16:29,675 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:16:29,676 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-150/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:16:29,710 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-150/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 19:16:29,716] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step150 is begin to save!
[2025-06-23 19:16:40,993] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-150/global_step150/mp_rank_00_model_states.pt
[2025-06-23 19:17:34,825] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step150 is begin to save!
[2025-06-23 19:17:46,081] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-150/global_step150/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:18:47,742 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-50] due to args.save_total_limit
{'loss': 0.0133, 'learning_rate': 1.8673333333333336e-05, 'epoch': 16.0}
[INFO|trainer.py:2655] 2025-06-23 19:19:47,506 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-160
[INFO|configuration_utils.py:425] 2025-06-23 19:19:47,507 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-160/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:20:14,537 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:20:14,540 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:20:14,540 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-160/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:20:14,571 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-160/spiece.model
[2025-06-23 19:20:14,576] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step160 is begin to save!
[2025-06-23 19:20:25,640] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-160/global_step160/mp_rank_00_model_states.pt
[2025-06-23 19:21:18,850] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step160 is begin to save!
[2025-06-23 19:21:40,875] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-160/global_step160/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:22:42,142 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-60] due to args.save_total_limit
{'loss': 0.0138, 'learning_rate': 1.8452222222222224e-05, 'epoch': 17.0}
[INFO|trainer.py:2655] 2025-06-23 19:23:44,463 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-170
[INFO|configuration_utils.py:425] 2025-06-23 19:23:44,464 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-170/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:24:12,168 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-170/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:24:12,171 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-170/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:24:12,172 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-170/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:24:12,200 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-170/spiece.model
[2025-06-23 19:24:12,205] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step170 is begin to save!
[2025-06-23 19:24:23,538] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-170/global_step170/mp_rank_00_model_states.pt
[2025-06-23 19:25:17,522] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step170 is begin to save!
[2025-06-23 19:25:28,850] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-170/global_step170/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:26:28,982 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-70] due to args.save_total_limit
{'loss': 0.0133, 'learning_rate': 1.823111111111111e-05, 'epoch': 18.0}
[INFO|trainer.py:2655] 2025-06-23 19:27:31,186 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-180
[INFO|configuration_utils.py:425] 2025-06-23 19:27:31,188 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-180/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:27:57,489 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-180/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:27:57,492 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-180/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:27:57,492 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-180/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:27:57,521 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-180/spiece.model
[2025-06-23 19:27:57,526] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step180 is begin to save!
[2025-06-23 19:28:08,523] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-180/global_step180/mp_rank_00_model_states.pt
[2025-06-23 19:28:57,330] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step180 is begin to save!
[2025-06-23 19:29:08,313] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-180/global_step180/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:30:06,684 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-80] due to args.save_total_limit
{'loss': 0.0128, 'learning_rate': 1.8010000000000002e-05, 'epoch': 19.0}
[INFO|trainer.py:2655] 2025-06-23 19:31:08,976 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-190
[INFO|configuration_utils.py:425] 2025-06-23 19:31:08,978 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-190/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:31:37,051 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-190/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:31:37,054 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-190/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:31:37,055 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-190/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:31:37,087 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-190/spiece.model
[2025-06-23 19:31:37,092] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step190 is begin to save!
[2025-06-23 19:31:48,254] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-190/global_step190/mp_rank_00_model_states.pt
[2025-06-23 19:32:41,660] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step190 is begin to save!
[2025-06-23 19:32:52,817] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-190/global_step190/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:33:52,576 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-90] due to args.save_total_limit
{'loss': 0.0134, 'learning_rate': 1.778888888888889e-05, 'epoch': 20.0}
[INFO|trainer.py:2907] 2025-06-23 19:34:54,557 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 19:34:54,557 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 19:34:54,557 >>   Batch size = 100
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.49), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.49), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.49), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.49), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.49), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.49), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.49), 'avg_ensemble_accuracy': 85.56, 'vote_ensemble_accuracy': 85.92}
[INFO|trainer.py:2907] 2025-06-23 19:35:22,782 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 19:35:22,782 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 19:35:22,783 >>   Batch size = 100
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.69298426068761 0.6930885314445381 0.6851431626922719 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6730268558935897
avg cont entropy: 0.6836981804798223
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.69298426068761 0.6930885314445381 0.6851431626922719 0.6810494166250649 0.6429795215301972 0.6560306638491367delta all entropy: 0.0 0.0 0.0 -0.00110327425553991 -0.00015640341720002393 0.00010427075692809318 0.0009437091169546274 0.0011566935815418011 0.0 0.0

delta avg entropy: 9.449957826845878e-05
avg entropy: 0.6730268558935897
avg cont entropy: 0.6836981804798223
delta all entropy: 0.0 0.0 0.0 -0.00110327425553991 -0.00015640341720002393 0.00010427075692809318 0.0009437091169546274 0.0011566935815418011 0.0 0.0
delta avg entropy: 9.449957826845878e-05
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.69298426068761 0.6930885314445381 0.6851431626922719 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6730268558935897
avg cont entropy: 0.6836981804798223
delta all entropy: 0.0 0.0 0.0 -0.00110327425553991 -0.00015640341720002393 0.00010427075692809318 0.0009437091169546274 0.0011566935815418011 0.0 0.0
delta avg entropy: 9.449957826845878e-05
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.69298426068761 0.6930885314445381 0.6851431626922719 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6730268558935897
avg cont entropy: 0.6836981804798223
delta all entropy: 0.0 0.0 0.0 -0.00110327425553991 -0.00015640341720002393 0.00010427075692809318 0.0009437091169546274 0.0011566935815418011 0.0 0.0
delta avg entropy: 9.449957826845878e-05
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.69298426068761 0.6930885314445381 0.6851431626922719 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6730268558935897
avg cont entropy: 0.6836981804798223
delta all entropy: 0.0 0.0 0.0 -0.00110327425553991 -0.00015640341720002393 0.00010427075692809318 0.0009437091169546274 0.0011566935815418011 0.0 0.0
delta avg entropy: 9.449957826845878e-05
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.69298426068761 0.6930885314445381 0.6851431626922719 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6730268558935897
avg cont entropy: 0.6836981804798223
delta all entropy: 0.0 0.0 0.0 -0.00110327425553991 -0.00015640341720002393 0.00010427075692809318 0.0009437091169546274 0.0011566935815418011 0.0 0.0
delta avg entropy: 9.449957826845878e-05
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6810494166250649 0.69298426068761 0.6930885314445381 0.6851431626922719 0.6810494166250649 0.6429795215301972 0.6560306638491367
avg entropy: 0.6730268558935897
avg cont entropy: 0.6836981804798223
delta all entropy: 0.0 0.0 0.0 -0.00110327425553991 -0.00015640341720002393 0.00010427075692809318 0.0009437091169546274 0.0011566935815418011 0.0 0.0
delta avg entropy: 9.449957826845878e-05
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 19:35:50,028 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-200
[INFO|configuration_utils.py:425] 2025-06-23 19:35:50,029 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-200/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:36:16,740 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-200/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:36:16,743 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:36:16,744 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-200/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:36:16,773 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-200/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 19:36:16,778] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is begin to save!
[2025-06-23 19:36:28,199] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2025-06-23 19:37:22,726] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is begin to save!
[2025-06-23 19:37:34,132] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:38:34,189 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-100] due to args.save_total_limit
{'loss': 0.0135, 'learning_rate': 1.756777777777778e-05, 'epoch': 21.0}
[INFO|trainer.py:2655] 2025-06-23 19:39:37,454 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-210
[INFO|configuration_utils.py:425] 2025-06-23 19:39:37,455 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-210/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:40:05,100 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-210/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:40:05,103 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-210/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:40:05,104 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-210/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:40:05,137 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-210/spiece.model
[2025-06-23 19:40:05,142] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step210 is begin to save!
[2025-06-23 19:40:17,658] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-210/global_step210/mp_rank_00_model_states.pt
[2025-06-23 19:41:13,128] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step210 is begin to save!
[2025-06-23 19:41:30,349] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-210/global_step210/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:42:33,965 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-110] due to args.save_total_limit
{'loss': 0.0131, 'learning_rate': 1.7346666666666668e-05, 'epoch': 22.0}
[INFO|trainer.py:2655] 2025-06-23 19:43:34,601 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-220
[INFO|configuration_utils.py:425] 2025-06-23 19:43:34,602 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-220/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:44:01,606 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-220/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:44:01,610 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-220/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:44:01,611 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-220/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:44:01,648 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-220/spiece.model
[2025-06-23 19:44:01,653] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step220 is begin to save!
[2025-06-23 19:44:12,555] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-220/global_step220/mp_rank_00_model_states.pt
[2025-06-23 19:45:02,617] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step220 is begin to save!
[2025-06-23 19:45:13,505] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-220/global_step220/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:46:12,020 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-120] due to args.save_total_limit
{'loss': 0.0128, 'learning_rate': 1.712555555555556e-05, 'epoch': 23.0}
[INFO|trainer.py:2655] 2025-06-23 19:47:15,055 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-230
[INFO|configuration_utils.py:425] 2025-06-23 19:47:15,056 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-230/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:47:41,255 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-230/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:47:41,259 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-230/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:47:41,259 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-230/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:47:41,301 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-230/spiece.model
[2025-06-23 19:47:41,307] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step230 is begin to save!
[2025-06-23 19:47:52,023] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-230/global_step230/mp_rank_00_model_states.pt
[2025-06-23 19:48:40,905] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step230 is begin to save!
[2025-06-23 19:48:51,689] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-230/global_step230/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:49:50,945 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-130] due to args.save_total_limit
{'loss': 0.0134, 'learning_rate': 1.6904444444444446e-05, 'epoch': 24.0}
[INFO|trainer.py:2655] 2025-06-23 19:50:54,226 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-240
[INFO|configuration_utils.py:425] 2025-06-23 19:50:54,227 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-240/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:51:20,321 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:51:20,324 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:51:20,325 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-240/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:51:20,357 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-240/spiece.model
[2025-06-23 19:51:20,361] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step240 is begin to save!
[2025-06-23 19:51:31,218] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-240/global_step240/mp_rank_00_model_states.pt
[2025-06-23 19:52:20,242] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step240 is begin to save!
[2025-06-23 19:52:31,195] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-240/global_step240/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:53:41,428 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-140] due to args.save_total_limit
{'loss': 0.0137, 'learning_rate': 1.6683333333333337e-05, 'epoch': 25.0}
[INFO|trainer.py:2907] 2025-06-23 19:54:43,159 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 19:54:43,159 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 19:54:43,159 >>   Batch size = 100
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.13), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.56), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.13), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.56), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.13), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.56), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.13), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.56), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.13), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.56), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.13), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.56), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.61), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(85.13), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.56), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
[INFO|trainer.py:2907] 2025-06-23 19:55:11,241 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 19:55:11,241 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 19:55:11,242 >>   Batch size = 100
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6821526908806048 0.6926192559325051 0.6930885314445381 0.6860338939853872 0.6810494166250649 0.6429795215301972 0.6580074975533652
avg entropy: 0.6732834780041956
avg cont entropy: 0.6840219555362961
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 0.0 -0.0005214081723049491 0.00010427075692809318 0.0018344404100698597 0.0011566935815418011 0.0 0.00197683370422852
delta avg entropy: 0.00035112168887441175
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6821526908806048 0.6926192559325051 0.6930885314445381 0.6860338939853872 0.6810494166250649 0.6429795215301972 0.6580074975533652
avg entropy: 0.6732834780041956
avg cont entropy: 0.6840219555362961
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 0.0 -0.0005214081723049491 0.00010427075692809318 0.0018344404100698597 0.0011566935815418011 0.0 0.00197683370422852
delta avg entropy: 0.00035112168887441175
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6821526908806048 0.6926192559325051 0.6930885314445381 0.6860338939853872 0.6810494166250649 0.6429795215301972 0.6580074975533652
avg entropy: 0.6732834780041956
avg cont entropy: 0.6840219555362961
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 0.0 -0.0005214081723049491 0.00010427075692809318 0.0018344404100698597 0.0011566935815418011 0.0 0.00197683370422852
delta avg entropy: 0.00035112168887441175
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6821526908806048 0.6926192559325051 0.6930885314445381 0.6860338939853872 0.6810494166250649 0.6429795215301972 0.6580074975533652
avg entropy: 0.6732834780041956
avg cont entropy: 0.6840219555362961
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 0.0 -0.0005214081723049491 0.00010427075692809318 0.0018344404100698597 0.0011566935815418011 0.0 0.00197683370422852
delta avg entropy: 0.00035112168887441175
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6821526908806048 0.6926192559325051 0.6930885314445381 0.6860338939853872 0.6810494166250649 0.6429795215301972 0.6580074975533652
avg entropy: 0.6732834780041956
avg cont entropy: 0.6840219555362961
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 0.0 -0.0005214081723049491 0.00010427075692809318 0.0018344404100698597 0.0011566935815418011 0.0 0.00197683370422852
delta avg entropy: 0.00035112168887441175
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6821526908806048 0.6926192559325051 0.6930885314445381 0.6860338939853872 0.6810494166250649 0.6429795215301972 0.6580074975533652
avg entropy: 0.6732834780041956
avg cont entropy: 0.6840219555362961
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 0.0 -0.0005214081723049491 0.00010427075692809318 0.0018344404100698597 0.0011566935815418011 0.0 0.00197683370422852
delta avg entropy: 0.00035112168887441175
all entropy: 0.6519079859154411 0.6617933207764413 0.6832026653984116 0.6821526908806048 0.6926192559325051 0.6930885314445381 0.6860338939853872 0.6810494166250649 0.6429795215301972 0.6580074975533652
avg entropy: 0.6732834780041956
avg cont entropy: 0.6840219555362961
delta all entropy: -0.0020895879095259584 0.0 0.001049974517806751 0.0 -0.0005214081723049491 0.00010427075692809318 0.0018344404100698597 0.0011566935815418011 0.0 0.00197683370422852
delta avg entropy: 0.00035112168887441175
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 19:55:37,980 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-250
[INFO|configuration_utils.py:425] 2025-06-23 19:55:37,981 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-250/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:56:04,534 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:56:04,537 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:56:04,537 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-250/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:56:04,567 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-250/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 19:56:04,572] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step250 is begin to save!
[2025-06-23 19:56:14,999] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-250/global_step250/mp_rank_00_model_states.pt
[2025-06-23 19:57:04,657] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step250 is begin to save!
[2025-06-23 19:57:15,079] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-250/global_step250/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 19:58:14,024 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-150] due to args.save_total_limit
{'loss': 0.0134, 'learning_rate': 1.6462222222222224e-05, 'epoch': 26.0}
[INFO|trainer.py:2655] 2025-06-23 19:59:15,843 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-260
[INFO|configuration_utils.py:425] 2025-06-23 19:59:15,845 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-260/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 19:59:43,568 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-260/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 19:59:43,571 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-260/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 19:59:43,571 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-260/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 19:59:43,601 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-260/spiece.model
[2025-06-23 19:59:43,606] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step260 is begin to save!
[2025-06-23 19:59:54,230] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-260/global_step260/mp_rank_00_model_states.pt
[2025-06-23 20:00:45,317] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step260 is begin to save!
[2025-06-23 20:00:55,964] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-260/global_step260/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:01:55,075 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-160] due to args.save_total_limit
{'loss': 0.0141, 'learning_rate': 1.624111111111111e-05, 'epoch': 27.0}
[INFO|trainer.py:2655] 2025-06-23 20:02:56,821 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-270
[INFO|configuration_utils.py:425] 2025-06-23 20:02:56,822 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-270/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:03:22,224 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-270/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:03:22,227 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-270/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:03:22,227 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-270/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:03:22,258 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-270/spiece.model
[2025-06-23 20:03:22,262] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step270 is begin to save!
[2025-06-23 20:03:32,812] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-270/global_step270/mp_rank_00_model_states.pt
[2025-06-23 20:04:22,308] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step270 is begin to save!
[2025-06-23 20:04:32,906] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-270/global_step270/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:05:31,451 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-170] due to args.save_total_limit
{'loss': 0.013, 'learning_rate': 1.6020000000000002e-05, 'epoch': 28.0}
[INFO|trainer.py:2655] 2025-06-23 20:06:33,415 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-280
[INFO|configuration_utils.py:425] 2025-06-23 20:06:33,417 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-280/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:07:00,452 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-280/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:07:00,455 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:07:00,456 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-280/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:07:00,486 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-280/spiece.model
[2025-06-23 20:07:00,491] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step280 is begin to save!
[2025-06-23 20:07:11,030] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-280/global_step280/mp_rank_00_model_states.pt
[2025-06-23 20:08:01,197] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step280 is begin to save!
[2025-06-23 20:08:11,741] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-280/global_step280/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:09:09,884 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-180] due to args.save_total_limit
{'loss': 0.0134, 'learning_rate': 1.579888888888889e-05, 'epoch': 29.0}
[INFO|trainer.py:2655] 2025-06-23 20:10:14,181 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-290
[INFO|configuration_utils.py:425] 2025-06-23 20:10:14,183 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-290/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:10:40,995 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-290/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:10:40,998 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-290/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:10:40,998 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-290/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:10:41,029 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-290/spiece.model
[2025-06-23 20:10:41,034] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step290 is begin to save!
[2025-06-23 20:10:51,554] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-290/global_step290/mp_rank_00_model_states.pt
[2025-06-23 20:11:40,987] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step290 is begin to save!
[2025-06-23 20:11:51,477] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-290/global_step290/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:12:51,843 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-190] due to args.save_total_limit
{'loss': 0.0127, 'learning_rate': 1.557777777777778e-05, 'epoch': 30.0}
[INFO|trainer.py:2907] 2025-06-23 20:13:55,338 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 20:13:55,338 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 20:13:55,339 >>   Batch size = 100
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.44), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.44), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.44), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.44), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.44), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.44), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.44), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.92}
[INFO|trainer.py:2907] 2025-06-23 20:14:23,484 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 20:14:23,485 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 20:14:23,485 >>   Batch size = 100
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.692358483831969 0.69314066410481 0.6868717427125868 0.6821526908806048 0.6429795215301972 0.6580074975533652
avg entropy: 0.6735606876976151
avg cont entropy: 0.6846945601792419
delta all entropy: 0.0 0.0 0.0 0.0 -0.0007821802728410487 0.00015640341720002393 0.002672289137269468 0.002259967837081711 0.0 0.00197683370422852
delta avg entropy: 0.0006283313822938674
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.692358483831969 0.69314066410481 0.6868717427125868 0.6821526908806048 0.6429795215301972 0.6580074975533652
avg entropy: 0.6735606876976151
avg cont entropy: 0.6846945601792419
delta all entropy: 0.0 0.0 0.0 0.0 -0.0007821802728410487 0.00015640341720002393 0.002672289137269468 0.002259967837081711 0.0 0.00197683370422852
delta avg entropy: 0.0006283313822938674
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.692358483831969 0.69314066410481 0.6868717427125868 0.6821526908806048 0.6429795215301972 0.6580074975533652
avg entropy: 0.6735606876976151
avg cont entropy: 0.6846945601792419
delta all entropy: 0.0 0.0 0.0 0.0 -0.0007821802728410487 0.00015640341720002393 0.002672289137269468 0.002259967837081711 0.0 0.00197683370422852
delta avg entropy: 0.0006283313822938674
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.692358483831969 0.69314066410481 0.6868717427125868 0.6821526908806048 0.6429795215301972 0.6580074975533652
avg entropy: 0.6735606876976151
avg cont entropy: 0.6846945601792419
delta all entropy: 0.0 0.0 0.0 0.0 -0.0007821802728410487 0.00015640341720002393 0.002672289137269468 0.002259967837081711 0.0 0.00197683370422852
delta avg entropy: 0.0006283313822938674
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.692358483831969 0.69314066410481 0.6868717427125868 0.6821526908806048 0.6429795215301972 0.6580074975533652
avg entropy: 0.6735606876976151
avg cont entropy: 0.6846945601792419
delta all entropy: 0.0 0.0 0.0 0.0 -0.0007821802728410487 0.00015640341720002393 0.002672289137269468 0.002259967837081711 0.0 0.00197683370422852
delta avg entropy: 0.0006283313822938674
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.692358483831969 0.69314066410481 0.6868717427125868 0.6821526908806048 0.6429795215301972 0.6580074975533652
avg entropy: 0.6735606876976151
avg cont entropy: 0.6846945601792419
delta all entropy: 0.0 0.0 0.0 0.0 -0.0007821802728410487 0.00015640341720002393 0.002672289137269468 0.002259967837081711 0.0 0.00197683370422852
delta avg entropy: 0.0006283313822938674
all entropy: 0.653997573824967 0.6617933207764413 0.6821526908806048 0.6821526908806048 0.692358483831969 0.69314066410481 0.6868717427125868 0.6821526908806048 0.6429795215301972 0.6580074975533652
avg entropy: 0.6735606876976151
avg cont entropy: 0.6846945601792419
delta all entropy: 0.0 0.0 0.0 0.0 -0.0007821802728410487 0.00015640341720002393 0.002672289137269468 0.002259967837081711 0.0 0.00197683370422852
delta avg entropy: 0.0006283313822938674
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 20:14:50,640 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-300
[INFO|configuration_utils.py:425] 2025-06-23 20:14:50,641 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-300/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:15:17,590 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-300/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:15:17,594 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:15:17,594 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-300/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:15:17,624 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-300/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 20:15:17,630] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is begin to save!
[2025-06-23 20:15:31,361] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-300/global_step300/mp_rank_00_model_states.pt
[2025-06-23 20:16:24,742] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is begin to save!
[2025-06-23 20:16:42,058] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-300/global_step300/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:17:42,854 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-200] due to args.save_total_limit
{'loss': 0.013, 'learning_rate': 1.5356666666666668e-05, 'epoch': 31.0}
[INFO|trainer.py:2655] 2025-06-23 20:18:45,777 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-310
[INFO|configuration_utils.py:425] 2025-06-23 20:18:45,779 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-310/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:19:13,297 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-310/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:19:13,300 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-310/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:19:13,300 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-310/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:19:13,336 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-310/spiece.model
[2025-06-23 20:19:13,341] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step310 is begin to save!
[2025-06-23 20:19:24,579] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-310/global_step310/mp_rank_00_model_states.pt
[2025-06-23 20:20:18,538] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step310 is begin to save!
[2025-06-23 20:20:29,784] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-310/global_step310/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:21:30,047 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-210] due to args.save_total_limit
{'loss': 0.0128, 'learning_rate': 1.5135555555555557e-05, 'epoch': 32.0}
[INFO|trainer.py:2655] 2025-06-23 20:22:32,751 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-320
[INFO|configuration_utils.py:425] 2025-06-23 20:22:32,752 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-320/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:23:00,568 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:23:00,572 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:23:00,572 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-320/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:23:00,616 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-320/spiece.model
[2025-06-23 20:23:00,622] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step320 is begin to save!
[2025-06-23 20:23:12,089] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-320/global_step320/mp_rank_00_model_states.pt
[2025-06-23 20:24:05,132] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step320 is begin to save!
[2025-06-23 20:24:16,585] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-320/global_step320/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:25:15,544 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-220] due to args.save_total_limit
{'loss': 0.0135, 'learning_rate': 1.491444444444445e-05, 'epoch': 33.0}
[INFO|trainer.py:2655] 2025-06-23 20:26:28,956 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-330
[INFO|configuration_utils.py:425] 2025-06-23 20:26:28,957 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-330/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:26:56,560 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-330/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:26:56,563 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-330/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:26:56,563 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-330/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:26:56,593 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-330/spiece.model
[2025-06-23 20:26:56,599] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step330 is begin to save!
[2025-06-23 20:27:07,918] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-330/global_step330/mp_rank_00_model_states.pt
[2025-06-23 20:27:57,041] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step330 is begin to save!
[2025-06-23 20:28:08,353] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-330/global_step330/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:29:02,505 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-230] due to args.save_total_limit
{'loss': 0.0131, 'learning_rate': 1.4693333333333337e-05, 'epoch': 34.0}
[INFO|trainer.py:2655] 2025-06-23 20:30:04,294 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-340
[INFO|configuration_utils.py:425] 2025-06-23 20:30:04,295 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-340/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:30:31,779 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-340/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:30:31,782 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:30:31,782 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-340/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:30:31,812 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-340/spiece.model
[2025-06-23 20:30:31,817] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step340 is begin to save!
[2025-06-23 20:30:43,098] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-340/global_step340/mp_rank_00_model_states.pt
[2025-06-23 20:31:33,379] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step340 is begin to save!
[2025-06-23 20:31:44,641] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-340/global_step340/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:32:43,412 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-240] due to args.save_total_limit
{'loss': 0.0123, 'learning_rate': 1.4472222222222225e-05, 'epoch': 35.0}
[INFO|trainer.py:2907] 2025-06-23 20:33:46,316 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 20:33:46,317 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 20:33:46,317 >>   Batch size = 100
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.43), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.56}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.43), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.56}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.43), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.56}
[INFO|trainer.py:2907] 2025-06-23 20:34:13,776 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 20:34:13,776 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 20:34:13,776 >>   Batch size = 100
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.43), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.56}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.43), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.56}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.43), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.56}
{'max_accuracy': np.float64(90.25), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.23), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.43), 'avg_ensemble_accuracy': 85.92, 'vote_ensemble_accuracy': 85.56}
all entropy: 0.653997573824967 0.6617933207764413 0.6832026653984116 0.6832026653984116 0.6920454974061679 0.6930885314445381 0.6876567982159768 0.6821526908806048 0.6452976901583611 0.6653568125489489
avg entropy: 0.6747794246052828
avg cont entropy: 0.6855715122840094
all entropy: 0.653997573824967 0.6617933207764413 0.6832026653984116 0.6832026653984116 0.6920454974061679 0.6930885314445381 0.6876567982159768 0.6821526908806048 0.6452976901583611 0.6653568125489489delta all entropy: 0.0 0.0 0.001049974517806751 0.001049974517806751 -0.0010951666986420694 0.00010427075692809318 0.0034573446406594988 0.002259967837081711 0.002318168628163919 0.009326148699812187

delta avg entropy: 0.001847068289961684
avg entropy: 0.6747794246052828
avg cont entropy: 0.6855715122840094
all entropy: 0.653997573824967 0.6617933207764413 0.6832026653984116 0.6832026653984116 0.6920454974061679 0.6930885314445381 0.6876567982159768 0.6821526908806048 0.6452976901583611 0.6653568125489489
delta all entropy: 0.0 0.0 0.001049974517806751 0.001049974517806751 -0.0010951666986420694 0.00010427075692809318 0.0034573446406594988 0.002259967837081711 0.002318168628163919 0.009326148699812187
delta avg entropy: 0.001847068289961684
avg entropy: 0.6747794246052828
avg cont entropy: 0.6855715122840094
delta all entropy: 0.0 0.0 0.001049974517806751 0.001049974517806751 -0.0010951666986420694 0.00010427075692809318 0.0034573446406594988 0.002259967837081711 0.002318168628163919 0.009326148699812187
delta avg entropy: 0.001847068289961684
all entropy: 0.653997573824967 0.6617933207764413 0.6832026653984116 0.6832026653984116 0.6920454974061679 0.6930885314445381 0.6876567982159768 0.6821526908806048 0.6452976901583611 0.6653568125489489
avg entropy: 0.6747794246052828
avg cont entropy: 0.6855715122840094
delta all entropy: 0.0 0.0 0.001049974517806751 0.001049974517806751 -0.0010951666986420694 0.00010427075692809318 0.0034573446406594988 0.002259967837081711 0.002318168628163919 0.009326148699812187
delta avg entropy: 0.001847068289961684
all entropy: 0.653997573824967 0.6617933207764413 0.6832026653984116 0.6832026653984116 0.6920454974061679 0.6930885314445381 0.6876567982159768 0.6821526908806048 0.6452976901583611 0.6653568125489489
avg entropy: 0.6747794246052828
avg cont entropy: 0.6855715122840094
delta all entropy: 0.0 0.0 0.001049974517806751 0.001049974517806751 -0.0010951666986420694 0.00010427075692809318 0.0034573446406594988 0.002259967837081711 0.002318168628163919 0.009326148699812187
delta avg entropy: 0.001847068289961684
all entropy: 0.653997573824967 0.6617933207764413 0.6832026653984116 0.6832026653984116 0.6920454974061679 0.6930885314445381 0.6876567982159768 0.6821526908806048 0.6452976901583611 0.6653568125489489
avg entropy: 0.6747794246052828
avg cont entropy: 0.6855715122840094
delta all entropy: 0.0 0.0 0.001049974517806751 0.001049974517806751 -0.0010951666986420694 0.00010427075692809318 0.0034573446406594988 0.002259967837081711 0.002318168628163919 0.009326148699812187
delta avg entropy: 0.001847068289961684
all entropy: 0.653997573824967 0.6617933207764413 0.6832026653984116 0.6832026653984116 0.6920454974061679 0.6930885314445381 0.6876567982159768 0.6821526908806048 0.6452976901583611 0.6653568125489489
avg entropy: 0.6747794246052828
avg cont entropy: 0.6855715122840094
delta all entropy: 0.0 0.0 0.001049974517806751 0.001049974517806751 -0.0010951666986420694 0.00010427075692809318 0.0034573446406594988 0.002259967837081711 0.002318168628163919 0.009326148699812187
delta avg entropy: 0.001847068289961684
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 20:34:41,101 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-350
[INFO|configuration_utils.py:425] 2025-06-23 20:34:41,102 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-350/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:35:07,893 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:35:07,896 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:35:07,896 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-350/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:35:07,925 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-350/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 20:35:07,930] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step350 is begin to save!
[2025-06-23 20:35:18,953] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-350/global_step350/mp_rank_00_model_states.pt
[2025-06-23 20:36:06,889] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step350 is begin to save!
[2025-06-23 20:36:17,895] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-350/global_step350/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:37:16,154 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-250] due to args.save_total_limit
{'loss': 0.0129, 'learning_rate': 1.4251111111111114e-05, 'epoch': 36.0}
[INFO|trainer.py:2655] 2025-06-23 20:38:18,787 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-360
[INFO|configuration_utils.py:425] 2025-06-23 20:38:18,789 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-360/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:38:44,865 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-360/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:38:44,868 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:38:44,869 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-360/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:38:44,898 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-360/spiece.model
[2025-06-23 20:38:44,903] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step360 is begin to save!
[2025-06-23 20:38:55,975] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-360/global_step360/mp_rank_00_model_states.pt
[2025-06-23 20:39:44,720] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step360 is begin to save!
[2025-06-23 20:39:55,767] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-360/global_step360/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:40:52,326 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-260] due to args.save_total_limit
{'loss': 0.0122, 'learning_rate': 1.4030000000000001e-05, 'epoch': 37.0}
[INFO|trainer.py:2655] 2025-06-23 20:41:55,303 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-370
[INFO|configuration_utils.py:425] 2025-06-23 20:41:55,304 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-370/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:42:22,674 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-370/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:42:22,677 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-370/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:42:22,677 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-370/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:42:22,707 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-370/spiece.model
[2025-06-23 20:42:22,712] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step370 is begin to save!
[2025-06-23 20:42:33,831] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-370/global_step370/mp_rank_00_model_states.pt
[2025-06-23 20:43:22,251] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step370 is begin to save!
[2025-06-23 20:43:33,303] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-370/global_step370/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:44:27,704 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-270] due to args.save_total_limit
{'loss': 0.0119, 'learning_rate': 1.3808888888888892e-05, 'epoch': 38.0}
[INFO|trainer.py:2655] 2025-06-23 20:45:31,333 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-380
[INFO|configuration_utils.py:425] 2025-06-23 20:45:31,335 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-380/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:45:57,667 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-380/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:45:57,670 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-380/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:45:57,670 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-380/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:45:57,703 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-380/spiece.model
[2025-06-23 20:45:57,708] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step380 is begin to save!
[2025-06-23 20:46:08,741] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-380/global_step380/mp_rank_00_model_states.pt
[2025-06-23 20:47:02,690] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step380 is begin to save!
[2025-06-23 20:47:13,736] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-380/global_step380/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:48:13,815 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-280] due to args.save_total_limit
{'loss': 0.0117, 'learning_rate': 1.3587777777777781e-05, 'epoch': 39.0}
[INFO|trainer.py:2655] 2025-06-23 20:49:17,219 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-390
[INFO|configuration_utils.py:425] 2025-06-23 20:49:17,221 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-390/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:49:45,097 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:49:45,100 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:49:45,101 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-390/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:49:45,131 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-390/spiece.model
[2025-06-23 20:49:45,136] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step390 is begin to save!
[2025-06-23 20:49:57,691] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-390/global_step390/mp_rank_00_model_states.pt
[2025-06-23 20:50:52,844] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step390 is begin to save!
[2025-06-23 20:51:10,080] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-390/global_step390/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:52:15,117 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-290] due to args.save_total_limit
{'loss': 0.0117, 'learning_rate': 1.336666666666667e-05, 'epoch': 40.0}
[INFO|trainer.py:2907] 2025-06-23 20:53:17,214 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 20:53:17,215 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 20:53:17,215 >>   Batch size = 100
{'max_accuracy': np.float64(89.53), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.32), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.53), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.32), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.53), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.32), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.53), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.32), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.53), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.32), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
[INFO|trainer.py:2907] 2025-06-23 20:53:44,727 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 20:53:44,728 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 20:53:44,728 >>   Batch size = 100
{'max_accuracy': np.float64(89.53), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.32), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.53), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(85.27), 'min_accuracy': np.float64(81.95), 'std_accuracy': np.float64(2.32), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
all entropy: 0.653997573824967 0.6686996168320491 0.6851431626922719 0.6832026653984116 0.6920454974061679 0.6928278409578548 0.6890688576857509 0.6841994535753173 0.6519079859154411 0.6686996168320491
avg entropy: 0.6769792271120281
avg cont entropy: 0.6865720681601822
delta all entropy: 0.0 0.006906296055607886 0.002990471811667117 0.001049974517806751 -0.0010951666986420694 -0.00015641972975521945 0.004869404110433573 0.004306730531794201 0.008928464385243862 0.012668952982912485
delta avg entropy: 0.004046870796706859
all entropy: 0.653997573824967 0.6686996168320491 0.6851431626922719 0.6832026653984116 0.6920454974061679 0.6928278409578548 0.6890688576857509 0.6841994535753173 0.6519079859154411 0.6686996168320491
avg entropy: 0.6769792271120281
avg cont entropy: 0.6865720681601822
delta all entropy: 0.0 0.006906296055607886 0.002990471811667117 0.001049974517806751 -0.0010951666986420694 -0.00015641972975521945 0.004869404110433573 0.004306730531794201 0.008928464385243862 0.012668952982912485
delta avg entropy: 0.004046870796706859
all entropy: 0.653997573824967 0.6686996168320491 0.6851431626922719 0.6832026653984116 0.6920454974061679 0.6928278409578548 0.6890688576857509 0.6841994535753173 0.6519079859154411 0.6686996168320491all entropy: 0.653997573824967 0.6686996168320491 0.6851431626922719 0.6832026653984116 0.6920454974061679 0.6928278409578548 0.6890688576857509 0.6841994535753173 0.6519079859154411 0.6686996168320491

avg entropy: 0.6769792271120281
avg cont entropy: 0.6865720681601822
avg entropy: 0.6769792271120281
avg cont entropy: 0.6865720681601822
delta all entropy: 0.0 0.006906296055607886 0.002990471811667117 0.001049974517806751 -0.0010951666986420694 -0.00015641972975521945 0.004869404110433573 0.004306730531794201 0.008928464385243862 0.012668952982912485
delta avg entropy: 0.004046870796706859
delta all entropy: 0.0 0.006906296055607886 0.002990471811667117 0.001049974517806751 -0.0010951666986420694 -0.00015641972975521945 0.004869404110433573 0.004306730531794201 0.008928464385243862 0.012668952982912485
delta avg entropy: 0.004046870796706859
all entropy: 0.653997573824967 0.6686996168320491 0.6851431626922719 0.6832026653984116 0.6920454974061679 0.6928278409578548 0.6890688576857509 0.6841994535753173 0.6519079859154411 0.6686996168320491
avg entropy: 0.6769792271120281
avg cont entropy: 0.6865720681601822
delta all entropy: 0.0 0.006906296055607886 0.002990471811667117 0.001049974517806751 -0.0010951666986420694 -0.00015641972975521945 0.004869404110433573 0.004306730531794201 0.008928464385243862 0.012668952982912485
delta avg entropy: 0.004046870796706859
all entropy: 0.653997573824967 0.6686996168320491 0.6851431626922719 0.6832026653984116 0.6920454974061679 0.6928278409578548 0.6890688576857509 0.6841994535753173 0.6519079859154411 0.6686996168320491
avg entropy: 0.6769792271120281
avg cont entropy: 0.6865720681601822
delta all entropy: 0.0 0.006906296055607886 0.002990471811667117 0.001049974517806751 -0.0010951666986420694 -0.00015641972975521945 0.004869404110433573 0.004306730531794201 0.008928464385243862 0.012668952982912485
delta avg entropy: 0.004046870796706859
all entropy: 0.653997573824967 0.6686996168320491 0.6851431626922719 0.6832026653984116 0.6920454974061679 0.6928278409578548 0.6890688576857509 0.6841994535753173 0.6519079859154411 0.6686996168320491
avg entropy: 0.6769792271120281
avg cont entropy: 0.6865720681601822
delta all entropy: 0.0 0.006906296055607886 0.002990471811667117 0.001049974517806751 -0.0010951666986420694 -0.00015641972975521945 0.004869404110433573 0.004306730531794201 0.008928464385243862 0.012668952982912485
delta avg entropy: 0.004046870796706859
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 20:54:11,985 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-400
[INFO|configuration_utils.py:425] 2025-06-23 20:54:11,986 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-400/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:54:44,007 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:54:44,010 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:54:44,011 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-400/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:54:44,042 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-400/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 20:54:44,047] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is begin to save!
[2025-06-23 20:55:01,801] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2025-06-23 20:56:16,888] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is begin to save!
[2025-06-23 20:56:28,142] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 20:57:24,078 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-300] due to args.save_total_limit
{'loss': 0.0121, 'learning_rate': 1.3145555555555558e-05, 'epoch': 41.0}
[INFO|trainer.py:2655] 2025-06-23 20:58:25,619 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-410
[INFO|configuration_utils.py:425] 2025-06-23 20:58:25,621 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-410/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 20:58:53,666 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 20:58:53,669 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 20:58:53,669 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-410/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 20:58:53,699 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-410/spiece.model
[2025-06-23 20:58:53,704] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step410 is begin to save!
[2025-06-23 20:59:05,047] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-410/global_step410/mp_rank_00_model_states.pt
[2025-06-23 20:59:58,713] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step410 is begin to save!
[2025-06-23 21:00:10,030] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-410/global_step410/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:01:11,346 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-310] due to args.save_total_limit
{'loss': 0.012, 'learning_rate': 1.2924444444444447e-05, 'epoch': 42.0}
[INFO|trainer.py:2655] 2025-06-23 21:02:12,853 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-420
[INFO|configuration_utils.py:425] 2025-06-23 21:02:12,854 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-420/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:02:41,118 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-420/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:02:41,121 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:02:41,122 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-420/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:02:41,152 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-420/spiece.model
[2025-06-23 21:02:41,157] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step420 is begin to save!
[2025-06-23 21:02:52,576] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-420/global_step420/mp_rank_00_model_states.pt
[2025-06-23 21:03:46,297] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step420 is begin to save!
[2025-06-23 21:03:57,613] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-420/global_step420/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:04:55,983 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-320] due to args.save_total_limit
{'loss': 0.0123, 'learning_rate': 1.2703333333333336e-05, 'epoch': 43.0}
[INFO|trainer.py:2655] 2025-06-23 21:05:57,520 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-430
[INFO|configuration_utils.py:425] 2025-06-23 21:05:57,521 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-430/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:06:25,599 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-430/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:06:25,602 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-430/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:06:25,602 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-430/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:06:25,633 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-430/spiece.model
[2025-06-23 21:06:25,638] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step430 is begin to save!
[2025-06-23 21:06:36,785] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-430/global_step430/mp_rank_00_model_states.pt
[2025-06-23 21:07:30,463] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step430 is begin to save!
[2025-06-23 21:07:42,377] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-430/global_step430/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:08:43,931 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-330] due to args.save_total_limit
{'loss': 0.0119, 'learning_rate': 1.2482222222222225e-05, 'epoch': 44.0}
[INFO|trainer.py:2655] 2025-06-23 21:09:43,930 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-440
[INFO|configuration_utils.py:425] 2025-06-23 21:09:43,931 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-440/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:10:12,926 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-440/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:10:12,930 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-440/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:10:12,930 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-440/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:10:12,965 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-440/spiece.model
[2025-06-23 21:10:12,970] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step440 is begin to save!
[2025-06-23 21:10:24,517] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-440/global_step440/mp_rank_00_model_states.pt
[2025-06-23 21:11:19,018] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step440 is begin to save!
[2025-06-23 21:11:34,286] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-440/global_step440/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:12:38,290 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-340] due to args.save_total_limit
{'loss': 0.0121, 'learning_rate': 1.2261111111111114e-05, 'epoch': 45.0}
[INFO|trainer.py:2907] 2025-06-23 21:13:40,152 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 21:13:40,152 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 21:13:40,152 >>   Batch size = 100
{'max_accuracy': np.float64(89.17), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.17), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.17), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.17), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.17), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.17), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
{'max_accuracy': np.float64(89.17), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 86.28, 'vote_ensemble_accuracy': 85.2}
[INFO|trainer.py:2907] 2025-06-23 21:14:08,376 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 21:14:08,377 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 21:14:08,377 >>   Batch size = 100
all entropy: 0.653997573824967 0.6718232574120175 0.6876567982159768 0.6860338939853872 0.691680263917418 0.6926192559325051 0.6890688576857509 0.6851431626922719 0.653997573824967 0.6686996168320491
avg entropy: 0.6780720254323309
avg cont entropy: 0.6874611603387016
delta all entropy: 0.0 0.010029936635576275 0.005504107335371988 0.003881203104782349 -0.0014604001873920192 -0.00036500475510492514 0.004869404110433573 0.005250439648748828 0.01101805229476982 0.012668952982912485
delta avg entropy: 0.005139669117009837
all entropy: 0.653997573824967 0.6718232574120175 0.6876567982159768 0.6860338939853872 0.691680263917418 0.6926192559325051 0.6890688576857509 0.6851431626922719 0.653997573824967 0.6686996168320491
avg entropy: 0.6780720254323309
avg cont entropy: 0.6874611603387016
delta all entropy: 0.0 0.010029936635576275 0.005504107335371988 0.003881203104782349 -0.0014604001873920192 -0.00036500475510492514 0.004869404110433573 0.005250439648748828 0.01101805229476982 0.012668952982912485
delta avg entropy: 0.005139669117009837
all entropy: 0.653997573824967 0.6718232574120175 0.6876567982159768 0.6860338939853872 0.691680263917418 0.6926192559325051 0.6890688576857509 0.6851431626922719 0.653997573824967 0.6686996168320491
avg entropy: 0.6780720254323309
avg cont entropy: 0.6874611603387016
delta all entropy: 0.0 0.010029936635576275 0.005504107335371988 0.003881203104782349 -0.0014604001873920192 -0.00036500475510492514 0.004869404110433573 0.005250439648748828 0.01101805229476982 0.012668952982912485
delta avg entropy: 0.005139669117009837
all entropy: 0.653997573824967 0.6718232574120175 0.6876567982159768 0.6860338939853872 0.691680263917418 0.6926192559325051 0.6890688576857509 0.6851431626922719 0.653997573824967 0.6686996168320491
avg entropy: 0.6780720254323309
avg cont entropy: 0.6874611603387016
delta all entropy: 0.0 0.010029936635576275 0.005504107335371988 0.003881203104782349 -0.0014604001873920192 -0.00036500475510492514 0.004869404110433573 0.005250439648748828 0.01101805229476982 0.012668952982912485
delta avg entropy: 0.005139669117009837
all entropy: 0.653997573824967 0.6718232574120175 0.6876567982159768 0.6860338939853872 0.691680263917418 0.6926192559325051 0.6890688576857509 0.6851431626922719 0.653997573824967 0.6686996168320491
avg entropy: 0.6780720254323309
avg cont entropy: 0.6874611603387016
delta all entropy: 0.0 0.010029936635576275 0.005504107335371988 0.003881203104782349 -0.0014604001873920192 -0.00036500475510492514 0.004869404110433573 0.005250439648748828 0.01101805229476982 0.012668952982912485
delta avg entropy: 0.005139669117009837
all entropy: 0.653997573824967 0.6718232574120175 0.6876567982159768 0.6860338939853872 0.691680263917418 0.6926192559325051 0.6890688576857509 0.6851431626922719 0.653997573824967 0.6686996168320491
avg entropy: 0.6780720254323309
avg cont entropy: 0.6874611603387016
delta all entropy: 0.0 0.010029936635576275 0.005504107335371988 0.003881203104782349 -0.0014604001873920192 -0.00036500475510492514 0.004869404110433573 0.005250439648748828 0.01101805229476982 0.012668952982912485
delta avg entropy: 0.005139669117009837
all entropy: 0.653997573824967 0.6718232574120175 0.6876567982159768 0.6860338939853872 0.691680263917418 0.6926192559325051 0.6890688576857509 0.6851431626922719 0.653997573824967 0.6686996168320491
avg entropy: 0.6780720254323309
avg cont entropy: 0.6874611603387016
delta all entropy: 0.0 0.010029936635576275 0.005504107335371988 0.003881203104782349 -0.0014604001873920192 -0.00036500475510492514 0.004869404110433573 0.005250439648748828 0.01101805229476982 0.012668952982912485
delta avg entropy: 0.005139669117009837
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 21:14:35,178 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-450
[INFO|configuration_utils.py:425] 2025-06-23 21:14:35,179 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-450/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:15:02,822 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-450/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:15:02,826 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-450/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:15:02,826 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-450/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:15:02,861 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-450/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 21:15:02,867] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step450 is begin to save!
[2025-06-23 21:15:16,258] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-450/global_step450/mp_rank_00_model_states.pt
[2025-06-23 21:16:10,150] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step450 is begin to save!
[2025-06-23 21:16:27,763] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-450/global_step450/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:17:32,666 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-350] due to args.save_total_limit
{'loss': 0.0116, 'learning_rate': 1.204e-05, 'epoch': 46.0}
[INFO|trainer.py:2655] 2025-06-23 21:18:35,852 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-460
[INFO|configuration_utils.py:425] 2025-06-23 21:18:35,853 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-460/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:19:02,156 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-460/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:19:02,160 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-460/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:19:02,161 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-460/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:19:02,197 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-460/spiece.model
[2025-06-23 21:19:02,202] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step460 is begin to save!
[2025-06-23 21:19:12,771] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-460/global_step460/mp_rank_00_model_states.pt
[2025-06-23 21:20:03,787] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step460 is begin to save!
[2025-06-23 21:20:14,549] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-460/global_step460/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:21:12,509 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-360] due to args.save_total_limit
{'loss': 0.0113, 'learning_rate': 1.181888888888889e-05, 'epoch': 47.0}
[INFO|trainer.py:2655] 2025-06-23 21:22:13,731 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-470
[INFO|configuration_utils.py:425] 2025-06-23 21:22:13,733 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-470/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:22:39,654 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:22:39,658 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:22:39,658 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-470/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:22:39,701 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-470/spiece.model
[2025-06-23 21:22:39,708] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step470 is begin to save!
[2025-06-23 21:22:50,350] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-470/global_step470/mp_rank_00_model_states.pt
[2025-06-23 21:23:44,405] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step470 is begin to save!
[2025-06-23 21:23:55,061] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-470/global_step470/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:24:56,988 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-370] due to args.save_total_limit
{'loss': 0.0109, 'learning_rate': 1.1597777777777778e-05, 'epoch': 48.0}
[INFO|trainer.py:2655] 2025-06-23 21:25:59,427 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-480
[INFO|configuration_utils.py:425] 2025-06-23 21:25:59,428 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-480/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:26:26,439 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-480/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:26:26,442 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:26:26,443 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-480/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:26:26,475 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-480/spiece.model
[2025-06-23 21:26:26,479] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step480 is begin to save!
[2025-06-23 21:26:37,221] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-480/global_step480/mp_rank_00_model_states.pt
[2025-06-23 21:27:31,333] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step480 is begin to save!
[2025-06-23 21:27:42,068] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-480/global_step480/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:28:41,850 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-380] due to args.save_total_limit
{'loss': 0.011, 'learning_rate': 1.1376666666666669e-05, 'epoch': 49.0}
[INFO|trainer.py:2655] 2025-06-23 21:29:55,568 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-490
[INFO|configuration_utils.py:425] 2025-06-23 21:29:55,569 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-490/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:30:21,430 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-490/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:30:21,433 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-490/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:30:21,433 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-490/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:30:21,462 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-490/spiece.model
[2025-06-23 21:30:21,467] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step490 is begin to save!
[2025-06-23 21:30:31,906] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-490/global_step490/mp_rank_00_model_states.pt
[2025-06-23 21:31:20,965] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step490 is begin to save!
[2025-06-23 21:31:31,385] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-490/global_step490/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:32:27,937 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-390] due to args.save_total_limit
{'loss': 0.0107, 'learning_rate': 1.1155555555555556e-05, 'epoch': 50.0}
[INFO|trainer.py:2907] 2025-06-23 21:33:27,741 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 21:33:27,741 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 21:33:27,741 >>   Batch size = 100
{'max_accuracy': np.float64(88.45), 'median_accuracy': np.float64(84.12), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.0, 'vote_ensemble_accuracy': 84.84}
{'max_accuracy': np.float64(88.45), 'median_accuracy': np.float64(84.12), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.0, 'vote_ensemble_accuracy': 84.84}
{'max_accuracy': np.float64(88.45), 'median_accuracy': np.float64(84.12), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.0, 'vote_ensemble_accuracy': 84.84}
{'max_accuracy': np.float64(88.45), 'median_accuracy': np.float64(84.12), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.0, 'vote_ensemble_accuracy': 84.84}
[INFO|trainer.py:2907] 2025-06-23 21:33:55,247 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 21:33:55,248 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 21:33:55,248 >>   Batch size = 100
{'max_accuracy': np.float64(88.45), 'median_accuracy': np.float64(84.12), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.0, 'vote_ensemble_accuracy': 84.84}
{'max_accuracy': np.float64(88.45), 'median_accuracy': np.float64(84.12), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.0, 'vote_ensemble_accuracy': 84.84}
{'max_accuracy': np.float64(88.45), 'median_accuracy': np.float64(84.12), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.0, 'vote_ensemble_accuracy': 84.84}
all entropy: 0.653997573824967 0.6733033357438176 0.6896960112606978 0.6890688576857509 0.691680263917418 0.6920454974061679 0.6907928972351914 0.6868717427125868 0.6560306638491367 0.6702887419175763
avg entropy: 0.6793775585553311
avg cont entropy: 0.6882842276275893
delta all entropy: 0.0 0.011510014967376359 0.007543320380092977 0.006916166805146062 -0.0014604001873920192 -0.0009387632814420455 0.006593443659874132 0.0069790196690636686 0.013051142318939446 0.014258078068439595
delta avg entropy: 0.006445202240009817
all entropy: 0.653997573824967 0.6733033357438176 0.6896960112606978 0.6890688576857509 0.691680263917418 0.6920454974061679 0.6907928972351914 0.6868717427125868 0.6560306638491367 0.6702887419175763
avg entropy: 0.6793775585553311
avg cont entropy: 0.6882842276275893
delta all entropy: 0.0 0.011510014967376359 0.007543320380092977 0.006916166805146062 -0.0014604001873920192 -0.0009387632814420455 0.006593443659874132 0.0069790196690636686 0.013051142318939446 0.014258078068439595
delta avg entropy: 0.006445202240009817
all entropy: 0.653997573824967 0.6733033357438176 0.6896960112606978 0.6890688576857509 0.691680263917418 0.6920454974061679 0.6907928972351914 0.6868717427125868 0.6560306638491367 0.6702887419175763
avg entropy: 0.6793775585553311
avg cont entropy: 0.6882842276275893
delta all entropy: 0.0 0.011510014967376359 0.007543320380092977 0.006916166805146062 -0.0014604001873920192 -0.0009387632814420455 0.006593443659874132 0.0069790196690636686 0.013051142318939446 0.014258078068439595
delta avg entropy: 0.006445202240009817
all entropy: 0.653997573824967 0.6733033357438176 0.6896960112606978 0.6890688576857509 0.691680263917418 0.6920454974061679 0.6907928972351914 0.6868717427125868 0.6560306638491367 0.6702887419175763
avg entropy: 0.6793775585553311
avg cont entropy: 0.6882842276275893
delta all entropy: 0.0 0.011510014967376359 0.007543320380092977 0.006916166805146062 -0.0014604001873920192 -0.0009387632814420455 0.006593443659874132 0.0069790196690636686 0.013051142318939446 0.014258078068439595
delta avg entropy: 0.006445202240009817
all entropy: 0.653997573824967 0.6733033357438176 0.6896960112606978 0.6890688576857509 0.691680263917418 0.6920454974061679 0.6907928972351914 0.6868717427125868 0.6560306638491367 0.6702887419175763
avg entropy: 0.6793775585553311
avg cont entropy: 0.6882842276275893
delta all entropy: 0.0 0.011510014967376359 0.007543320380092977 0.006916166805146062 -0.0014604001873920192 -0.0009387632814420455 0.006593443659874132 0.0069790196690636686 0.013051142318939446 0.014258078068439595
delta avg entropy: 0.006445202240009817
all entropy: 0.653997573824967 0.6733033357438176 0.6896960112606978 0.6890688576857509 0.691680263917418 0.6920454974061679 0.6907928972351914 0.6868717427125868 0.6560306638491367 0.6702887419175763
avg entropy: 0.6793775585553311
avg cont entropy: 0.6882842276275893
delta all entropy: 0.0 0.011510014967376359 0.007543320380092977 0.006916166805146062 -0.0014604001873920192 -0.0009387632814420455 0.006593443659874132 0.0069790196690636686 0.013051142318939446 0.014258078068439595
delta avg entropy: 0.006445202240009817
all entropy: 0.653997573824967 0.6733033357438176 0.6896960112606978 0.6890688576857509 0.691680263917418 0.6920454974061679 0.6907928972351914 0.6868717427125868 0.6560306638491367 0.6702887419175763
avg entropy: 0.6793775585553311
avg cont entropy: 0.6882842276275893
delta all entropy: 0.0 0.011510014967376359 0.007543320380092977 0.006916166805146062 -0.0014604001873920192 -0.0009387632814420455 0.006593443659874132 0.0069790196690636686 0.013051142318939446 0.014258078068439595
delta avg entropy: 0.006445202240009817
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 21:34:22,587 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-500
[INFO|configuration_utils.py:425] 2025-06-23 21:34:22,588 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-500/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:34:50,512 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:34:50,515 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:34:50,515 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-500/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:34:50,544 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-500/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 21:34:50,549] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is begin to save!
[2025-06-23 21:35:01,119] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-500/global_step500/mp_rank_00_model_states.pt
[2025-06-23 21:35:49,341] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is begin to save!
[2025-06-23 21:35:59,917] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-500/global_step500/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:36:51,632 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-400] due to args.save_total_limit
{'loss': 0.0111, 'learning_rate': 1.0934444444444447e-05, 'epoch': 51.0}
[INFO|trainer.py:2655] 2025-06-23 21:37:56,223 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-510
[INFO|configuration_utils.py:425] 2025-06-23 21:37:56,224 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-510/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:38:22,671 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-510/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:38:22,674 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-510/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:38:22,675 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-510/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:38:22,703 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-510/spiece.model
[2025-06-23 21:38:22,708] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step510 is begin to save!
[2025-06-23 21:38:33,311] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-510/global_step510/mp_rank_00_model_states.pt
[2025-06-23 21:39:25,561] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step510 is begin to save!
[2025-06-23 21:39:36,164] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-510/global_step510/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:40:36,970 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-410] due to args.save_total_limit
{'loss': 0.0107, 'learning_rate': 1.0713333333333336e-05, 'epoch': 52.0}
[INFO|trainer.py:2655] 2025-06-23 21:41:42,102 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-520
[INFO|configuration_utils.py:425] 2025-06-23 21:41:42,104 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-520/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:42:09,445 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-520/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:42:09,447 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-520/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:42:09,448 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-520/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:42:09,477 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-520/spiece.model
[2025-06-23 21:42:09,481] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step520 is begin to save!
[2025-06-23 21:42:20,117] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-520/global_step520/mp_rank_00_model_states.pt
[2025-06-23 21:43:14,391] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step520 is begin to save!
[2025-06-23 21:43:25,025] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-520/global_step520/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:44:26,425 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-420] due to args.save_total_limit
{'loss': 0.0103, 'learning_rate': 1.0492222222222222e-05, 'epoch': 53.0}
[INFO|trainer.py:2655] 2025-06-23 21:45:30,870 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-530
[INFO|configuration_utils.py:425] 2025-06-23 21:45:30,871 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-530/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:45:57,853 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-530/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:45:57,856 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-530/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:45:57,856 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-530/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:45:57,885 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-530/spiece.model
[2025-06-23 21:45:57,890] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step530 is begin to save!
[2025-06-23 21:46:08,686] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-530/global_step530/mp_rank_00_model_states.pt
[2025-06-23 21:47:01,403] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step530 is begin to save!
[2025-06-23 21:47:12,066] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-530/global_step530/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:48:10,587 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-430] due to args.save_total_limit
{'loss': 0.0107, 'learning_rate': 1.0271111111111115e-05, 'epoch': 54.0}
[INFO|trainer.py:2655] 2025-06-23 21:49:13,111 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-540
[INFO|configuration_utils.py:425] 2025-06-23 21:49:13,113 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-540/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:49:40,857 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-540/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:49:40,860 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-540/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:49:40,860 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-540/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:49:40,890 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-540/spiece.model
[2025-06-23 21:49:40,895] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step540 is begin to save!
[2025-06-23 21:49:53,530] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-540/global_step540/mp_rank_00_model_states.pt
[2025-06-23 21:50:47,779] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step540 is begin to save!
[2025-06-23 21:51:05,215] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-540/global_step540/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:52:12,270 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-440] due to args.save_total_limit
{'loss': 0.0108, 'learning_rate': 1.0050000000000002e-05, 'epoch': 55.0}
[INFO|trainer.py:2907] 2025-06-23 21:53:15,460 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 21:53:15,460 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 21:53:15,460 >>   Batch size = 100
{'max_accuracy': np.float64(88.09), 'median_accuracy': np.float64(84.48), 'mean_accuracy': np.float64(85.09), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.24), 'avg_ensemble_accuracy': 86.64, 'vote_ensemble_accuracy': 85.56}
{'max_accuracy': np.float64(88.09), 'median_accuracy': np.float64(84.48), 'mean_accuracy': np.float64(85.09), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.24), 'avg_ensemble_accuracy': 86.64, 'vote_ensemble_accuracy': 85.56}
{'max_accuracy': np.float64(88.09), 'median_accuracy': np.float64(84.48), 'mean_accuracy': np.float64(85.09), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.24), 'avg_ensemble_accuracy': 86.64, 'vote_ensemble_accuracy': 85.56}
{'max_accuracy': np.float64(88.09), 'median_accuracy': np.float64(84.48), 'mean_accuracy': np.float64(85.09), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.24), 'avg_ensemble_accuracy': 86.64, 'vote_ensemble_accuracy': 85.56}
[INFO|trainer.py:2907] 2025-06-23 21:53:43,033 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 21:53:43,033 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 21:53:43,033 >>   Batch size = 100
{'max_accuracy': np.float64(88.09), 'median_accuracy': np.float64(84.48), 'mean_accuracy': np.float64(85.09), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.24), 'avg_ensemble_accuracy': 86.64, 'vote_ensemble_accuracy': 85.56}
{'max_accuracy': np.float64(88.09), 'median_accuracy': np.float64(84.48), 'mean_accuracy': np.float64(85.09), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.24), 'avg_ensemble_accuracy': 86.64, 'vote_ensemble_accuracy': 85.56}
{'max_accuracy': np.float64(88.09), 'median_accuracy': np.float64(84.48), 'mean_accuracy': np.float64(85.09), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.24), 'avg_ensemble_accuracy': 86.64, 'vote_ensemble_accuracy': 85.56}
all entropy: 0.6560306638491367 0.678682484276075 0.6907928972351914 0.6896960112606978 0.691680263917418 0.6907928972351914 0.692358483831969 0.6876567982159768 0.6599283081693581 0.6761008358708656
avg entropy: 0.6813719643861881
avg cont entropy: 0.6888791354221825
delta all entropy: 0.002033090024169626 0.016889163499633764 0.008640206354586621 0.007543320380092977 -0.0014604001873920192 -0.002191363452418549 0.008159030256651656 0.007764075172453699 0.016948786639160884 0.020070172021728894
delta avg entropy: 0.008439608070866755
all entropy: 0.6560306638491367 0.678682484276075 0.6907928972351914 0.6896960112606978 0.691680263917418 0.6907928972351914 0.692358483831969 0.6876567982159768 0.6599283081693581 0.6761008358708656
avg entropy: 0.6813719643861881
avg cont entropy: 0.6888791354221825
delta all entropy: 0.002033090024169626 0.016889163499633764 0.008640206354586621 0.007543320380092977 -0.0014604001873920192 -0.002191363452418549 0.008159030256651656 0.007764075172453699 0.016948786639160884 0.020070172021728894
delta avg entropy: 0.008439608070866755
all entropy: 0.6560306638491367 0.678682484276075 0.6907928972351914 0.6896960112606978 0.691680263917418 0.6907928972351914 0.692358483831969 0.6876567982159768 0.6599283081693581 0.6761008358708656
avg entropy: 0.6813719643861881
avg cont entropy: 0.6888791354221825
delta all entropy: 0.002033090024169626 0.016889163499633764 0.008640206354586621 0.007543320380092977 -0.0014604001873920192 -0.002191363452418549 0.008159030256651656 0.007764075172453699 0.016948786639160884 0.020070172021728894
delta avg entropy: 0.008439608070866755
all entropy: 0.6560306638491367 0.678682484276075 0.6907928972351914 0.6896960112606978 0.691680263917418 0.6907928972351914 0.692358483831969 0.6876567982159768 0.6599283081693581 0.6761008358708656
avg entropy: 0.6813719643861881
avg cont entropy: 0.6888791354221825
delta all entropy: 0.002033090024169626 0.016889163499633764 0.008640206354586621 0.007543320380092977 -0.0014604001873920192 -0.002191363452418549 0.008159030256651656 0.007764075172453699 0.016948786639160884 0.020070172021728894
delta avg entropy: 0.008439608070866755
all entropy: 0.6560306638491367 0.678682484276075 0.6907928972351914 0.6896960112606978 0.691680263917418 0.6907928972351914 0.692358483831969 0.6876567982159768 0.6599283081693581 0.6761008358708656
avg entropy: 0.6813719643861881
avg cont entropy: 0.6888791354221825
delta all entropy: 0.002033090024169626 0.016889163499633764 0.008640206354586621 0.007543320380092977 -0.0014604001873920192 -0.002191363452418549 0.008159030256651656 0.007764075172453699 0.016948786639160884 0.020070172021728894
delta avg entropy: 0.008439608070866755
all entropy: 0.6560306638491367 0.678682484276075 0.6907928972351914 0.6896960112606978 0.691680263917418 0.6907928972351914 0.692358483831969 0.6876567982159768 0.6599283081693581 0.6761008358708656
avg entropy: 0.6813719643861881
avg cont entropy: 0.6888791354221825
delta all entropy: 0.002033090024169626 0.016889163499633764 0.008640206354586621 0.007543320380092977 -0.0014604001873920192 -0.002191363452418549 0.008159030256651656 0.007764075172453699 0.016948786639160884 0.020070172021728894
delta avg entropy: 0.008439608070866755
all entropy: 0.6560306638491367 0.678682484276075 0.6907928972351914 0.6896960112606978 0.691680263917418 0.6907928972351914 0.692358483831969 0.6876567982159768 0.6599283081693581 0.6761008358708656
avg entropy: 0.6813719643861881
avg cont entropy: 0.6888791354221825
delta all entropy: 0.002033090024169626 0.016889163499633764 0.008640206354586621 0.007543320380092977 -0.0014604001873920192 -0.002191363452418549 0.008159030256651656 0.007764075172453699 0.016948786639160884 0.020070172021728894
delta avg entropy: 0.008439608070866755
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 21:54:10,794 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-550
[INFO|configuration_utils.py:425] 2025-06-23 21:54:10,795 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-550/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:54:49,838 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-550/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:54:49,841 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-550/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:54:49,842 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-550/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:54:49,872 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-550/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 21:54:49,878] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step550 is begin to save!
[2025-06-23 21:55:07,614] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-550/global_step550/mp_rank_00_model_states.pt
[2025-06-23 21:56:12,279] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step550 is begin to save!
[2025-06-23 21:56:24,229] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-550/global_step550/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 21:57:22,626 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-450] due to args.save_total_limit
{'loss': 0.0107, 'learning_rate': 9.828888888888891e-06, 'epoch': 56.0}
[INFO|trainer.py:2655] 2025-06-23 21:58:24,844 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-560
[INFO|configuration_utils.py:425] 2025-06-23 21:58:24,846 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-560/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 21:58:54,026 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-560/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 21:58:54,029 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-560/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 21:58:54,030 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-560/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 21:58:54,065 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-560/spiece.model
[2025-06-23 21:58:54,070] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step560 is begin to save!
[2025-06-23 21:59:05,917] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-560/global_step560/mp_rank_00_model_states.pt
[2025-06-23 22:00:00,077] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step560 is begin to save!
[2025-06-23 22:00:11,874] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-560/global_step560/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:01:12,343 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-460] due to args.save_total_limit
{'loss': 0.0098, 'learning_rate': 9.607777777777779e-06, 'epoch': 57.0}
[INFO|trainer.py:2655] 2025-06-23 22:02:26,906 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-570
[INFO|configuration_utils.py:425] 2025-06-23 22:02:26,908 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-570/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:02:53,928 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-570/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:02:53,931 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-570/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:02:53,931 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-570/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:02:53,970 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-570/spiece.model
[2025-06-23 22:02:53,976] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step570 is begin to save!
[2025-06-23 22:03:05,071] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-570/global_step570/mp_rank_00_model_states.pt
[2025-06-23 22:03:54,620] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step570 is begin to save!
[2025-06-23 22:04:05,591] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-570/global_step570/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:05:01,084 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-470] due to args.save_total_limit
{'loss': 0.0099, 'learning_rate': 9.38666666666667e-06, 'epoch': 58.0}
[INFO|trainer.py:2655] 2025-06-23 22:06:05,186 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-580
[INFO|configuration_utils.py:425] 2025-06-23 22:06:05,187 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-580/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:06:33,178 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-580/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:06:33,181 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-580/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:06:33,182 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-580/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:06:33,211 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-580/spiece.model
[2025-06-23 22:06:33,216] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step580 is begin to save!
[2025-06-23 22:06:44,295] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-580/global_step580/mp_rank_00_model_states.pt
[2025-06-23 22:07:37,528] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step580 is begin to save!
[2025-06-23 22:07:48,684] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-580/global_step580/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:08:48,731 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-480] due to args.save_total_limit
{'loss': 0.0099, 'learning_rate': 9.165555555555559e-06, 'epoch': 59.0}
[INFO|trainer.py:2655] 2025-06-23 22:09:50,820 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-590
[INFO|configuration_utils.py:425] 2025-06-23 22:09:50,821 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-590/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:10:17,695 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:10:17,698 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:10:17,699 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-590/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:10:17,729 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-590/spiece.model
[2025-06-23 22:10:17,734] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step590 is begin to save!
[2025-06-23 22:10:29,084] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-590/global_step590/mp_rank_00_model_states.pt
[2025-06-23 22:11:23,034] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step590 is begin to save!
[2025-06-23 22:11:34,376] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-590/global_step590/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:12:34,005 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-490] due to args.save_total_limit
{'loss': 0.0101, 'learning_rate': 8.944444444444446e-06, 'epoch': 60.0}
[INFO|trainer.py:2907] 2025-06-23 22:13:36,087 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 22:13:36,088 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 22:13:36,088 >>   Batch size = 100
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 85.92}
[INFO|trainer.py:2907] 2025-06-23 22:14:03,480 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 22:14:03,480 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 22:14:03,480 >>   Batch size = 100
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 85.92}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.91), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.15), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 85.92}
all entropy: 0.6580074975533652 0.678682484276075 0.6926192559325051 0.6907928972351914 0.6883891439800401 0.6890688576857509 0.6928278409578548 0.6890688576857509 0.6636027524745085 0.6810494166250649
avg entropy: 0.6824109004406107
avg cont entropy: 0.6892573942943947
delta all entropy: 0.004009923728398146 0.016889163499633764 0.010466565051900245 0.008640206354586621 -0.00475152012476987 -0.003915403001859108 0.008628387382537461 0.009176134642227773 0.020623230944311288 0.025018752775928244
delta avg entropy: 0.009478544125289456
all entropy: 0.6580074975533652 0.678682484276075 0.6926192559325051 0.6907928972351914 0.6883891439800401 0.6890688576857509 0.6928278409578548 0.6890688576857509 0.6636027524745085 0.6810494166250649all entropy: 0.6580074975533652 0.678682484276075 0.6926192559325051 0.6907928972351914 0.6883891439800401 0.6890688576857509 0.6928278409578548 0.6890688576857509 0.6636027524745085 0.6810494166250649

avg entropy: 0.6824109004406107avg entropy: 0.6824109004406107

avg cont entropy: 0.6892573942943947avg cont entropy: 0.6892573942943947

delta all entropy: 0.004009923728398146 0.016889163499633764 0.010466565051900245 0.008640206354586621 -0.00475152012476987 -0.003915403001859108 0.008628387382537461 0.009176134642227773 0.020623230944311288 0.025018752775928244delta all entropy: 0.004009923728398146 0.016889163499633764 0.010466565051900245 0.008640206354586621 -0.00475152012476987 -0.003915403001859108 0.008628387382537461 0.009176134642227773 0.020623230944311288 0.025018752775928244

delta avg entropy: 0.009478544125289456
delta avg entropy: 0.009478544125289456
all entropy: 0.6580074975533652 0.678682484276075 0.6926192559325051 0.6907928972351914 0.6883891439800401 0.6890688576857509 0.6928278409578548 0.6890688576857509 0.6636027524745085 0.6810494166250649
avg entropy: 0.6824109004406107
avg cont entropy: 0.6892573942943947
delta all entropy: 0.004009923728398146 0.016889163499633764 0.010466565051900245 0.008640206354586621 -0.00475152012476987 -0.003915403001859108 0.008628387382537461 0.009176134642227773 0.020623230944311288 0.025018752775928244
delta avg entropy: 0.009478544125289456
all entropy: 0.6580074975533652 0.678682484276075 0.6926192559325051 0.6907928972351914 0.6883891439800401 0.6890688576857509 0.6928278409578548 0.6890688576857509 0.6636027524745085 0.6810494166250649
avg entropy: 0.6824109004406107
avg cont entropy: 0.6892573942943947
delta all entropy: 0.004009923728398146 0.016889163499633764 0.010466565051900245 0.008640206354586621 -0.00475152012476987 -0.003915403001859108 0.008628387382537461 0.009176134642227773 0.020623230944311288 0.025018752775928244
delta avg entropy: 0.009478544125289456
all entropy: 0.6580074975533652 0.678682484276075 0.6926192559325051 0.6907928972351914 0.6883891439800401 0.6890688576857509 0.6928278409578548 0.6890688576857509 0.6636027524745085 0.6810494166250649
avg entropy: 0.6824109004406107
avg cont entropy: 0.6892573942943947
delta all entropy: 0.004009923728398146 0.016889163499633764 0.010466565051900245 0.008640206354586621 -0.00475152012476987 -0.003915403001859108 0.008628387382537461 0.009176134642227773 0.020623230944311288 0.025018752775928244
delta avg entropy: 0.009478544125289456
all entropy: 0.6580074975533652 0.678682484276075 0.6926192559325051 0.6907928972351914 0.6883891439800401 0.6890688576857509 0.6928278409578548 0.6890688576857509 0.6636027524745085 0.6810494166250649
avg entropy: 0.6824109004406107
avg cont entropy: 0.6892573942943947
delta all entropy: 0.004009923728398146 0.016889163499633764 0.010466565051900245 0.008640206354586621 -0.00475152012476987 -0.003915403001859108 0.008628387382537461 0.009176134642227773 0.020623230944311288 0.025018752775928244
delta avg entropy: 0.009478544125289456
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 22:14:31,547 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-600
[INFO|configuration_utils.py:425] 2025-06-23 22:14:31,548 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-600/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:14:57,459 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-600/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:14:57,462 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:14:57,462 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-600/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:14:57,492 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-600/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 22:14:57,497] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is begin to save!
[2025-06-23 22:15:08,601] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-600/global_step600/mp_rank_00_model_states.pt
[2025-06-23 22:16:01,559] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is begin to save!
[2025-06-23 22:16:12,633] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-600/global_step600/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:17:12,641 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-500] due to args.save_total_limit
{'loss': 0.0102, 'learning_rate': 8.723333333333335e-06, 'epoch': 61.0}
[INFO|trainer.py:2655] 2025-06-23 22:18:13,975 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-610
[INFO|configuration_utils.py:425] 2025-06-23 22:18:13,976 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-610/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:18:41,084 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-610/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:18:41,087 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-610/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:18:41,087 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-610/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:18:41,117 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-610/spiece.model
[2025-06-23 22:18:41,122] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step610 is begin to save!
[2025-06-23 22:18:52,384] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-610/global_step610/mp_rank_00_model_states.pt
[2025-06-23 22:19:45,578] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step610 is begin to save!
[2025-06-23 22:19:56,861] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-610/global_step610/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:20:53,699 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-510] due to args.save_total_limit
{'loss': 0.0096, 'learning_rate': 8.502222222222226e-06, 'epoch': 62.0}
[INFO|trainer.py:2655] 2025-06-23 22:21:55,607 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-620
[INFO|configuration_utils.py:425] 2025-06-23 22:21:55,608 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-620/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:22:22,360 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-620/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:22:22,363 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-620/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:22:22,364 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-620/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:22:22,394 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-620/spiece.model
[2025-06-23 22:22:22,398] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step620 is begin to save!
[2025-06-23 22:22:33,625] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-620/global_step620/mp_rank_00_model_states.pt
[2025-06-23 22:23:27,540] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step620 is begin to save!
[2025-06-23 22:23:38,780] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-620/global_step620/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:24:39,960 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-520] due to args.save_total_limit
{'loss': 0.0089, 'learning_rate': 8.281111111111113e-06, 'epoch': 63.0}
[INFO|trainer.py:2655] 2025-06-23 22:25:42,581 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-630
[INFO|configuration_utils.py:425] 2025-06-23 22:25:42,582 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-630/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:26:10,504 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-630/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:26:10,507 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-630/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:26:10,507 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-630/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:26:10,538 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-630/spiece.model
[2025-06-23 22:26:10,543] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step630 is begin to save!
[2025-06-23 22:26:21,836] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-630/global_step630/mp_rank_00_model_states.pt
[2025-06-23 22:27:14,933] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step630 is begin to save!
[2025-06-23 22:27:29,558] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-630/global_step630/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:28:31,832 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-530] due to args.save_total_limit
{'loss': 0.0096, 'learning_rate': 8.060000000000002e-06, 'epoch': 64.0}
[INFO|trainer.py:2655] 2025-06-23 22:29:33,187 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-640
[INFO|configuration_utils.py:425] 2025-06-23 22:29:33,188 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-640/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:30:00,723 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-640/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:30:00,726 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-640/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:30:00,726 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-640/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:30:00,757 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-640/spiece.model
[2025-06-23 22:30:00,762] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step640 is begin to save!
[2025-06-23 22:30:18,657] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-640/global_step640/mp_rank_00_model_states.pt
[2025-06-23 22:31:18,182] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step640 is begin to save!
[2025-06-23 22:31:29,366] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-640/global_step640/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:32:28,901 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-540] due to args.save_total_limit
{'loss': 0.0096, 'learning_rate': 7.83888888888889e-06, 'epoch': 65.0}
[INFO|trainer.py:2907] 2025-06-23 22:33:41,550 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 22:33:41,551 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 22:33:41,551 >>   Batch size = 100
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 86.28}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 86.28}
[INFO|trainer.py:2907] 2025-06-23 22:34:08,943 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 22:34:08,944 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 22:34:08,944 >>   Batch size = 100
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 86.28}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 86.28}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 86.28}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 86.28}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.73, 'vote_ensemble_accuracy': 86.28}
all entropy: 0.6580074975533652 0.6821526908806048 0.6930885314445381 0.6912627451198297 0.6883891439800401 0.6890688576857509 0.69314066410481 0.6907928972351914 0.6653568125489489 0.6832026653984116
avg entropy: 0.6834462505951491
avg cont entropy: 0.6894183923963622
delta all entropy: 0.004009923728398146 0.020359370104163554 0.010935840563933263 0.00911005423922484 -0.00475152012476987 -0.003915403001859108 0.008941210529492705 0.010900174191668333 0.022377291018751633 0.027172001549274905
delta avg entropy: 0.01051389427982784
all entropy: 0.6580074975533652 0.6821526908806048 0.6930885314445381 0.6912627451198297 0.6883891439800401 0.6890688576857509 0.69314066410481 0.6907928972351914 0.6653568125489489 0.6832026653984116
avg entropy: 0.6834462505951491
avg cont entropy: 0.6894183923963622
delta all entropy: 0.004009923728398146 0.020359370104163554 0.010935840563933263 0.00911005423922484 -0.00475152012476987 -0.003915403001859108 0.008941210529492705 0.010900174191668333 0.022377291018751633 0.027172001549274905
delta avg entropy: 0.01051389427982784
all entropy: 0.6580074975533652 0.6821526908806048 0.6930885314445381 0.6912627451198297 0.6883891439800401 0.6890688576857509 0.69314066410481 0.6907928972351914 0.6653568125489489 0.6832026653984116
avg entropy: 0.6834462505951491
avg cont entropy: 0.6894183923963622
all entropy: 0.6580074975533652 0.6821526908806048 0.6930885314445381 0.6912627451198297 0.6883891439800401 0.6890688576857509 0.69314066410481 0.6907928972351914 0.6653568125489489 0.6832026653984116
delta all entropy: 0.004009923728398146 0.020359370104163554 0.010935840563933263 0.00911005423922484 -0.00475152012476987 -0.003915403001859108 0.008941210529492705 0.010900174191668333 0.022377291018751633 0.027172001549274905
delta avg entropy: 0.01051389427982784
avg entropy: 0.6834462505951491
avg cont entropy: 0.6894183923963622
delta all entropy: 0.004009923728398146 0.020359370104163554 0.010935840563933263 0.00911005423922484 -0.00475152012476987 -0.003915403001859108 0.008941210529492705 0.010900174191668333 0.022377291018751633 0.027172001549274905
delta avg entropy: 0.01051389427982784
all entropy: 0.6580074975533652 0.6821526908806048 0.6930885314445381 0.6912627451198297 0.6883891439800401 0.6890688576857509 0.69314066410481 0.6907928972351914 0.6653568125489489 0.6832026653984116
avg entropy: 0.6834462505951491
avg cont entropy: 0.6894183923963622
delta all entropy: 0.004009923728398146 0.020359370104163554 0.010935840563933263 0.00911005423922484 -0.00475152012476987 -0.003915403001859108 0.008941210529492705 0.010900174191668333 0.022377291018751633 0.027172001549274905
delta avg entropy: 0.01051389427982784
all entropy: 0.6580074975533652 0.6821526908806048 0.6930885314445381 0.6912627451198297 0.6883891439800401 0.6890688576857509 0.69314066410481 0.6907928972351914 0.6653568125489489 0.6832026653984116
avg entropy: 0.6834462505951491
avg cont entropy: 0.6894183923963622
delta all entropy: 0.004009923728398146 0.020359370104163554 0.010935840563933263 0.00911005423922484 -0.00475152012476987 -0.003915403001859108 0.008941210529492705 0.010900174191668333 0.022377291018751633 0.027172001549274905
delta avg entropy: 0.01051389427982784
all entropy: 0.6580074975533652 0.6821526908806048 0.6930885314445381 0.6912627451198297 0.6883891439800401 0.6890688576857509 0.69314066410481 0.6907928972351914 0.6653568125489489 0.6832026653984116
avg entropy: 0.6834462505951491
avg cont entropy: 0.6894183923963622
delta all entropy: 0.004009923728398146 0.020359370104163554 0.010935840563933263 0.00911005423922484 -0.00475152012476987 -0.003915403001859108 0.008941210529492705 0.010900174191668333 0.022377291018751633 0.027172001549274905
delta avg entropy: 0.01051389427982784
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 22:34:37,212 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-650
[INFO|configuration_utils.py:425] 2025-06-23 22:34:37,213 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-650/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:35:04,927 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-650/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:35:04,930 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-650/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:35:04,931 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-650/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:35:04,963 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-650/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 22:35:04,970] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step650 is begin to save!
[2025-06-23 22:35:15,646] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-650/global_step650/mp_rank_00_model_states.pt
[2025-06-23 22:36:04,482] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step650 is begin to save!
[2025-06-23 22:36:15,118] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-650/global_step650/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:37:09,432 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-550] due to args.save_total_limit
{'loss': 0.0086, 'learning_rate': 7.617777777777778e-06, 'epoch': 66.0}
[INFO|trainer.py:2655] 2025-06-23 22:38:12,899 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-660
[INFO|configuration_utils.py:425] 2025-06-23 22:38:12,900 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-660/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:38:40,320 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-660/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:38:40,323 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-660/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:38:40,323 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-660/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:38:40,365 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-660/spiece.model
[2025-06-23 22:38:40,372] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step660 is begin to save!
[2025-06-23 22:38:50,975] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-660/global_step660/mp_rank_00_model_states.pt
[2025-06-23 22:39:44,149] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step660 is begin to save!
[2025-06-23 22:39:54,730] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-660/global_step660/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:40:52,780 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-560] due to args.save_total_limit
{'loss': 0.0092, 'learning_rate': 7.396666666666669e-06, 'epoch': 67.0}
[INFO|trainer.py:2655] 2025-06-23 22:41:55,850 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-670
[INFO|configuration_utils.py:425] 2025-06-23 22:41:55,851 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-670/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:42:23,036 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-670/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:42:23,039 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-670/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:42:23,040 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-670/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:42:23,069 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-670/spiece.model
[2025-06-23 22:42:23,074] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step670 is begin to save!
[2025-06-23 22:42:33,659] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-670/global_step670/mp_rank_00_model_states.pt
[2025-06-23 22:43:24,493] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step670 is begin to save!
[2025-06-23 22:43:35,084] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-670/global_step670/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:44:31,319 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-570] due to args.save_total_limit
{'loss': 0.0088, 'learning_rate': 7.1755555555555556e-06, 'epoch': 68.0}
[INFO|trainer.py:2655] 2025-06-23 22:45:37,366 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-680
[INFO|configuration_utils.py:425] 2025-06-23 22:45:37,367 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-680/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:46:04,458 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-680/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:46:04,461 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:46:04,461 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-680/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:46:04,495 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-680/spiece.model
[2025-06-23 22:46:04,501] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step680 is begin to save!
[2025-06-23 22:46:15,038] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-680/global_step680/mp_rank_00_model_states.pt
[2025-06-23 22:47:09,246] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step680 is begin to save!
[2025-06-23 22:47:19,744] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-680/global_step680/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:48:19,217 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-580] due to args.save_total_limit
{'loss': 0.0095, 'learning_rate': 6.9544444444444455e-06, 'epoch': 69.0}
[INFO|trainer.py:2655] 2025-06-23 22:49:23,309 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-690
[INFO|configuration_utils.py:425] 2025-06-23 22:49:23,311 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-690/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:49:51,893 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-690/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:49:51,896 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-690/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:49:51,897 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-690/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:49:51,927 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-690/spiece.model
[2025-06-23 22:49:51,932] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step690 is begin to save!
[2025-06-23 22:50:02,650] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-690/global_step690/mp_rank_00_model_states.pt
[2025-06-23 22:50:55,232] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step690 is begin to save!
[2025-06-23 22:51:05,900] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-690/global_step690/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:52:02,247 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-590] due to args.save_total_limit
{'loss': 0.0091, 'learning_rate': 6.733333333333335e-06, 'epoch': 70.0}
[INFO|trainer.py:2907] 2025-06-23 22:53:03,527 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 22:53:03,527 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 22:53:03,528 >>   Batch size = 100
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.36, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.36, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.36, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.36, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.36, 'vote_ensemble_accuracy': 86.64}
[INFO|trainer.py:2907] 2025-06-23 22:53:31,117 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 22:53:31,119 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 22:53:31,120 >>   Batch size = 100
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.36, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.87), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.18), 'avg_ensemble_accuracy': 87.36, 'vote_ensemble_accuracy': 86.64}
all entropy: 0.6580074975533652 0.6841994535753173 0.6930885314445381 0.69298426068761 0.6860338939853872 0.6883891439800401 0.6926192559325051 0.6912627451198297 0.6670557026278856 0.6832026653984116
avg entropy: 0.683684315030489
avg cont entropy: 0.6893899271505719
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010935840563933263 0.01083156980700517 -0.007106770119422845 -0.004595116707569846 0.008419802357187756 0.011370022076306552 0.024076181097688387 0.027172001549274905
delta avg entropy: 0.010751958715167754
all entropy: 0.6580074975533652 0.6841994535753173 0.6930885314445381 0.69298426068761 0.6860338939853872 0.6883891439800401 0.6926192559325051 0.6912627451198297 0.6670557026278856 0.6832026653984116
avg entropy: 0.683684315030489
avg cont entropy: 0.6893899271505719
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010935840563933263 0.01083156980700517 -0.007106770119422845 -0.004595116707569846 0.008419802357187756 0.011370022076306552 0.024076181097688387 0.027172001549274905
delta avg entropy: 0.010751958715167754
all entropy: 0.6580074975533652 0.6841994535753173 0.6930885314445381 0.69298426068761 0.6860338939853872 0.6883891439800401 0.6926192559325051 0.6912627451198297 0.6670557026278856 0.6832026653984116
avg entropy: 0.683684315030489
avg cont entropy: 0.6893899271505719
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010935840563933263 0.01083156980700517 -0.007106770119422845 -0.004595116707569846 0.008419802357187756 0.011370022076306552 0.024076181097688387 0.027172001549274905
delta avg entropy: 0.010751958715167754
all entropy: 0.6580074975533652 0.6841994535753173 0.6930885314445381 0.69298426068761 0.6860338939853872 0.6883891439800401 0.6926192559325051 0.6912627451198297 0.6670557026278856 0.6832026653984116
avg entropy: 0.683684315030489
avg cont entropy: 0.6893899271505719
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010935840563933263 0.01083156980700517 -0.007106770119422845 -0.004595116707569846 0.008419802357187756 0.011370022076306552 0.024076181097688387 0.027172001549274905
delta avg entropy: 0.010751958715167754
all entropy: 0.6580074975533652 0.6841994535753173 0.6930885314445381 0.69298426068761 0.6860338939853872 0.6883891439800401 0.6926192559325051 0.6912627451198297 0.6670557026278856 0.6832026653984116
avg entropy: 0.683684315030489
avg cont entropy: 0.6893899271505719
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010935840563933263 0.01083156980700517 -0.007106770119422845 -0.004595116707569846 0.008419802357187756 0.011370022076306552 0.024076181097688387 0.027172001549274905
delta avg entropy: 0.010751958715167754
all entropy: 0.6580074975533652 0.6841994535753173 0.6930885314445381 0.69298426068761 0.6860338939853872 0.6883891439800401 0.6926192559325051 0.6912627451198297 0.6670557026278856 0.6832026653984116
avg entropy: 0.683684315030489
avg cont entropy: 0.6893899271505719
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010935840563933263 0.01083156980700517 -0.007106770119422845 -0.004595116707569846 0.008419802357187756 0.011370022076306552 0.024076181097688387 0.027172001549274905
delta avg entropy: 0.010751958715167754
all entropy: 0.6580074975533652 0.6841994535753173 0.6930885314445381 0.69298426068761 0.6860338939853872 0.6883891439800401 0.6926192559325051 0.6912627451198297 0.6670557026278856 0.6832026653984116
avg entropy: 0.683684315030489
avg cont entropy: 0.6893899271505719
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010935840563933263 0.01083156980700517 -0.007106770119422845 -0.004595116707569846 0.008419802357187756 0.011370022076306552 0.024076181097688387 0.027172001549274905
delta avg entropy: 0.010751958715167754
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 22:53:58,878 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-700
[INFO|configuration_utils.py:425] 2025-06-23 22:53:58,879 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-700/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:54:25,455 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-700/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:54:25,458 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:54:25,458 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-700/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:54:25,488 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-700/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 22:54:25,493] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step700 is begin to save!
[2025-06-23 22:54:35,971] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-700/global_step700/mp_rank_00_model_states.pt
[2025-06-23 22:55:28,425] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step700 is begin to save!
[2025-06-23 22:55:38,883] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-700/global_step700/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 22:56:35,749 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-600] due to args.save_total_limit
{'loss': 0.0087, 'learning_rate': 6.512222222222222e-06, 'epoch': 71.0}
[INFO|trainer.py:2655] 2025-06-23 22:57:38,820 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-710
[INFO|configuration_utils.py:425] 2025-06-23 22:57:38,822 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-710/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 22:58:04,426 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-710/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 22:58:04,429 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-710/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 22:58:04,430 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-710/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 22:58:04,460 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-710/spiece.model
[2025-06-23 22:58:04,465] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step710 is begin to save!
[2025-06-23 22:58:14,859] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-710/global_step710/mp_rank_00_model_states.pt
[2025-06-23 22:59:03,059] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step710 is begin to save!
[2025-06-23 22:59:16,174] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-710/global_step710/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:00:14,446 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-610] due to args.save_total_limit
{'loss': 0.0089, 'learning_rate': 6.291111111111112e-06, 'epoch': 72.0}
[INFO|trainer.py:2655] 2025-06-23 23:01:16,099 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-720
[INFO|configuration_utils.py:425] 2025-06-23 23:01:16,100 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-720/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:01:42,839 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-720/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:01:42,842 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-720/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:01:42,842 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-720/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:01:42,874 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-720/spiece.model
[2025-06-23 23:01:42,879] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step720 is begin to save!
[2025-06-23 23:01:59,188] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-720/global_step720/mp_rank_00_model_states.pt
[2025-06-23 23:02:54,421] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step720 is begin to save!
[2025-06-23 23:03:06,198] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-720/global_step720/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:04:06,349 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-620] due to args.save_total_limit
{'loss': 0.0089, 'learning_rate': 6.070000000000001e-06, 'epoch': 73.0}
[INFO|trainer.py:2655] 2025-06-23 23:05:21,000 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-730
[INFO|configuration_utils.py:425] 2025-06-23 23:05:21,002 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-730/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:05:49,203 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-730/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:05:49,206 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-730/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:05:49,206 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-730/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:05:49,238 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-730/spiece.model
[2025-06-23 23:05:49,244] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step730 is begin to save!
[2025-06-23 23:06:00,579] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-730/global_step730/mp_rank_00_model_states.pt
[2025-06-23 23:06:51,228] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step730 is begin to save!
[2025-06-23 23:07:02,537] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-730/global_step730/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:08:00,319 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-630] due to args.save_total_limit
{'loss': 0.0085, 'learning_rate': 5.848888888888889e-06, 'epoch': 74.0}
[INFO|trainer.py:2655] 2025-06-23 23:09:02,362 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-740
[INFO|configuration_utils.py:425] 2025-06-23 23:09:02,364 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-740/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:09:29,869 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-740/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:09:29,872 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-740/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:09:29,873 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-740/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:09:29,914 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-740/spiece.model
[2025-06-23 23:09:29,920] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step740 is begin to save!
[2025-06-23 23:09:41,357] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-740/global_step740/mp_rank_00_model_states.pt
[2025-06-23 23:10:35,239] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step740 is begin to save!
[2025-06-23 23:10:46,649] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-740/global_step740/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:11:39,955 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-640] due to args.save_total_limit
{'loss': 0.0087, 'learning_rate': 5.6277777777777786e-06, 'epoch': 75.0}
[INFO|trainer.py:2907] 2025-06-23 23:12:43,189 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 23:12:43,190 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 23:12:43,190 >>   Batch size = 100
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.73), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.25), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.73), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.25), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.73), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.25), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.73), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.25), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.73), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.25), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.73), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.25), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 86.64}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.73), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.25), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 86.64}
[INFO|trainer.py:2907] 2025-06-23 23:13:11,423 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 23:13:11,424 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 23:13:11,424 >>   Batch size = 100
all entropy: 0.6580074975533652 0.6841994535753173 0.69314066410481 0.69298426068761 0.6774185681062022 0.6876567982159768 0.692358483831969 0.6926192559325051 0.6670557026278856 0.6851431626922719
avg entropy: 0.6830583847327913
avg cont entropy: 0.6892298892190827
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010987973224205194 0.01083156980700517 -0.015722095998607855 -0.005327462471633182 0.008159030256651656 0.012726532888981956 0.024076181097688387 0.02911249884313527
delta avg entropy: 0.010126028417470078
all entropy: 0.6580074975533652 0.6841994535753173 0.69314066410481 0.69298426068761 0.6774185681062022 0.6876567982159768 0.692358483831969 0.6926192559325051 0.6670557026278856 0.6851431626922719
avg entropy: 0.6830583847327913
avg cont entropy: 0.6892298892190827
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010987973224205194 0.01083156980700517 -0.015722095998607855 -0.005327462471633182 0.008159030256651656 0.012726532888981956 0.024076181097688387 0.02911249884313527
delta avg entropy: 0.010126028417470078
all entropy: 0.6580074975533652 0.6841994535753173 0.69314066410481 0.69298426068761 0.6774185681062022 0.6876567982159768 0.692358483831969 0.6926192559325051 0.6670557026278856 0.6851431626922719
avg entropy: 0.6830583847327913
avg cont entropy: 0.6892298892190827
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010987973224205194 0.01083156980700517 -0.015722095998607855 -0.005327462471633182 0.008159030256651656 0.012726532888981956 0.024076181097688387 0.02911249884313527
delta avg entropy: 0.010126028417470078
all entropy: 0.6580074975533652 0.6841994535753173 0.69314066410481 0.69298426068761 0.6774185681062022 0.6876567982159768 0.692358483831969 0.6926192559325051 0.6670557026278856 0.6851431626922719
avg entropy: 0.6830583847327913
avg cont entropy: 0.6892298892190827
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010987973224205194 0.01083156980700517 -0.015722095998607855 -0.005327462471633182 0.008159030256651656 0.012726532888981956 0.024076181097688387 0.02911249884313527
delta avg entropy: 0.010126028417470078
all entropy: 0.6580074975533652 0.6841994535753173 0.69314066410481 0.69298426068761 0.6774185681062022 0.6876567982159768 0.692358483831969 0.6926192559325051 0.6670557026278856 0.6851431626922719
avg entropy: 0.6830583847327913
avg cont entropy: 0.6892298892190827
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010987973224205194 0.01083156980700517 -0.015722095998607855 -0.005327462471633182 0.008159030256651656 0.012726532888981956 0.024076181097688387 0.02911249884313527
delta avg entropy: 0.010126028417470078
all entropy: 0.6580074975533652 0.6841994535753173 0.69314066410481 0.69298426068761 0.6774185681062022 0.6876567982159768 0.692358483831969 0.6926192559325051 0.6670557026278856 0.6851431626922719
avg entropy: 0.6830583847327913
avg cont entropy: 0.6892298892190827
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010987973224205194 0.01083156980700517 -0.015722095998607855 -0.005327462471633182 0.008159030256651656 0.012726532888981956 0.024076181097688387 0.02911249884313527
delta avg entropy: 0.010126028417470078
all entropy: 0.6580074975533652 0.6841994535753173 0.69314066410481 0.69298426068761 0.6774185681062022 0.6876567982159768 0.692358483831969 0.6926192559325051 0.6670557026278856 0.6851431626922719
avg entropy: 0.6830583847327913
avg cont entropy: 0.6892298892190827
delta all entropy: 0.004009923728398146 0.022406132798876044 0.010987973224205194 0.01083156980700517 -0.015722095998607855 -0.005327462471633182 0.008159030256651656 0.012726532888981956 0.024076181097688387 0.02911249884313527
delta avg entropy: 0.010126028417470078
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 23:13:38,546 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-750
[INFO|configuration_utils.py:425] 2025-06-23 23:13:38,547 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-750/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:14:05,942 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-750/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:14:05,945 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:14:05,946 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-750/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:14:05,974 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-750/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 23:14:05,980] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step750 is begin to save!
[2025-06-23 23:14:16,980] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-750/global_step750/mp_rank_00_model_states.pt
[2025-06-23 23:15:08,060] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step750 is begin to save!
[2025-06-23 23:15:19,055] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-750/global_step750/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:16:16,932 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-650] due to args.save_total_limit
{'loss': 0.0083, 'learning_rate': 5.4066666666666685e-06, 'epoch': 76.0}
[INFO|trainer.py:2655] 2025-06-23 23:17:20,005 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-760
[INFO|configuration_utils.py:425] 2025-06-23 23:17:20,006 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-760/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:17:46,067 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-760/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:17:46,071 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-760/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:17:46,071 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-760/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:17:46,100 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-760/spiece.model
[2025-06-23 23:17:46,105] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step760 is begin to save!
[2025-06-23 23:17:57,242] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-760/global_step760/mp_rank_00_model_states.pt
[2025-06-23 23:18:50,919] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step760 is begin to save!
[2025-06-23 23:19:02,002] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-760/global_step760/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:19:58,407 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-660] due to args.save_total_limit
{'loss': 0.0083, 'learning_rate': 5.185555555555556e-06, 'epoch': 77.0}
[INFO|trainer.py:2655] 2025-06-23 23:20:59,801 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-770
[INFO|configuration_utils.py:425] 2025-06-23 23:20:59,803 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-770/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:21:26,319 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-770/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:21:26,322 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-770/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:21:26,322 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-770/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:21:26,352 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-770/spiece.model
[2025-06-23 23:21:26,357] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step770 is begin to save!
[2025-06-23 23:21:37,523] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-770/global_step770/mp_rank_00_model_states.pt
[2025-06-23 23:22:28,088] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step770 is begin to save!
[2025-06-23 23:22:39,278] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-770/global_step770/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:23:34,342 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-670] due to args.save_total_limit
{'loss': 0.0079, 'learning_rate': 4.964444444444445e-06, 'epoch': 78.0}
[INFO|trainer.py:2655] 2025-06-23 23:24:37,399 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-780
[INFO|configuration_utils.py:425] 2025-06-23 23:24:37,400 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-780/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:25:04,882 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:25:04,885 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:25:04,886 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-780/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:25:04,915 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-780/spiece.model
[2025-06-23 23:25:04,920] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step780 is begin to save!
[2025-06-23 23:25:16,005] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-780/global_step780/mp_rank_00_model_states.pt
[2025-06-23 23:26:09,074] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step780 is begin to save!
[2025-06-23 23:26:20,149] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-780/global_step780/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:27:19,890 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-680] due to args.save_total_limit
{'loss': 0.0083, 'learning_rate': 4.7433333333333325e-06, 'epoch': 79.0}
[INFO|trainer.py:2655] 2025-06-23 23:28:21,924 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-790
[INFO|configuration_utils.py:425] 2025-06-23 23:28:21,926 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-790/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:28:49,580 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-790/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:28:49,583 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-790/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:28:49,584 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-790/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:28:49,614 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-790/spiece.model
[2025-06-23 23:28:49,619] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step790 is begin to save!
[2025-06-23 23:29:00,876] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-790/global_step790/mp_rank_00_model_states.pt
[2025-06-23 23:29:52,298] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step790 is begin to save!
[2025-06-23 23:30:03,598] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-790/global_step790/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:31:00,048 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-690] due to args.save_total_limit
{'loss': 0.0091, 'learning_rate': 4.5222222222222225e-06, 'epoch': 80.0}
[INFO|trainer.py:2907] 2025-06-23 23:32:04,738 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 23:32:04,738 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 23:32:04,738 >>   Batch size = 100
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.77), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(1.96), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.77), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(1.96), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.77), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(1.96), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.77), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(1.96), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
[INFO|trainer.py:2907] 2025-06-23 23:32:32,590 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 23:32:32,591 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 23:32:32,591 >>   Batch size = 100
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.77), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(1.96), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.77), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(1.96), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.77), 'min_accuracy': np.float64(81.59), 'std_accuracy': np.float64(1.96), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
all entropy: 0.6580074975533652 0.6860338939853872 0.6930885314445381 0.6930885314445381 0.678682484276075 0.6860338939853872 0.6912627451198297 0.6926192559325051 0.6702887419175763 0.6860338939853872
avg entropy: 0.6835139469644589
avg cont entropy: 0.6890323141434106
delta all entropy: 0.004009923728398146 0.024240573208945904 0.010935840563933263 0.010935840563933263 -0.014458179828734985 -0.006950366702222821 0.007063291544512351 0.012726532888981956 0.02730922038737904 0.030003230136250503
delta avg entropy: 0.010581590649137662
all entropy: 0.6580074975533652 0.6860338939853872 0.6930885314445381 0.6930885314445381 0.678682484276075 0.6860338939853872 0.6912627451198297 0.6926192559325051 0.6702887419175763 0.6860338939853872
avg entropy: 0.6835139469644589
avg cont entropy: 0.6890323141434106
delta all entropy: 0.004009923728398146 0.024240573208945904 0.010935840563933263 0.010935840563933263 -0.014458179828734985 -0.006950366702222821 0.007063291544512351 0.012726532888981956 0.02730922038737904 0.030003230136250503
delta avg entropy: 0.010581590649137662
all entropy: 0.6580074975533652 0.6860338939853872 0.6930885314445381 0.6930885314445381 0.678682484276075 0.6860338939853872 0.6912627451198297 0.6926192559325051 0.6702887419175763 0.6860338939853872
avg entropy: 0.6835139469644589
avg cont entropy: 0.6890323141434106
delta all entropy: 0.004009923728398146 0.024240573208945904 0.010935840563933263 0.010935840563933263 -0.014458179828734985 -0.006950366702222821 0.007063291544512351 0.012726532888981956 0.02730922038737904 0.030003230136250503
delta avg entropy: 0.010581590649137662
all entropy: 0.6580074975533652 0.6860338939853872 0.6930885314445381 0.6930885314445381 0.678682484276075 0.6860338939853872 0.6912627451198297 0.6926192559325051 0.6702887419175763 0.6860338939853872
avg entropy: 0.6835139469644589
avg cont entropy: 0.6890323141434106
delta all entropy: 0.004009923728398146 0.024240573208945904 0.010935840563933263 0.010935840563933263 -0.014458179828734985 -0.006950366702222821 0.007063291544512351 0.012726532888981956 0.02730922038737904 0.030003230136250503
delta avg entropy: 0.010581590649137662
all entropy: 0.6580074975533652 0.6860338939853872 0.6930885314445381 0.6930885314445381 0.678682484276075 0.6860338939853872 0.6912627451198297 0.6926192559325051 0.6702887419175763 0.6860338939853872
avg entropy: 0.6835139469644589
avg cont entropy: 0.6890323141434106
delta all entropy: 0.004009923728398146 0.024240573208945904 0.010935840563933263 0.010935840563933263 -0.014458179828734985 -0.006950366702222821 0.007063291544512351 0.012726532888981956 0.02730922038737904 0.030003230136250503
delta avg entropy: 0.010581590649137662
all entropy: 0.6580074975533652 0.6860338939853872 0.6930885314445381 0.6930885314445381 0.678682484276075 0.6860338939853872 0.6912627451198297 0.6926192559325051 0.6702887419175763 0.6860338939853872
avg entropy: 0.6835139469644589
avg cont entropy: 0.6890323141434106
delta all entropy: 0.004009923728398146 0.024240573208945904 0.010935840563933263 0.010935840563933263 -0.014458179828734985 -0.006950366702222821 0.007063291544512351 0.012726532888981956 0.02730922038737904 0.030003230136250503
delta avg entropy: 0.010581590649137662
all entropy: 0.6580074975533652 0.6860338939853872 0.6930885314445381 0.6930885314445381 0.678682484276075 0.6860338939853872 0.6912627451198297 0.6926192559325051 0.6702887419175763 0.6860338939853872
avg entropy: 0.6835139469644589
avg cont entropy: 0.6890323141434106
delta all entropy: 0.004009923728398146 0.024240573208945904 0.010935840563933263 0.010935840563933263 -0.014458179828734985 -0.006950366702222821 0.007063291544512351 0.012726532888981956 0.02730922038737904 0.030003230136250503
delta avg entropy: 0.010581590649137662
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 23:33:00,137 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-800
[INFO|configuration_utils.py:425] 2025-06-23 23:33:00,138 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-800/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:33:25,948 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-800/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:33:25,951 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:33:25,951 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-800/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:33:25,981 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-800/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 23:33:25,987] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is begin to save!
[2025-06-23 23:33:36,992] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-800/global_step800/mp_rank_00_model_states.pt
[2025-06-23 23:34:31,488] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is begin to save!
[2025-06-23 23:34:46,957] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-800/global_step800/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:35:49,450 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-700] due to args.save_total_limit
{'loss': 0.0085, 'learning_rate': 4.3011111111111125e-06, 'epoch': 81.0}
[INFO|trainer.py:2655] 2025-06-23 23:36:59,407 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-810
[INFO|configuration_utils.py:425] 2025-06-23 23:36:59,408 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-810/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:37:27,820 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-810/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:37:27,823 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-810/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:37:27,823 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-810/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:37:27,851 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-810/spiece.model
[2025-06-23 23:37:27,856] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step810 is begin to save!
[2025-06-23 23:37:39,201] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-810/global_step810/mp_rank_00_model_states.pt
[2025-06-23 23:38:26,995] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step810 is begin to save!
[2025-06-23 23:38:38,283] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-810/global_step810/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:39:31,999 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-710] due to args.save_total_limit
{'loss': 0.0083, 'learning_rate': 4.08e-06, 'epoch': 82.0}
[INFO|trainer.py:2655] 2025-06-23 23:40:33,394 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-820
[INFO|configuration_utils.py:425] 2025-06-23 23:40:33,395 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-820/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:41:00,791 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-820/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:41:00,793 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:41:00,794 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-820/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:41:00,823 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-820/spiece.model
[2025-06-23 23:41:00,828] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step820 is begin to save!
[2025-06-23 23:41:11,879] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-820/global_step820/mp_rank_00_model_states.pt
[2025-06-23 23:42:02,908] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step820 is begin to save!
[2025-06-23 23:42:13,962] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-820/global_step820/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:43:13,309 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-720] due to args.save_total_limit
{'loss': 0.009, 'learning_rate': 3.858888888888889e-06, 'epoch': 83.0}
[INFO|trainer.py:2655] 2025-06-23 23:44:15,471 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-830
[INFO|configuration_utils.py:425] 2025-06-23 23:44:15,472 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-830/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:44:42,763 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-830/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:44:42,766 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-830/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:44:42,766 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-830/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:44:42,795 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-830/spiece.model
[2025-06-23 23:44:42,800] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step830 is begin to save!
[2025-06-23 23:44:53,877] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-830/global_step830/mp_rank_00_model_states.pt
[2025-06-23 23:45:48,147] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step830 is begin to save!
[2025-06-23 23:45:59,181] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-830/global_step830/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:46:53,907 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-730] due to args.save_total_limit
{'loss': 0.0084, 'learning_rate': 3.637777777777779e-06, 'epoch': 84.0}
[INFO|trainer.py:2655] 2025-06-23 23:47:56,695 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-840
[INFO|configuration_utils.py:425] 2025-06-23 23:47:56,696 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-840/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:48:23,657 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-840/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:48:23,660 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-840/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:48:23,661 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-840/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:48:23,690 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-840/spiece.model
[2025-06-23 23:48:23,695] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step840 is begin to save!
[2025-06-23 23:48:34,872] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-840/global_step840/mp_rank_00_model_states.pt
[2025-06-23 23:49:24,506] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step840 is begin to save!
[2025-06-23 23:49:38,746] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-840/global_step840/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:50:41,560 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-740] due to args.save_total_limit
{'loss': 0.0084, 'learning_rate': 3.4166666666666664e-06, 'epoch': 85.0}
[INFO|trainer.py:2907] 2025-06-23 23:51:44,771 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 23:51:44,772 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 23:51:44,772 >>   Batch size = 100
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.01), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.01), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.01), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.01), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.01), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.01), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.73), 'median_accuracy': np.float64(84.66), 'mean_accuracy': np.float64(84.8), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.01), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
[INFO|trainer.py:2907] 2025-06-23 23:52:12,818 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-23 23:52:12,819 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-23 23:52:12,819 >>   Batch size = 100
all entropy: 0.6580074975533652 0.6868717427125868 0.69298426068761 0.6930885314445381 0.6774185681062022 0.6832026653984116 0.6902706709253009 0.6926192559325051 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834638271216924
avg cont entropy: 0.6888110314004385
delta all entropy: 0.004009923728398146 0.025078421936145512 0.01083156980700517 0.010935840563933263 -0.015722095998607855 -0.00978159528919842 0.006071217349983615 0.012726532888981956 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.01053147080637119
all entropy: 0.6580074975533652 0.6868717427125868 0.69298426068761 0.6930885314445381 0.6774185681062022 0.6832026653984116 0.6902706709253009 0.6926192559325051 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834638271216924
avg cont entropy: 0.6888110314004385
all entropy: 0.6580074975533652 0.6868717427125868 0.69298426068761 0.6930885314445381 0.6774185681062022 0.6832026653984116 0.6902706709253009 0.6926192559325051 0.6733033357438176 0.6868717427125868delta all entropy: 0.004009923728398146 0.025078421936145512 0.01083156980700517 0.010935840563933263 -0.015722095998607855 -0.00978159528919842 0.006071217349983615 0.012726532888981956 0.030323814213620404 0.03084107886345011

delta avg entropy: 0.01053147080637119
avg entropy: 0.6834638271216924
avg cont entropy: 0.6888110314004385
delta all entropy: 0.004009923728398146 0.025078421936145512 0.01083156980700517 0.010935840563933263 -0.015722095998607855 -0.00978159528919842 0.006071217349983615 0.012726532888981956 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.01053147080637119
all entropy: 0.6580074975533652 0.6868717427125868 0.69298426068761 0.6930885314445381 0.6774185681062022 0.6832026653984116 0.6902706709253009 0.6926192559325051 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834638271216924
avg cont entropy: 0.6888110314004385
delta all entropy: 0.004009923728398146 0.025078421936145512 0.01083156980700517 0.010935840563933263 -0.015722095998607855 -0.00978159528919842 0.006071217349983615 0.012726532888981956 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.01053147080637119
all entropy: 0.6580074975533652 0.6868717427125868 0.69298426068761 0.6930885314445381 0.6774185681062022 0.6832026653984116 0.6902706709253009 0.6926192559325051 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834638271216924
avg cont entropy: 0.6888110314004385
delta all entropy: 0.004009923728398146 0.025078421936145512 0.01083156980700517 0.010935840563933263 -0.015722095998607855 -0.00978159528919842 0.006071217349983615 0.012726532888981956 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.01053147080637119
all entropy: 0.6580074975533652 0.6868717427125868 0.69298426068761 0.6930885314445381 0.6774185681062022 0.6832026653984116 0.6902706709253009 0.6926192559325051 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834638271216924
avg cont entropy: 0.6888110314004385
delta all entropy: 0.004009923728398146 0.025078421936145512 0.01083156980700517 0.010935840563933263 -0.015722095998607855 -0.00978159528919842 0.006071217349983615 0.012726532888981956 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.01053147080637119
all entropy: 0.6580074975533652 0.6868717427125868 0.69298426068761 0.6930885314445381 0.6774185681062022 0.6832026653984116 0.6902706709253009 0.6926192559325051 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834638271216924
avg cont entropy: 0.6888110314004385
delta all entropy: 0.004009923728398146 0.025078421936145512 0.01083156980700517 0.010935840563933263 -0.015722095998607855 -0.00978159528919842 0.006071217349983615 0.012726532888981956 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.01053147080637119
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-23 23:52:39,998 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-850
[INFO|configuration_utils.py:425] 2025-06-23 23:52:39,999 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-850/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:53:06,452 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-850/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:53:06,453 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-850/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:53:06,454 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-850/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:53:06,481 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-850/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-23 23:53:06,486] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step850 is begin to save!
[2025-06-23 23:53:18,725] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-850/global_step850/mp_rank_00_model_states.pt
[2025-06-23 23:54:15,420] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step850 is begin to save!
[2025-06-23 23:54:26,075] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-850/global_step850/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:55:23,967 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-750] due to args.save_total_limit
{'loss': 0.0082, 'learning_rate': 3.1955555555555564e-06, 'epoch': 86.0}
[INFO|trainer.py:2655] 2025-06-23 23:56:27,612 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-860
[INFO|configuration_utils.py:425] 2025-06-23 23:56:27,613 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-860/config.json
[INFO|modeling_utils.py:1070] 2025-06-23 23:56:53,466 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-860/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-23 23:56:53,469 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-860/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-23 23:56:53,469 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-860/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-23 23:56:53,503 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-860/spiece.model
[2025-06-23 23:56:53,508] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step860 is begin to save!
[2025-06-23 23:57:04,063] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-860/global_step860/mp_rank_00_model_states.pt
[2025-06-23 23:57:54,364] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step860 is begin to save!
[2025-06-23 23:58:04,925] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-860/global_step860/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-23 23:59:05,543 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-760] due to args.save_total_limit
{'loss': 0.0082, 'learning_rate': 2.9744444444444455e-06, 'epoch': 87.0}
[INFO|trainer.py:2655] 2025-06-24 00:00:10,378 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-870
[INFO|configuration_utils.py:425] 2025-06-24 00:00:10,380 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-870/config.json
[INFO|modeling_utils.py:1070] 2025-06-24 00:00:36,768 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-870/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-24 00:00:36,771 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-870/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-24 00:00:36,772 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-870/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-24 00:00:36,813 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-870/spiece.model
[2025-06-24 00:00:36,819] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step870 is begin to save!
[2025-06-24 00:00:47,262] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-870/global_step870/mp_rank_00_model_states.pt
[2025-06-24 00:01:39,916] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step870 is begin to save!
[2025-06-24 00:01:50,418] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-870/global_step870/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-24 00:02:50,094 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-770] due to args.save_total_limit
{'loss': 0.0082, 'learning_rate': 2.753333333333333e-06, 'epoch': 88.0}
[INFO|trainer.py:2655] 2025-06-24 00:03:51,462 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-880
[INFO|configuration_utils.py:425] 2025-06-24 00:03:51,465 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-880/config.json
[INFO|modeling_utils.py:1070] 2025-06-24 00:04:16,860 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-880/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-24 00:04:16,863 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-880/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-24 00:04:16,863 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-880/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-24 00:04:16,894 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-880/spiece.model
[2025-06-24 00:04:16,899] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step880 is begin to save!
[2025-06-24 00:04:27,393] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-880/global_step880/mp_rank_00_model_states.pt
[2025-06-24 00:05:18,851] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step880 is begin to save!
[2025-06-24 00:05:29,337] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-880/global_step880/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-24 00:06:27,552 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-780] due to args.save_total_limit
{'loss': 0.009, 'learning_rate': 2.5322222222222225e-06, 'epoch': 89.0}
[INFO|trainer.py:2655] 2025-06-24 00:07:42,720 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-890
[INFO|configuration_utils.py:425] 2025-06-24 00:07:42,722 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-890/config.json
[INFO|modeling_utils.py:1070] 2025-06-24 00:08:11,342 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-890/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-24 00:08:11,345 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-890/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-24 00:08:11,345 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-890/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-24 00:08:11,374 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-890/spiece.model
[2025-06-24 00:08:11,380] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step890 is begin to save!
[2025-06-24 00:08:21,982] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-890/global_step890/mp_rank_00_model_states.pt
[2025-06-24 00:09:10,661] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step890 is begin to save!
[2025-06-24 00:09:21,244] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-890/global_step890/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-24 00:10:15,343 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-790] due to args.save_total_limit
{'loss': 0.0089, 'learning_rate': 2.3111111111111125e-06, 'epoch': 90.0}
[INFO|trainer.py:2907] 2025-06-24 00:11:18,134 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-24 00:11:18,135 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-24 00:11:18,135 >>   Batch size = 100
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
[INFO|trainer.py:2907] 2025-06-24 00:11:45,523 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-24 00:11:45,523 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-24 00:11:45,523 >>   Batch size = 100
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868

avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995

avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|trainer.py:2655] 2025-06-24 00:12:13,822 >> Saving model checkpoint to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-900
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO|configuration_utils.py:425] 2025-06-24 00:12:13,823 >> Configuration saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-900/config.json
[INFO|modeling_utils.py:1070] 2025-06-24 00:12:39,959 >> Model weights saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-900/pytorch_model.bin
[INFO|tokenization_utils_base.py:2043] 2025-06-24 00:12:39,961 >> tokenizer config file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2049] 2025-06-24 00:12:39,961 >> Special tokens file saved in /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-900/special_tokens_map.json
[INFO|tokenization_t5_fast.py:159] 2025-06-24 00:12:39,988 >> Copy vocab file to /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-900/spiece.model
/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[2025-06-24 00:12:39,993] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step900 is begin to save!
[2025-06-24 00:12:50,509] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-900/global_step900/mp_rank_00_model_states.pt
[2025-06-24 00:13:40,117] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step900 is begin to save!
[2025-06-24 00:13:50,661] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: /home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-900/global_step900/mp_rank_00_model_states.pt
[INFO|trainer.py:2733] 2025-06-24 00:14:46,739 >> Deleting older checkpoint [/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623/checkpoint-800] due to args.save_total_limit
[INFO|trainer.py:2031] 2025-06-24 00:14:52,744 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 21219.7257, 'train_samples_per_second': 1.32, 'train_steps_per_second': 0.047, 'train_loss': 0.011242832392454147, 'epoch': 90.0}
[INFO|trainer.py:2907] 2025-06-24 00:14:52,747 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-24 00:14:52,748 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-24 00:14:52,748 >>   Batch size = 100
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
all entropy: 0.6599283081693581 0.6868717427125868 0.6926192559325051 0.69314066410481 0.6774185681062022 0.6821526908806048 0.6890688576857509 0.69298426068761 0.6733033357438176 0.6868717427125868
avg entropy: 0.6834359426735832
avg cont entropy: 0.688627191937263
delta all entropy: 0.005930734344391064 0.025078421936145512 0.010466565051900245 0.010987973224205194 -0.015722095998607855 -0.01083156980700517 0.004869404110433573 0.013091537644086881 0.030323814213620404 0.03084107886345011
delta avg entropy: 0.010503586358261995
06/24/2025 00:15:19 - INFO - __main__ - dev_unsupervised_all entropy = [0.65992831 0.68687174 0.69261926 0.69314066 0.67741857 0.68215269
 0.68906886 0.69298426 0.67330334 0.68687174]
06/24/2025 00:15:19 - INFO - __main__ - dev_unsupervised_avg entropy = 0.6834359426735832
06/24/2025 00:15:19 - INFO - __main__ - dev_unsupervised_avg cont entropy = 0.688627191937263
06/24/2025 00:15:19 - INFO - __main__ - dev_unsupervised_delta all entropy = [ 0.00593073  0.02507842  0.01046657  0.01098797 -0.0157221  -0.01083157
  0.0048694   0.01309154  0.03032381  0.03084108]
06/24/2025 00:15:19 - INFO - __main__ - dev_unsupervised_delta avg entropy = 0.010503586358261995
[INFO|trainer.py:2907] 2025-06-24 00:15:19,826 >> ***** Running Evaluation *****
[INFO|trainer.py:2909] 2025-06-24 00:15:19,826 >>   Num examples = 5540
[INFO|trainer.py:2912] 2025-06-24 00:15:19,826 >>   Batch size = 100
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
Best checkpoint at step 750: 
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.73,min_accuracy=81.23,std_accuracy=2.25,avg_ensemble_accuracy=88.09,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg entropy at step 700:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.87,min_accuracy=81.23,std_accuracy=2.18,avg_ensemble_accuracy=87.36,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg cont entropy at step 650:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.8,min_accuracy=81.59,std_accuracy=2.18,avg_ensemble_accuracy=87.73,vote_ensemble_accuracy=86.28
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
Best checkpoint at step 750: 
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.73,min_accuracy=81.23,std_accuracy=2.25,avg_ensemble_accuracy=88.09,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg entropy at step 700:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.87,min_accuracy=81.23,std_accuracy=2.18,avg_ensemble_accuracy=87.36,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg cont entropy at step 650:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.8,min_accuracy=81.59,std_accuracy=2.18,avg_ensemble_accuracy=87.73,vote_ensemble_accuracy=86.28
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
Best checkpoint at step 750: 
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.73,min_accuracy=81.23,std_accuracy=2.25,avg_ensemble_accuracy=88.09,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg entropy at step 700:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.87,min_accuracy=81.23,std_accuracy=2.18,avg_ensemble_accuracy=87.36,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg cont entropy at step 650:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.8,min_accuracy=81.59,std_accuracy=2.18,avg_ensemble_accuracy=87.73,vote_ensemble_accuracy=86.28
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
Best checkpoint at step 750: 
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.73,min_accuracy=81.23,std_accuracy=2.25,avg_ensemble_accuracy=88.09,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg entropy at step 700:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.87,min_accuracy=81.23,std_accuracy=2.18,avg_ensemble_accuracy=87.36,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg cont entropy at step 650:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.8,min_accuracy=81.59,std_accuracy=2.18,avg_ensemble_accuracy=87.73,vote_ensemble_accuracy=86.28
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
Best checkpoint at step 750: 
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.73,min_accuracy=81.23,std_accuracy=2.25,avg_ensemble_accuracy=88.09,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg entropy at step 700:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.87,min_accuracy=81.23,std_accuracy=2.18,avg_ensemble_accuracy=87.36,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg cont entropy at step 650:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.8,min_accuracy=81.59,std_accuracy=2.18,avg_ensemble_accuracy=87.73,vote_ensemble_accuracy=86.28
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
Best checkpoint at step 750: 
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.73,min_accuracy=81.23,std_accuracy=2.25,avg_ensemble_accuracy=88.09,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg entropy at step 700:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.87,min_accuracy=81.23,std_accuracy=2.18,avg_ensemble_accuracy=87.36,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg cont entropy at step 650:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.8,min_accuracy=81.59,std_accuracy=2.18,avg_ensemble_accuracy=87.73,vote_ensemble_accuracy=86.28
{'max_accuracy': np.float64(87.36), 'median_accuracy': np.float64(84.84), 'mean_accuracy': np.float64(84.69), 'min_accuracy': np.float64(81.23), 'std_accuracy': np.float64(2.03), 'avg_ensemble_accuracy': 88.09, 'vote_ensemble_accuracy': 87.36}
06/24/2025 00:15:47 - INFO - __main__ - max_accuracy = 87.36
06/24/2025 00:15:47 - INFO - __main__ - median_accuracy = 84.84
06/24/2025 00:15:47 - INFO - __main__ - mean_accuracy = 84.69
06/24/2025 00:15:47 - INFO - __main__ - min_accuracy = 81.23
06/24/2025 00:15:47 - INFO - __main__ - std_accuracy = 2.03
06/24/2025 00:15:47 - INFO - __main__ - avg_ensemble_accuracy = 88.09
06/24/2025 00:15:47 - INFO - __main__ - vote_ensemble_accuracy = 87.36
Best checkpoint at step 750: 
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.73,min_accuracy=81.23,std_accuracy=2.25,avg_ensemble_accuracy=88.09,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg entropy at step 700:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.87,min_accuracy=81.23,std_accuracy=2.18,avg_ensemble_accuracy=87.36,vote_ensemble_accuracy=86.64
Best checkpoint selected by avg cont entropy at step 650:
max_accuracy=87.73,median_accuracy=84.66,mean_accuracy=84.8,min_accuracy=81.59,std_accuracy=2.18,avg_ensemble_accuracy=87.73,vote_ensemble_accuracy=86.28
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33m/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/rte/11B_ttt_t0.train.source.validation.super_glue.rte.validation.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250623[0m at: [34mhttps://wandb.ai/parsahejabi-academic-team/swarm-distillation/runs/d7bynzwx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250623_182113-d7bynzwx/logs[0m
bosco:1948868:1980819 [1] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948868:1980819 [1] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948868:1980819 [1] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948868:1980819 [1] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948868:1980819 [1] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948868:1980819 [1] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948868:1980819 [1] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948868:1980819 [1] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948868:1980819 [1] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948868:1952710 [1] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948870:1980821 [3] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948870:1980821 [3] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948870:1980821 [3] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948870:1980821 [3] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948870:1980821 [3] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948870:1980821 [3] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948870:1980821 [3] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948870:1980821 [3] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948870:1980821 [3] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948870:1952706 [3] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948871:1980823 [4] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948871:1980823 [4] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948871:1980823 [4] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948871:1980823 [4] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948871:1980823 [4] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948871:1980823 [4] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948871:1980823 [4] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948871:1980823 [4] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948871:1980823 [4] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948871:1952703 [4] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948869:1980825 [2] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948869:1980825 [2] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948869:1980825 [2] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948869:1980825 [2] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948869:1980825 [2] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948869:1980825 [2] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948869:1980825 [2] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948869:1980825 [2] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948869:1980825 [2] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948868:1952710 [1] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948869:1952704 [2] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948871:1952703 [4] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948872:1980827 [5] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948872:1980827 [5] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948872:1980827 [5] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948872:1980827 [5] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948872:1980827 [5] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948872:1980827 [5] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948872:1980827 [5] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948872:1980827 [5] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948872:1980827 [5] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948872:1952707 [5] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948868:1980819 [1] NCCL INFO comm 0x563823b6a150 rank 1 nranks 7 cudaDev 1 busId 3a000 - Abort COMPLETE
bosco:1948871:1980823 [4] NCCL INFO comm 0x55da1e4d1500 rank 4 nranks 7 cudaDev 4 busId ad000 - Abort COMPLETE
bosco:1948873:1980829 [6] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948873:1980829 [6] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948873:1980829 [6] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948873:1980829 [6] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948873:1980829 [6] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948873:1980829 [6] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948873:1980829 [6] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948873:1980829 [6] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948873:1980829 [6] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948870:1952706 [3] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948872:1952707 [5] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948873:1952709 [6] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948872:1980827 [5] NCCL INFO comm 0x559427c97290 rank 5 nranks 7 cudaDev 5 busId ae000 - Abort COMPLETE
bosco:1948870:1980821 [3] NCCL INFO comm 0x55c946db9410 rank 3 nranks 7 cudaDev 3 busId 3c000 - Abort COMPLETE
bosco:1948869:1980825 [2] NCCL INFO comm 0x55de7e7f4b90 rank 2 nranks 7 cudaDev 2 busId 3b000 - Abort COMPLETE
bosco:1948873:1980829 [6] NCCL INFO comm 0x55ef34271710 rank 6 nranks 7 cudaDev 6 busId bd000 - Abort COMPLETE
bosco:1948873:1952645 [6] NCCL INFO [Service thread] Connection closed by localRank 3
bosco:1948873:1952645 [6] NCCL INFO [Service thread] Connection closed by localRank 5
bosco:1948869:1952641 [0] NCCL INFO [Service thread] Connection closed by localRank 1
bosco:1948867:1952642 [0] NCCL INFO [Service thread] Connection closed by localRank 3
bosco:1948867:1952642 [0] NCCL INFO [Service thread] Connection closed by localRank 1
[2025-06-24 00:15:50,985] [INFO] [launch.py:351:main] Process 1948870 exits successfully.
[2025-06-24 00:15:50,987] [INFO] [launch.py:351:main] Process 1948871 exits successfully.
[2025-06-24 00:15:50,987] [INFO] [launch.py:351:main] Process 1948868 exits successfully.
[2025-06-24 00:15:50,987] [INFO] [launch.py:351:main] Process 1948869 exits successfully.
[2025-06-24 00:15:50,988] [INFO] [launch.py:351:main] Process 1948872 exits successfully.
[2025-06-24 00:15:51,989] [INFO] [launch.py:351:main] Process 1948873 exits successfully.
[rank0]:[W624 00:16:00.772293569 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
bosco:1948867:1980850 [0] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948867:1980850 [0] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948867:1980850 [0] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948867:1980850 [0] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948867:1980850 [0] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948867:1980850 [0] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948867:1952705 [0] NCCL INFO misc/socket.cc:881 -> 3
bosco:1948867:1980850 [0] NCCL INFO misc/socket.cc:64 -> 3
bosco:1948867:1980850 [0] NCCL INFO misc/socket.cc:80 -> 3
bosco:1948867:1980850 [0] NCCL INFO misc/socket.cc:829 -> 3
bosco:1948867:1980850 [0] NCCL INFO comm 0x55eaef19bf20 rank 0 nranks 7 cudaDev 0 busId 2d000 - Abort COMPLETE
[2025-06-24 00:16:03,002] [INFO] [launch.py:351:main] Process 1948867 exits successfully.
