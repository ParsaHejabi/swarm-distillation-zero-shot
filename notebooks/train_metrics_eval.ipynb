{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate training set metrics for saved checkpoints\n",
    "\n",
    "This notebook loads model checkpoints from the `checkpoints` directory and evaluates them on the same dataset that was used for training. The metrics are printed for each checkpoint so you can verify that the model was learning during training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "from ttt.dataloader import DatasetByPrompt\n",
    "from ttt.options import DataArguments\n",
    "from ttt.utils import compute_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Path to the experiment directory containing checkpoint subfolders\n",
    "run_dir = Path(\"/home/parsa/Codebases/GitHub_Repositories/swarm-distillation-zero-shot/checkpoints/anli_r1/11B_ttt_t0.train.source.validation.anli.none.dev_r1.T0pp.peft.lora.lora_alpha4.lora_drop0.3.bn1.pw1.0.np5.bsz1.ga4.lr2e-5.steps.1000_20250708\")\n",
    "\n",
    "# Cache directory used during training (adjust if needed)\n",
    "cache_dir = run_dir.parent.parent.parent / 'pretrain_models' / 'huggingface'\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List all checkpoints sorted by step number\n",
    "checkpoints = sorted([p for p in run_dir.glob('checkpoint-*') if p.is_dir()],\n                        key=lambda p: int(p.name.split('-')[-1]))\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset used for training (ANLI R1 validation split)\n",
    "data_args = DataArguments(\n",
    "    dataset_name='anli',\n",
    "    prompt_set_name='anli',\n",
    "    subset_name='none',\n",
    "    testset_name='dev_r1'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints[0])\n",
    "train_ds = DatasetByPrompt(data_args, str(cache_dir), tokenizer)\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, label_pad_token_id=-100)\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_checkpoint(ckpt_path):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(ckpt_path).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_data = []\n",
    "    golds = []\n",
    "    for idx in range(len(train_ds)):\n",
    "        example, label = train_ds[idx]\n",
    "        all_data.extend(example)\n",
    "        golds.append(label)\n",
    "\n",
    "    logprobs = []\n",
    "    batch_size = 8\n",
    "    for i in range(0, len(all_data), batch_size):\n",
    "        batch = collator(all_data[i:i+batch_size])\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            loss = model(**batch).loss\n",
    "        logprobs.extend(loss.detach().cpu().tolist())\n",
    "\n",
    "    results, _ = compute_metrics(\n",
    "        logprobs, len(train_ds), train_ds.num_choices, train_ds.num_prompts,\n",
    "        golds, metric\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_by_step = OrderedDict()\n",
    "for ckpt in checkpoints:\n",
    "    step = int(ckpt.name.split('-')[-1])\n",
    "    metrics_by_step[step] = evaluate_checkpoint(ckpt)\n",
    "\n",
    "metrics_by_step\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
